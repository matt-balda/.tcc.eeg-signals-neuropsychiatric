{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3233540,"sourceType":"datasetVersion","datasetId":1960298}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":138.166714,"end_time":"2024-11-21T17:48:22.32305","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-11-21T17:46:04.156336","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"dafebbba","cell_type":"markdown","source":"| Description\n\nAuthors of notebook: Mateus Balda and Alessandro Bof\n\nReference paper and dataset: https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2021.707581/full\n\n| Notebook structure\n0. Setup and Imports  \n1. Utility Functions  \n2. Data Loading and Preprocessing  \n3. Data Preparation for Training  \n4. Model Definition  \n5. Training and Evaluation  \n6. Results\n7. Conclusions, Problems, and Limitations\n\n| Training\n1. Binary classification for `y` main.disorder (Disorder vs. Health control)\n2. Balancing using BorderlineSMOTE for minority classes\n3. Training across the 21 subsets","metadata":{"papermill":{"duration":0.016914,"end_time":"2024-11-21T17:46:06.905102","exception":false,"start_time":"2024-11-21T17:46:06.888188","status":"completed"},"tags":[]}},{"id":"48cab304","cell_type":"markdown","source":"## | 0. Setup and Imports","metadata":{"papermill":{"duration":0.014176,"end_time":"2024-11-21T17:46:06.933804","exception":false,"start_time":"2024-11-21T17:46:06.919628","status":"completed"},"tags":[]}},{"id":"2bc1c36d","cell_type":"code","source":"!pip install -q imbalanced-learn==0.12.4;\n!pip install rtdl;\n!pip install libzero==0.0.4;","metadata":{"papermill":{"duration":23.839201,"end_time":"2024-11-21T17:46:30.78706","exception":false,"start_time":"2024-11-21T17:46:06.947859","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:17:09.814143Z","iopub.execute_input":"2025-06-10T03:17:09.814409Z","iopub.status.idle":"2025-06-10T03:18:58.314539Z","shell.execute_reply.started":"2025-06-10T03:17:09.814389Z","shell.execute_reply":"2025-06-10T03:18:58.313712Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting rtdl\n  Downloading rtdl-0.0.13-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.11/dist-packages (from rtdl) (1.26.4)\nCollecting torch<2,>=1.7 (from rtdl)\n  Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.18->rtdl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.18->rtdl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.18->rtdl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.18->rtdl) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.18->rtdl) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.18->rtdl) (2.4.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.7->rtdl) (4.13.2)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2,>=1.7->rtdl)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2,>=1.7->rtdl)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2,>=1.7->rtdl)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2,>=1.7->rtdl)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7->rtdl) (75.2.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7->rtdl) (0.45.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.18->rtdl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.18->rtdl) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.18->rtdl) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1.18->rtdl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1.18->rtdl) (2024.2.0)\nDownloading rtdl-0.0.13-py3-none-any.whl (23 kB)\nDownloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, rtdl\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchdata 0.11.0 requires torch>=2, but you have torch 1.13.1 which is incompatible.\ntorchmetrics 1.7.1 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\npytorch-lightning 2.5.1.post0 requires torch>=2.1.0, but you have torch 1.13.1 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\naccelerate 1.5.2 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 rtdl-0.0.13 torch-1.13.1\nCollecting libzero==0.0.4\n  Downloading libzero-0.0.4-py3-none-any.whl.metadata (834 bytes)\nRequirement already satisfied: numpy<2,>=1.17 in /usr/local/lib/python3.11/dist-packages (from libzero==0.0.4) (1.26.4)\nCollecting pynvml<9,>=8.0 (from libzero==0.0.4)\n  Downloading pynvml-8.0.4-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: torch<2,>=1.6 in /usr/local/lib/python3.11/dist-packages (from libzero==0.0.4) (1.13.1)\nRequirement already satisfied: tqdm<5,>=4.0 in /usr/local/lib/python3.11/dist-packages (from libzero==0.0.4) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.17->libzero==0.0.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.17->libzero==0.0.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.17->libzero==0.0.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.17->libzero==0.0.4) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.17->libzero==0.0.4) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.17->libzero==0.0.4) (2.4.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.6->libzero==0.0.4) (4.13.2)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.6->libzero==0.0.4) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.6->libzero==0.0.4) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.6->libzero==0.0.4) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.6->libzero==0.0.4) (11.7.99)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.6->libzero==0.0.4) (75.2.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.6->libzero==0.0.4) (0.45.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.17->libzero==0.0.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.17->libzero==0.0.4) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.17->libzero==0.0.4) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1.17->libzero==0.0.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1.17->libzero==0.0.4) (2024.2.0)\nDownloading libzero-0.0.4-py3-none-any.whl (26 kB)\nDownloading pynvml-8.0.4-py3-none-any.whl (36 kB)\nInstalling collected packages: pynvml, libzero\n  Attempting uninstall: pynvml\n    Found existing installation: pynvml 12.0.0\n    Uninstalling pynvml-12.0.0:\n      Successfully uninstalled pynvml-12.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nucx-py-cu12 0.42.0 requires pynvml<13.0.0a0,>=12.0.0, but you have pynvml 8.0.4 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pynvml<13.0.0a0,>=12.0.0, but you have pynvml 8.0.4 which is incompatible.\ndask-cuda 25.2.0 requires pynvml<13.0.0a0,>=12.0.0, but you have pynvml 8.0.4 which is incompatible.\nucxx-cu12 0.42.0 requires pynvml<13.0.0a0,>=12.0.0, but you have pynvml 8.0.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed libzero-0.0.4 pynvml-8.0.4\n","output_type":"stream"}],"execution_count":1},{"id":"b153e8b7","cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nfrom IPython.display import FileLink, display, HTML\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.impute import KNNImputer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom imblearn.over_sampling import BorderlineSMOTE\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\ntorch.__version__","metadata":{"papermill":{"duration":6.691258,"end_time":"2024-11-21T17:46:37.492706","exception":false,"start_time":"2024-11-21T17:46:30.801448","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:18:58.316396Z","iopub.execute_input":"2025-06-10T03:18:58.316657Z","iopub.status.idle":"2025-06-10T03:19:02.007333Z","shell.execute_reply.started":"2025-06-10T03:18:58.316631Z","shell.execute_reply":"2025-06-10T03:19:02.006636Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'1.13.1+cu117'"},"metadata":{}}],"execution_count":2},{"id":"9cb21f74","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nexperiment_name = \"binary_classification_main_disorder_fttransformer\"\ndevice","metadata":{"papermill":{"duration":0.025645,"end_time":"2024-11-21T17:46:37.533053","exception":false,"start_time":"2024-11-21T17:46:37.507408","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:02.008186Z","iopub.execute_input":"2025-06-10T03:19:02.008634Z","iopub.status.idle":"2025-06-10T03:19:02.107522Z","shell.execute_reply.started":"2025-06-10T03:19:02.008609Z","shell.execute_reply":"2025-06-10T03:19:02.106362Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"id":"8e255a82","cell_type":"code","source":"np.random.seed(123)\ntorch.manual_seed(123)","metadata":{"papermill":{"duration":0.028306,"end_time":"2024-11-21T17:46:37.575884","exception":false,"start_time":"2024-11-21T17:46:37.547578","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:02.110138Z","iopub.execute_input":"2025-06-10T03:19:02.110584Z","iopub.status.idle":"2025-06-10T03:19:02.135655Z","shell.execute_reply.started":"2025-06-10T03:19:02.110550Z","shell.execute_reply":"2025-06-10T03:19:02.134853Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7ba0f8467330>"},"metadata":{}}],"execution_count":4},{"id":"8ed1e095","cell_type":"markdown","source":"## | 1. Utility Functions","metadata":{}},{"id":"9fba47b1","cell_type":"markdown","source":"### 1. OUTLIERS","metadata":{}},{"id":"ba38cbee","cell_type":"code","source":"def detect_outliers_summary(df):\n    outliers_summary = {}\n\n    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_limit = Q1 - 1.5 * IQR\n        upper_limit = Q3 + 1.5 * IQR\n\n        outliers = df[(df[col] < lower_limit) | (df[col] > upper_limit)][col]\n        \n        outliers_summary[col] = {\n            'num_outliers': len(outliers),\n            'percent_outliers': len(outliers) / len(df) * 100,\n            'outliers': outliers.tolist(),\n            'lower_limit': lower_limit,\n            'upper_limit': upper_limit\n        }\n\n    return pd.DataFrame(outliers_summary).T\n\n\ndef treat_all_outliers_iqr(df, factor=1.5):\n    df_treated = df.copy()\n    \n    for column in df_treated.select_dtypes(include=[np.number]).columns:\n        Q1 = np.percentile(df_treated[column], 25)\n        Q3 = np.percentile(df_treated[column], 75)\n        IQR = Q3 - Q1\n\n        lower_bound = Q1 - factor * IQR\n        upper_bound = Q3 + factor * IQR\n\n        df_treated[column] = np.clip(df_treated[column], lower_bound, upper_bound)\n\n    return df_treated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:02.136675Z","iopub.execute_input":"2025-06-10T03:19:02.136988Z","iopub.status.idle":"2025-06-10T03:19:02.152000Z","shell.execute_reply.started":"2025-06-10T03:19:02.136968Z","shell.execute_reply":"2025-06-10T03:19:02.151235Z"}},"outputs":[],"execution_count":5},{"id":"38ee10b8","cell_type":"markdown","source":"### 2. NANS","metadata":{}},{"id":"f095b590","cell_type":"code","source":"def remove_missing_columns(df, threshold=0.5):\n    limit = int(threshold * len(df))\n    df = df.dropna(thresh=limit, axis=1)\n    return df\n    \ndef find_most_null_column(df, threshold=0.5):\n    null_ratios = df.isnull().mean()\n    for col, ration in null_ratios.items():\n        if ration > threshold:\n            return col\n    return None\n\ndef analyze_missing_values(df):\n    missing_values = df.isnull().sum()\n    missing_values = missing_values[missing_values > 0]\n    total_number_nans = df.isnull().sum().sum()\n    \n    return missing_values, total_number_nans\n\ndef handle_nans(df):\n    columns_with_nans = df.columns[df.isnull().any()].tolist()\n    \n    knn_imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n    \n    df_imputed = pd.DataFrame(knn_imputer.fit_transform(df[columns_with_nans]),\n                                columns=columns_with_nans)\n    \n    df[columns_with_nans] = df_imputed[columns_with_nans]\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:02.152747Z","iopub.execute_input":"2025-06-10T03:19:02.153071Z","iopub.status.idle":"2025-06-10T03:19:02.174495Z","shell.execute_reply.started":"2025-06-10T03:19:02.153042Z","shell.execute_reply":"2025-06-10T03:19:02.173694Z"}},"outputs":[],"execution_count":6},{"id":"2d543b71","cell_type":"markdown","source":"### 3. SETS & SUBSETS","metadata":{}},{"id":"88fce82e","cell_type":"code","source":"class Sets:\n    # AB = PSD (Power Spectral Density) 19 * 6\n    # COH = FC (Functional Connectivity) 171 * 6\n    \n    def __init__(self, dataframe: pd.DataFrame):\n        \n        if not isinstance(dataframe, pd.DataFrame):\n            raise ValueError(\"The parameter must be a pandas DataFrame\")\n        \n        self.df = dataframe.copy()\n        self.df_ab_psd = None\n        self.df_coh_fc = None\n        self.df_ab_psd_coh_fc = None\n        \n        self.__create_df_psd()\n        self.__create_df_fc()\n        self.__create_union_psd_fc()\n\n    # 19 (6 bands)\n    def __create_df_psd(self):\n        columns_AB = [col for col in self.df.columns if col.startswith('AB')]\n        self.df_ab_psd = self.df[columns_AB]\n    \n    # 171 (6 bands)\n    def __create_df_fc(self):\n        columns_COH = [col for col in self.df.columns if col.startswith('COH')]\n        self.df_coh_fc = self.df[columns_COH]\n        \n    def __create_union_psd_fc(self):\n        if self.df_ab_psd is not None and self.df_coh_fc is not None:\n            self.df_ab_psd_coh_fc = pd.concat([self.df_ab_psd, self.df_coh_fc], axis=1)\n        else:\n            raise ValueError(\"Subsets AB and COH were not created correctly\")\n\n    def create_dfs_bands(self, bands: list[str] = None, df: pd.DataFrame = None):\n        dfs_bands = {}\n        \n        if bands is None:\n            bands = ['delta', 'theta', 'alpha', 'beta', 'highbeta','gamma']\n        \n        for band in bands:\n            columns_band = [col for col in df.columns if f'.{band}.' in col]\n            if columns_band:\n                dfs_bands[band] = df[columns_band]\n        \n        return dfs_bands","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:02.175280Z","iopub.execute_input":"2025-06-10T03:19:02.175569Z","iopub.status.idle":"2025-06-10T03:19:02.196513Z","shell.execute_reply.started":"2025-06-10T03:19:02.175544Z","shell.execute_reply":"2025-06-10T03:19:02.195869Z"}},"outputs":[],"execution_count":7},{"id":"1fa9ca46","cell_type":"markdown","source":"## | 2. Data Loading and Preprocessing","metadata":{"papermill":{"duration":0.013958,"end_time":"2024-11-21T17:46:37.604828","exception":false,"start_time":"2024-11-21T17:46:37.59087","status":"completed"},"tags":[]}},{"id":"0198d03a","cell_type":"code","source":"df = pd.read_csv('../input/eeg-psychiatric-disorders-dataset/EEG.machinelearing_data_BRMH.csv')\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:02.197409Z","iopub.execute_input":"2025-06-10T03:19:02.197671Z","iopub.status.idle":"2025-06-10T03:19:02.553581Z","shell.execute_reply.started":"2025-06-10T03:19:02.197642Z","shell.execute_reply":"2025-06-10T03:19:02.552841Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(945, 1149)"},"metadata":{}}],"execution_count":8},{"id":"7818aa75","cell_type":"code","source":"# Checking / imputing Nan\n\noutput1 = analyze_missing_values(df)\noutput2 = find_most_null_column(df)\ndf = remove_missing_columns(df)\noutput3 = analyze_missing_values(df)\ndf = handle_nans(df)\n\ndisplay(output1)\ndisplay(HTML('<hr>'))\ndisplay(output2)\ndisplay(HTML('<hr>'))\ndisplay(output3)\ndisplay(df.shape)\ndisplay(HTML('<hr>'))\ndisplay(df.isna().sum().sum())\ndisplay(df)","metadata":{"papermill":{"duration":0.419845,"end_time":"2024-11-21T17:46:38.039168","exception":false,"start_time":"2024-11-21T17:46:37.619323","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:02.554472Z","iopub.execute_input":"2025-06-10T03:19:02.554760Z","iopub.status.idle":"2025-06-10T03:19:02.705506Z","shell.execute_reply.started":"2025-06-10T03:19:02.554740Z","shell.execute_reply":"2025-06-10T03:19:02.704655Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"(education        15\n IQ               13\n Unnamed: 122    945\n dtype: int64,\n 973)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<hr>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Unnamed: 122'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<hr>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(education    15\n IQ           13\n dtype: int64,\n 28)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(945, 1148)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<hr>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"     no. sex   age    eeg.date  education          IQ       main.disorder  \\\n0      1   M  57.0   2012.8.30   13.43871  101.580472  Addictive disorder   \n1      2   M  37.0    2012.9.6    6.00000  120.000000  Addictive disorder   \n2      3   M  32.0   2012.9.10   16.00000  113.000000  Addictive disorder   \n3      4   M  35.0   2012.10.8   18.00000  126.000000  Addictive disorder   \n4      5   M  36.0  2012.10.18   16.00000  112.000000  Addictive disorder   \n..   ...  ..   ...         ...        ...         ...                 ...   \n940  941   M  22.0   2014.8.28   13.00000  116.000000     Healthy control   \n941  942   M  26.0   2014.9.19   13.00000  118.000000     Healthy control   \n942  943   M  26.0   2014.9.27   16.00000  113.000000     Healthy control   \n943  944   M  24.0   2014.9.20   13.00000  107.000000     Healthy control   \n944  945   M  21.0  2015.10.23   13.00000  105.000000     Healthy control   \n\n        specific.disorder  AB.A.delta.a.FP1  AB.A.delta.b.FP2  ...  \\\n0    Alcohol use disorder         35.998557         21.717375  ...   \n1    Alcohol use disorder         13.425118         11.002916  ...   \n2    Alcohol use disorder         29.941780         27.544684  ...   \n3    Alcohol use disorder         21.496226         21.846832  ...   \n4    Alcohol use disorder         37.775667         33.607679  ...   \n..                    ...               ...               ...  ...   \n940       Healthy control         41.851823         36.771496  ...   \n941       Healthy control         18.986856         19.401387  ...   \n942       Healthy control         28.781317         32.369230  ...   \n943       Healthy control         19.929100         25.196375  ...   \n944       Healthy control         65.195346         69.241972  ...   \n\n     COH.F.gamma.o.Pz.p.P4  COH.F.gamma.o.Pz.q.T6  COH.F.gamma.o.Pz.r.O1  \\\n0                55.989192              16.739679              23.452271   \n1                45.595619              17.510824              26.777368   \n2                99.475453              70.654171              39.131547   \n3                59.986561              63.822201              36.478254   \n4                61.462720              59.166097              51.465531   \n..                     ...                    ...                    ...   \n940              82.905657              34.850706              63.970519   \n941              65.917918              66.700117              44.756285   \n942              61.040959              27.632209              45.552852   \n943              99.113664              48.328934              41.248470   \n944              78.600293              68.255430              70.687410   \n\n     COH.F.gamma.o.Pz.s.O2  COH.F.gamma.p.P4.q.T6  COH.F.gamma.p.P4.r.O1  \\\n0                45.678820              30.167520              16.918761   \n1                28.201062              57.108861              32.375401   \n2                69.920996              71.063644              38.534505   \n3                47.117006              84.658376              24.724096   \n4                58.635415              80.685608              62.138436   \n..                     ...                    ...                    ...   \n940              63.982003              51.244725              62.203684   \n941              49.787513              98.905995              54.021304   \n942              33.638817              46.690983              19.382928   \n943              28.192238              48.665743              42.007147   \n944              74.433908              74.294750              53.254681   \n\n     COH.F.gamma.p.P4.s.O2  COH.F.gamma.q.T6.r.O1  COH.F.gamma.q.T6.s.O2  \\\n0                48.850427               9.422630              34.507082   \n1                60.351749              13.900981              57.831848   \n2                69.908764              27.180532              64.803155   \n3                50.299349              35.319695              79.822944   \n4                75.888749              61.003944              87.455509   \n..                     ...                    ...                    ...   \n940              62.062237              31.013031              31.183413   \n941              93.902401              52.740396              92.807331   \n942              41.050717               7.045821              41.962451   \n943              28.735945              27.176500              27.529522   \n944              72.755265              47.810386              80.166825   \n\n     COH.F.gamma.r.O1.s.O2  \n0                28.613029  \n1                43.463261  \n2                31.485799  \n3                41.141873  \n4                70.531662  \n..                     ...  \n940              98.325230  \n941              56.320868  \n942              19.092111  \n943              20.028446  \n944              64.380273  \n\n[945 rows x 1148 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>no.</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>eeg.date</th>\n      <th>education</th>\n      <th>IQ</th>\n      <th>main.disorder</th>\n      <th>specific.disorder</th>\n      <th>AB.A.delta.a.FP1</th>\n      <th>AB.A.delta.b.FP2</th>\n      <th>...</th>\n      <th>COH.F.gamma.o.Pz.p.P4</th>\n      <th>COH.F.gamma.o.Pz.q.T6</th>\n      <th>COH.F.gamma.o.Pz.r.O1</th>\n      <th>COH.F.gamma.o.Pz.s.O2</th>\n      <th>COH.F.gamma.p.P4.q.T6</th>\n      <th>COH.F.gamma.p.P4.r.O1</th>\n      <th>COH.F.gamma.p.P4.s.O2</th>\n      <th>COH.F.gamma.q.T6.r.O1</th>\n      <th>COH.F.gamma.q.T6.s.O2</th>\n      <th>COH.F.gamma.r.O1.s.O2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>M</td>\n      <td>57.0</td>\n      <td>2012.8.30</td>\n      <td>13.43871</td>\n      <td>101.580472</td>\n      <td>Addictive disorder</td>\n      <td>Alcohol use disorder</td>\n      <td>35.998557</td>\n      <td>21.717375</td>\n      <td>...</td>\n      <td>55.989192</td>\n      <td>16.739679</td>\n      <td>23.452271</td>\n      <td>45.678820</td>\n      <td>30.167520</td>\n      <td>16.918761</td>\n      <td>48.850427</td>\n      <td>9.422630</td>\n      <td>34.507082</td>\n      <td>28.613029</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>M</td>\n      <td>37.0</td>\n      <td>2012.9.6</td>\n      <td>6.00000</td>\n      <td>120.000000</td>\n      <td>Addictive disorder</td>\n      <td>Alcohol use disorder</td>\n      <td>13.425118</td>\n      <td>11.002916</td>\n      <td>...</td>\n      <td>45.595619</td>\n      <td>17.510824</td>\n      <td>26.777368</td>\n      <td>28.201062</td>\n      <td>57.108861</td>\n      <td>32.375401</td>\n      <td>60.351749</td>\n      <td>13.900981</td>\n      <td>57.831848</td>\n      <td>43.463261</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>M</td>\n      <td>32.0</td>\n      <td>2012.9.10</td>\n      <td>16.00000</td>\n      <td>113.000000</td>\n      <td>Addictive disorder</td>\n      <td>Alcohol use disorder</td>\n      <td>29.941780</td>\n      <td>27.544684</td>\n      <td>...</td>\n      <td>99.475453</td>\n      <td>70.654171</td>\n      <td>39.131547</td>\n      <td>69.920996</td>\n      <td>71.063644</td>\n      <td>38.534505</td>\n      <td>69.908764</td>\n      <td>27.180532</td>\n      <td>64.803155</td>\n      <td>31.485799</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>M</td>\n      <td>35.0</td>\n      <td>2012.10.8</td>\n      <td>18.00000</td>\n      <td>126.000000</td>\n      <td>Addictive disorder</td>\n      <td>Alcohol use disorder</td>\n      <td>21.496226</td>\n      <td>21.846832</td>\n      <td>...</td>\n      <td>59.986561</td>\n      <td>63.822201</td>\n      <td>36.478254</td>\n      <td>47.117006</td>\n      <td>84.658376</td>\n      <td>24.724096</td>\n      <td>50.299349</td>\n      <td>35.319695</td>\n      <td>79.822944</td>\n      <td>41.141873</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>M</td>\n      <td>36.0</td>\n      <td>2012.10.18</td>\n      <td>16.00000</td>\n      <td>112.000000</td>\n      <td>Addictive disorder</td>\n      <td>Alcohol use disorder</td>\n      <td>37.775667</td>\n      <td>33.607679</td>\n      <td>...</td>\n      <td>61.462720</td>\n      <td>59.166097</td>\n      <td>51.465531</td>\n      <td>58.635415</td>\n      <td>80.685608</td>\n      <td>62.138436</td>\n      <td>75.888749</td>\n      <td>61.003944</td>\n      <td>87.455509</td>\n      <td>70.531662</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>940</th>\n      <td>941</td>\n      <td>M</td>\n      <td>22.0</td>\n      <td>2014.8.28</td>\n      <td>13.00000</td>\n      <td>116.000000</td>\n      <td>Healthy control</td>\n      <td>Healthy control</td>\n      <td>41.851823</td>\n      <td>36.771496</td>\n      <td>...</td>\n      <td>82.905657</td>\n      <td>34.850706</td>\n      <td>63.970519</td>\n      <td>63.982003</td>\n      <td>51.244725</td>\n      <td>62.203684</td>\n      <td>62.062237</td>\n      <td>31.013031</td>\n      <td>31.183413</td>\n      <td>98.325230</td>\n    </tr>\n    <tr>\n      <th>941</th>\n      <td>942</td>\n      <td>M</td>\n      <td>26.0</td>\n      <td>2014.9.19</td>\n      <td>13.00000</td>\n      <td>118.000000</td>\n      <td>Healthy control</td>\n      <td>Healthy control</td>\n      <td>18.986856</td>\n      <td>19.401387</td>\n      <td>...</td>\n      <td>65.917918</td>\n      <td>66.700117</td>\n      <td>44.756285</td>\n      <td>49.787513</td>\n      <td>98.905995</td>\n      <td>54.021304</td>\n      <td>93.902401</td>\n      <td>52.740396</td>\n      <td>92.807331</td>\n      <td>56.320868</td>\n    </tr>\n    <tr>\n      <th>942</th>\n      <td>943</td>\n      <td>M</td>\n      <td>26.0</td>\n      <td>2014.9.27</td>\n      <td>16.00000</td>\n      <td>113.000000</td>\n      <td>Healthy control</td>\n      <td>Healthy control</td>\n      <td>28.781317</td>\n      <td>32.369230</td>\n      <td>...</td>\n      <td>61.040959</td>\n      <td>27.632209</td>\n      <td>45.552852</td>\n      <td>33.638817</td>\n      <td>46.690983</td>\n      <td>19.382928</td>\n      <td>41.050717</td>\n      <td>7.045821</td>\n      <td>41.962451</td>\n      <td>19.092111</td>\n    </tr>\n    <tr>\n      <th>943</th>\n      <td>944</td>\n      <td>M</td>\n      <td>24.0</td>\n      <td>2014.9.20</td>\n      <td>13.00000</td>\n      <td>107.000000</td>\n      <td>Healthy control</td>\n      <td>Healthy control</td>\n      <td>19.929100</td>\n      <td>25.196375</td>\n      <td>...</td>\n      <td>99.113664</td>\n      <td>48.328934</td>\n      <td>41.248470</td>\n      <td>28.192238</td>\n      <td>48.665743</td>\n      <td>42.007147</td>\n      <td>28.735945</td>\n      <td>27.176500</td>\n      <td>27.529522</td>\n      <td>20.028446</td>\n    </tr>\n    <tr>\n      <th>944</th>\n      <td>945</td>\n      <td>M</td>\n      <td>21.0</td>\n      <td>2015.10.23</td>\n      <td>13.00000</td>\n      <td>105.000000</td>\n      <td>Healthy control</td>\n      <td>Healthy control</td>\n      <td>65.195346</td>\n      <td>69.241972</td>\n      <td>...</td>\n      <td>78.600293</td>\n      <td>68.255430</td>\n      <td>70.687410</td>\n      <td>74.433908</td>\n      <td>74.294750</td>\n      <td>53.254681</td>\n      <td>72.755265</td>\n      <td>47.810386</td>\n      <td>80.166825</td>\n      <td>64.380273</td>\n    </tr>\n  </tbody>\n</table>\n<p>945 rows × 1148 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"id":"f297557c","cell_type":"code","source":"# detecting outliers\n\noutliers_summary = detect_outliers_summary(df)\noutliers_summary.to_csv('outliers_summary.csv', index=True)\noutliers_summary['num_outliers'].sum()","metadata":{"papermill":{"duration":2.168088,"end_time":"2024-11-21T17:46:40.262857","exception":false,"start_time":"2024-11-21T17:46:38.094769","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:02.708624Z","iopub.execute_input":"2025-06-10T03:19:02.708888Z","iopub.status.idle":"2025-06-10T03:19:04.692363Z","shell.execute_reply.started":"2025-06-10T03:19:02.708871Z","shell.execute_reply":"2025-06-10T03:19:04.691668Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"26452"},"metadata":{}}],"execution_count":10},{"id":"825653d3","cell_type":"code","source":"label_encoder = LabelEncoder()\ntarget_name = 'main.disorder'\n\nX = df.iloc[:,8:]\ntarget_main = df[target_name]\n\nquantitative_features = df.loc[:, ['age', 'education', 'IQ']]\n\nsex = df['sex']\nsex_encoded = label_encoder.fit_transform(sex)\nsex_encoded = pd.Series(sex_encoded, name='sex')\n\nX = pd.concat([quantitative_features, sex_encoded, X], axis=1)\n\nX.shape, target_main.shape, quantitative_features.shape, sex.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:04.693229Z","iopub.execute_input":"2025-06-10T03:19:04.693508Z","iopub.status.idle":"2025-06-10T03:19:04.711630Z","shell.execute_reply.started":"2025-06-10T03:19:04.693487Z","shell.execute_reply":"2025-06-10T03:19:04.710875Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"((945, 1144), (945,), (945, 3), (945,))"},"metadata":{}}],"execution_count":11},{"id":"1b9587d9","cell_type":"code","source":"main_disorders = np.unique(target_main).tolist()\nmain_disorders.remove('Healthy control')\nmain_disorders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:04.712683Z","iopub.execute_input":"2025-06-10T03:19:04.713057Z","iopub.status.idle":"2025-06-10T03:19:04.720879Z","shell.execute_reply.started":"2025-06-10T03:19:04.713027Z","shell.execute_reply":"2025-06-10T03:19:04.720079Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['Addictive disorder',\n 'Anxiety disorder',\n 'Mood disorder',\n 'Obsessive compulsive disorder',\n 'Schizophrenia',\n 'Trauma and stress related disorder']"},"metadata":{}}],"execution_count":12},{"id":"e37dc56e","cell_type":"code","source":"X_concated = pd.concat([X, target_main], axis=1)\n\ndisplay(X.head())\ndisplay(X_concated.head())\ndisplay(X.isna().sum().sum())\ndisplay(X_concated.isna().sum().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:04.721762Z","iopub.execute_input":"2025-06-10T03:19:04.722027Z","iopub.status.idle":"2025-06-10T03:19:04.788146Z","shell.execute_reply.started":"2025-06-10T03:19:04.722011Z","shell.execute_reply":"2025-06-10T03:19:04.787481Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"    age  education          IQ  sex  AB.A.delta.a.FP1  AB.A.delta.b.FP2  \\\n0  57.0   13.43871  101.580472    1         35.998557         21.717375   \n1  37.0    6.00000  120.000000    1         13.425118         11.002916   \n2  32.0   16.00000  113.000000    1         29.941780         27.544684   \n3  35.0   18.00000  126.000000    1         21.496226         21.846832   \n4  36.0   16.00000  112.000000    1         37.775667         33.607679   \n\n   AB.A.delta.c.F7  AB.A.delta.d.F3  AB.A.delta.e.Fz  AB.A.delta.f.F4  ...  \\\n0        21.518280        26.825048        26.611516        25.732649  ...   \n1        11.942516        15.272216        14.151570        12.456034  ...   \n2        17.150159        23.608960        27.087811        13.541237  ...   \n3        17.364316        13.833701        14.100954        13.100939  ...   \n4        21.865556        21.771413        22.854536        21.456377  ...   \n\n   COH.F.gamma.o.Pz.p.P4  COH.F.gamma.o.Pz.q.T6  COH.F.gamma.o.Pz.r.O1  \\\n0              55.989192              16.739679              23.452271   \n1              45.595619              17.510824              26.777368   \n2              99.475453              70.654171              39.131547   \n3              59.986561              63.822201              36.478254   \n4              61.462720              59.166097              51.465531   \n\n   COH.F.gamma.o.Pz.s.O2  COH.F.gamma.p.P4.q.T6  COH.F.gamma.p.P4.r.O1  \\\n0              45.678820              30.167520              16.918761   \n1              28.201062              57.108861              32.375401   \n2              69.920996              71.063644              38.534505   \n3              47.117006              84.658376              24.724096   \n4              58.635415              80.685608              62.138436   \n\n   COH.F.gamma.p.P4.s.O2  COH.F.gamma.q.T6.r.O1  COH.F.gamma.q.T6.s.O2  \\\n0              48.850427               9.422630              34.507082   \n1              60.351749              13.900981              57.831848   \n2              69.908764              27.180532              64.803155   \n3              50.299349              35.319695              79.822944   \n4              75.888749              61.003944              87.455509   \n\n   COH.F.gamma.r.O1.s.O2  \n0              28.613029  \n1              43.463261  \n2              31.485799  \n3              41.141873  \n4              70.531662  \n\n[5 rows x 1144 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>education</th>\n      <th>IQ</th>\n      <th>sex</th>\n      <th>AB.A.delta.a.FP1</th>\n      <th>AB.A.delta.b.FP2</th>\n      <th>AB.A.delta.c.F7</th>\n      <th>AB.A.delta.d.F3</th>\n      <th>AB.A.delta.e.Fz</th>\n      <th>AB.A.delta.f.F4</th>\n      <th>...</th>\n      <th>COH.F.gamma.o.Pz.p.P4</th>\n      <th>COH.F.gamma.o.Pz.q.T6</th>\n      <th>COH.F.gamma.o.Pz.r.O1</th>\n      <th>COH.F.gamma.o.Pz.s.O2</th>\n      <th>COH.F.gamma.p.P4.q.T6</th>\n      <th>COH.F.gamma.p.P4.r.O1</th>\n      <th>COH.F.gamma.p.P4.s.O2</th>\n      <th>COH.F.gamma.q.T6.r.O1</th>\n      <th>COH.F.gamma.q.T6.s.O2</th>\n      <th>COH.F.gamma.r.O1.s.O2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>57.0</td>\n      <td>13.43871</td>\n      <td>101.580472</td>\n      <td>1</td>\n      <td>35.998557</td>\n      <td>21.717375</td>\n      <td>21.518280</td>\n      <td>26.825048</td>\n      <td>26.611516</td>\n      <td>25.732649</td>\n      <td>...</td>\n      <td>55.989192</td>\n      <td>16.739679</td>\n      <td>23.452271</td>\n      <td>45.678820</td>\n      <td>30.167520</td>\n      <td>16.918761</td>\n      <td>48.850427</td>\n      <td>9.422630</td>\n      <td>34.507082</td>\n      <td>28.613029</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>37.0</td>\n      <td>6.00000</td>\n      <td>120.000000</td>\n      <td>1</td>\n      <td>13.425118</td>\n      <td>11.002916</td>\n      <td>11.942516</td>\n      <td>15.272216</td>\n      <td>14.151570</td>\n      <td>12.456034</td>\n      <td>...</td>\n      <td>45.595619</td>\n      <td>17.510824</td>\n      <td>26.777368</td>\n      <td>28.201062</td>\n      <td>57.108861</td>\n      <td>32.375401</td>\n      <td>60.351749</td>\n      <td>13.900981</td>\n      <td>57.831848</td>\n      <td>43.463261</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>32.0</td>\n      <td>16.00000</td>\n      <td>113.000000</td>\n      <td>1</td>\n      <td>29.941780</td>\n      <td>27.544684</td>\n      <td>17.150159</td>\n      <td>23.608960</td>\n      <td>27.087811</td>\n      <td>13.541237</td>\n      <td>...</td>\n      <td>99.475453</td>\n      <td>70.654171</td>\n      <td>39.131547</td>\n      <td>69.920996</td>\n      <td>71.063644</td>\n      <td>38.534505</td>\n      <td>69.908764</td>\n      <td>27.180532</td>\n      <td>64.803155</td>\n      <td>31.485799</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35.0</td>\n      <td>18.00000</td>\n      <td>126.000000</td>\n      <td>1</td>\n      <td>21.496226</td>\n      <td>21.846832</td>\n      <td>17.364316</td>\n      <td>13.833701</td>\n      <td>14.100954</td>\n      <td>13.100939</td>\n      <td>...</td>\n      <td>59.986561</td>\n      <td>63.822201</td>\n      <td>36.478254</td>\n      <td>47.117006</td>\n      <td>84.658376</td>\n      <td>24.724096</td>\n      <td>50.299349</td>\n      <td>35.319695</td>\n      <td>79.822944</td>\n      <td>41.141873</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36.0</td>\n      <td>16.00000</td>\n      <td>112.000000</td>\n      <td>1</td>\n      <td>37.775667</td>\n      <td>33.607679</td>\n      <td>21.865556</td>\n      <td>21.771413</td>\n      <td>22.854536</td>\n      <td>21.456377</td>\n      <td>...</td>\n      <td>61.462720</td>\n      <td>59.166097</td>\n      <td>51.465531</td>\n      <td>58.635415</td>\n      <td>80.685608</td>\n      <td>62.138436</td>\n      <td>75.888749</td>\n      <td>61.003944</td>\n      <td>87.455509</td>\n      <td>70.531662</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1144 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    age  education          IQ  sex  AB.A.delta.a.FP1  AB.A.delta.b.FP2  \\\n0  57.0   13.43871  101.580472    1         35.998557         21.717375   \n1  37.0    6.00000  120.000000    1         13.425118         11.002916   \n2  32.0   16.00000  113.000000    1         29.941780         27.544684   \n3  35.0   18.00000  126.000000    1         21.496226         21.846832   \n4  36.0   16.00000  112.000000    1         37.775667         33.607679   \n\n   AB.A.delta.c.F7  AB.A.delta.d.F3  AB.A.delta.e.Fz  AB.A.delta.f.F4  ...  \\\n0        21.518280        26.825048        26.611516        25.732649  ...   \n1        11.942516        15.272216        14.151570        12.456034  ...   \n2        17.150159        23.608960        27.087811        13.541237  ...   \n3        17.364316        13.833701        14.100954        13.100939  ...   \n4        21.865556        21.771413        22.854536        21.456377  ...   \n\n   COH.F.gamma.o.Pz.q.T6  COH.F.gamma.o.Pz.r.O1  COH.F.gamma.o.Pz.s.O2  \\\n0              16.739679              23.452271              45.678820   \n1              17.510824              26.777368              28.201062   \n2              70.654171              39.131547              69.920996   \n3              63.822201              36.478254              47.117006   \n4              59.166097              51.465531              58.635415   \n\n   COH.F.gamma.p.P4.q.T6  COH.F.gamma.p.P4.r.O1  COH.F.gamma.p.P4.s.O2  \\\n0              30.167520              16.918761              48.850427   \n1              57.108861              32.375401              60.351749   \n2              71.063644              38.534505              69.908764   \n3              84.658376              24.724096              50.299349   \n4              80.685608              62.138436              75.888749   \n\n   COH.F.gamma.q.T6.r.O1  COH.F.gamma.q.T6.s.O2  COH.F.gamma.r.O1.s.O2  \\\n0               9.422630              34.507082              28.613029   \n1              13.900981              57.831848              43.463261   \n2              27.180532              64.803155              31.485799   \n3              35.319695              79.822944              41.141873   \n4              61.003944              87.455509              70.531662   \n\n        main.disorder  \n0  Addictive disorder  \n1  Addictive disorder  \n2  Addictive disorder  \n3  Addictive disorder  \n4  Addictive disorder  \n\n[5 rows x 1145 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>education</th>\n      <th>IQ</th>\n      <th>sex</th>\n      <th>AB.A.delta.a.FP1</th>\n      <th>AB.A.delta.b.FP2</th>\n      <th>AB.A.delta.c.F7</th>\n      <th>AB.A.delta.d.F3</th>\n      <th>AB.A.delta.e.Fz</th>\n      <th>AB.A.delta.f.F4</th>\n      <th>...</th>\n      <th>COH.F.gamma.o.Pz.q.T6</th>\n      <th>COH.F.gamma.o.Pz.r.O1</th>\n      <th>COH.F.gamma.o.Pz.s.O2</th>\n      <th>COH.F.gamma.p.P4.q.T6</th>\n      <th>COH.F.gamma.p.P4.r.O1</th>\n      <th>COH.F.gamma.p.P4.s.O2</th>\n      <th>COH.F.gamma.q.T6.r.O1</th>\n      <th>COH.F.gamma.q.T6.s.O2</th>\n      <th>COH.F.gamma.r.O1.s.O2</th>\n      <th>main.disorder</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>57.0</td>\n      <td>13.43871</td>\n      <td>101.580472</td>\n      <td>1</td>\n      <td>35.998557</td>\n      <td>21.717375</td>\n      <td>21.518280</td>\n      <td>26.825048</td>\n      <td>26.611516</td>\n      <td>25.732649</td>\n      <td>...</td>\n      <td>16.739679</td>\n      <td>23.452271</td>\n      <td>45.678820</td>\n      <td>30.167520</td>\n      <td>16.918761</td>\n      <td>48.850427</td>\n      <td>9.422630</td>\n      <td>34.507082</td>\n      <td>28.613029</td>\n      <td>Addictive disorder</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>37.0</td>\n      <td>6.00000</td>\n      <td>120.000000</td>\n      <td>1</td>\n      <td>13.425118</td>\n      <td>11.002916</td>\n      <td>11.942516</td>\n      <td>15.272216</td>\n      <td>14.151570</td>\n      <td>12.456034</td>\n      <td>...</td>\n      <td>17.510824</td>\n      <td>26.777368</td>\n      <td>28.201062</td>\n      <td>57.108861</td>\n      <td>32.375401</td>\n      <td>60.351749</td>\n      <td>13.900981</td>\n      <td>57.831848</td>\n      <td>43.463261</td>\n      <td>Addictive disorder</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>32.0</td>\n      <td>16.00000</td>\n      <td>113.000000</td>\n      <td>1</td>\n      <td>29.941780</td>\n      <td>27.544684</td>\n      <td>17.150159</td>\n      <td>23.608960</td>\n      <td>27.087811</td>\n      <td>13.541237</td>\n      <td>...</td>\n      <td>70.654171</td>\n      <td>39.131547</td>\n      <td>69.920996</td>\n      <td>71.063644</td>\n      <td>38.534505</td>\n      <td>69.908764</td>\n      <td>27.180532</td>\n      <td>64.803155</td>\n      <td>31.485799</td>\n      <td>Addictive disorder</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35.0</td>\n      <td>18.00000</td>\n      <td>126.000000</td>\n      <td>1</td>\n      <td>21.496226</td>\n      <td>21.846832</td>\n      <td>17.364316</td>\n      <td>13.833701</td>\n      <td>14.100954</td>\n      <td>13.100939</td>\n      <td>...</td>\n      <td>63.822201</td>\n      <td>36.478254</td>\n      <td>47.117006</td>\n      <td>84.658376</td>\n      <td>24.724096</td>\n      <td>50.299349</td>\n      <td>35.319695</td>\n      <td>79.822944</td>\n      <td>41.141873</td>\n      <td>Addictive disorder</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36.0</td>\n      <td>16.00000</td>\n      <td>112.000000</td>\n      <td>1</td>\n      <td>37.775667</td>\n      <td>33.607679</td>\n      <td>21.865556</td>\n      <td>21.771413</td>\n      <td>22.854536</td>\n      <td>21.456377</td>\n      <td>...</td>\n      <td>59.166097</td>\n      <td>51.465531</td>\n      <td>58.635415</td>\n      <td>80.685608</td>\n      <td>62.138436</td>\n      <td>75.888749</td>\n      <td>61.003944</td>\n      <td>87.455509</td>\n      <td>70.531662</td>\n      <td>Addictive disorder</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1145 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0"},"metadata":{}}],"execution_count":13},{"id":"1edb2907","cell_type":"code","source":"# treat outliers X_concated\n\nX = treat_all_outliers_iqr(X, factor=1.5)\noutliers_summary = detect_outliers_summary(X)\noutliers_summary['num_outliers'].sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:04.788877Z","iopub.execute_input":"2025-06-10T03:19:04.789194Z","iopub.status.idle":"2025-06-10T03:19:20.679736Z","shell.execute_reply.started":"2025-06-10T03:19:04.789161Z","shell.execute_reply":"2025-06-10T03:19:20.679067Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":14},{"id":"7227738c","cell_type":"code","source":"binary_datasets = {}\n\nfor disorder in main_disorders:\n    \n    mask = X_concated[target_name].isin(['Healthy control', disorder])\n    \n    df_filtered = X_concated[mask]\n    df_filtered_X = X.loc[mask]\n    \n    y_binary = df_filtered[target_name].apply(lambda x: 0 if x == 'Healthy control' else 1)\n    \n    borderline_smote = BorderlineSMOTE(kind='borderline-2', random_state=42, k_neighbors=10, m_neighbors=15)\n    X_resampled, y_resampled = borderline_smote.fit_resample(df_filtered_X, y_binary)\n    \n    dataset = pd.concat([X_resampled, y_resampled], axis=1)\n    \n    binary_datasets[disorder] = {\n        'data': dataset,\n        'labels': ['Healthy control', disorder]\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:20.680589Z","iopub.execute_input":"2025-06-10T03:19:20.680891Z","iopub.status.idle":"2025-06-10T03:19:21.922504Z","shell.execute_reply.started":"2025-06-10T03:19:20.680866Z","shell.execute_reply":"2025-06-10T03:19:21.921680Z"}},"outputs":[],"execution_count":15},{"id":"cfd24df8","cell_type":"code","source":"binary_datasets['Mood disorder']['data']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:21.923421Z","iopub.execute_input":"2025-06-10T03:19:21.923738Z","iopub.status.idle":"2025-06-10T03:19:21.945075Z","shell.execute_reply.started":"2025-06-10T03:19:21.923712Z","shell.execute_reply":"2025-06-10T03:19:21.944298Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"           age  education          IQ  sex  AB.A.delta.a.FP1  \\\n0    32.870000  16.000000  108.000000    0         12.159137   \n1    20.240000  12.000000  127.000000    0         12.404484   \n2    19.890000  13.000000  113.000000    0         16.573145   \n3    39.180000  16.000000  112.600000    0         26.650019   \n4    28.420000   9.000000   98.800000    0         14.624474   \n..         ...        ...         ...  ...               ...   \n527  29.859179  16.056555  129.547562    1         14.332130   \n528  29.536491  12.767765  122.175057    0         16.018351   \n529  27.580821  16.000000  114.744465    1         23.887229   \n530  21.262096  12.877715  104.755431    1         41.877695   \n531  28.310714  16.000000  120.808442    1         12.657177   \n\n     AB.A.delta.b.FP2  AB.A.delta.c.F7  AB.A.delta.d.F3  AB.A.delta.e.Fz  \\\n0           13.113503         9.031007        14.879389        15.834830   \n1            9.737819        13.925651        12.325169        15.130696   \n2           15.586708        14.094928        12.660197        12.979617   \n3           22.823161        17.942133        17.148214        15.338128   \n4           14.277301        11.099375        16.047926        17.851932   \n..                ...              ...              ...              ...   \n527         23.574633        11.045143        18.351897        16.785585   \n528         17.909770        13.825752        14.791642        18.013545   \n529         23.936216        16.964908        23.727482        27.984030   \n530         44.607081        34.711693        28.247273        35.248240   \n531         13.350691        11.046637        11.577865        18.406045   \n\n     AB.A.delta.f.F4  ...  COH.F.gamma.o.Pz.q.T6  COH.F.gamma.o.Pz.r.O1  \\\n0          19.595759  ...              59.590594              77.310851   \n1          10.292518  ...              53.402639              60.535899   \n2          10.611797  ...              20.062716              36.041763   \n3          14.571966  ...              83.045735              85.752150   \n4          17.165379  ...              58.218838              61.211830   \n..               ...  ...                    ...                    ...   \n527        19.659401  ...              31.533846              55.828495   \n528        20.968432  ...              86.092666              53.775009   \n529        22.996285  ...              65.154648              48.855452   \n530        36.466592  ...              68.056056              70.364430   \n531        18.524563  ...              61.770018              41.084526   \n\n     COH.F.gamma.o.Pz.s.O2  COH.F.gamma.p.P4.q.T6  COH.F.gamma.p.P4.r.O1  \\\n0                75.280467              57.311188              48.015594   \n1                73.838548              72.298636              43.330485   \n2                36.436509              47.491240              20.875426   \n3                85.875624              86.286859              79.520327   \n4                65.038486              76.899037              49.849540   \n..                     ...                    ...                    ...   \n527              45.141456              50.233665              45.187841   \n528              92.172863              90.911438              45.939741   \n529              65.079410              69.760110              47.340168   \n530              73.989504              74.344268              53.351700   \n531              58.721209              83.692158              48.329199   \n\n     COH.F.gamma.p.P4.s.O2  COH.F.gamma.q.T6.r.O1  COH.F.gamma.q.T6.s.O2  \\\n0                59.579033              68.503920              82.885151   \n1                71.298037              23.109295              62.098128   \n2                53.747615               8.009907              50.202188   \n3                84.541455              82.452224              86.950905   \n4                71.791429              39.910653              82.739819   \n..                     ...                    ...                    ...   \n527              55.899282              19.821091              39.923730   \n528              88.578716              42.796943              85.889060   \n529              68.406285              31.144459              94.580466   \n530              72.497074              48.287194              79.983862   \n531              73.856083              48.508317              76.775900   \n\n     COH.F.gamma.r.O1.s.O2  main.disorder  \n0                86.986191              1  \n1                67.749204              1  \n2                33.647889              1  \n3                87.384296              1  \n4                62.285969              1  \n..                     ...            ...  \n527              47.636457              0  \n528              55.994294              0  \n529              36.281022              0  \n530              64.618663              0  \n531              64.945388              0  \n\n[532 rows x 1145 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>education</th>\n      <th>IQ</th>\n      <th>sex</th>\n      <th>AB.A.delta.a.FP1</th>\n      <th>AB.A.delta.b.FP2</th>\n      <th>AB.A.delta.c.F7</th>\n      <th>AB.A.delta.d.F3</th>\n      <th>AB.A.delta.e.Fz</th>\n      <th>AB.A.delta.f.F4</th>\n      <th>...</th>\n      <th>COH.F.gamma.o.Pz.q.T6</th>\n      <th>COH.F.gamma.o.Pz.r.O1</th>\n      <th>COH.F.gamma.o.Pz.s.O2</th>\n      <th>COH.F.gamma.p.P4.q.T6</th>\n      <th>COH.F.gamma.p.P4.r.O1</th>\n      <th>COH.F.gamma.p.P4.s.O2</th>\n      <th>COH.F.gamma.q.T6.r.O1</th>\n      <th>COH.F.gamma.q.T6.s.O2</th>\n      <th>COH.F.gamma.r.O1.s.O2</th>\n      <th>main.disorder</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32.870000</td>\n      <td>16.000000</td>\n      <td>108.000000</td>\n      <td>0</td>\n      <td>12.159137</td>\n      <td>13.113503</td>\n      <td>9.031007</td>\n      <td>14.879389</td>\n      <td>15.834830</td>\n      <td>19.595759</td>\n      <td>...</td>\n      <td>59.590594</td>\n      <td>77.310851</td>\n      <td>75.280467</td>\n      <td>57.311188</td>\n      <td>48.015594</td>\n      <td>59.579033</td>\n      <td>68.503920</td>\n      <td>82.885151</td>\n      <td>86.986191</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.240000</td>\n      <td>12.000000</td>\n      <td>127.000000</td>\n      <td>0</td>\n      <td>12.404484</td>\n      <td>9.737819</td>\n      <td>13.925651</td>\n      <td>12.325169</td>\n      <td>15.130696</td>\n      <td>10.292518</td>\n      <td>...</td>\n      <td>53.402639</td>\n      <td>60.535899</td>\n      <td>73.838548</td>\n      <td>72.298636</td>\n      <td>43.330485</td>\n      <td>71.298037</td>\n      <td>23.109295</td>\n      <td>62.098128</td>\n      <td>67.749204</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.890000</td>\n      <td>13.000000</td>\n      <td>113.000000</td>\n      <td>0</td>\n      <td>16.573145</td>\n      <td>15.586708</td>\n      <td>14.094928</td>\n      <td>12.660197</td>\n      <td>12.979617</td>\n      <td>10.611797</td>\n      <td>...</td>\n      <td>20.062716</td>\n      <td>36.041763</td>\n      <td>36.436509</td>\n      <td>47.491240</td>\n      <td>20.875426</td>\n      <td>53.747615</td>\n      <td>8.009907</td>\n      <td>50.202188</td>\n      <td>33.647889</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>39.180000</td>\n      <td>16.000000</td>\n      <td>112.600000</td>\n      <td>0</td>\n      <td>26.650019</td>\n      <td>22.823161</td>\n      <td>17.942133</td>\n      <td>17.148214</td>\n      <td>15.338128</td>\n      <td>14.571966</td>\n      <td>...</td>\n      <td>83.045735</td>\n      <td>85.752150</td>\n      <td>85.875624</td>\n      <td>86.286859</td>\n      <td>79.520327</td>\n      <td>84.541455</td>\n      <td>82.452224</td>\n      <td>86.950905</td>\n      <td>87.384296</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28.420000</td>\n      <td>9.000000</td>\n      <td>98.800000</td>\n      <td>0</td>\n      <td>14.624474</td>\n      <td>14.277301</td>\n      <td>11.099375</td>\n      <td>16.047926</td>\n      <td>17.851932</td>\n      <td>17.165379</td>\n      <td>...</td>\n      <td>58.218838</td>\n      <td>61.211830</td>\n      <td>65.038486</td>\n      <td>76.899037</td>\n      <td>49.849540</td>\n      <td>71.791429</td>\n      <td>39.910653</td>\n      <td>82.739819</td>\n      <td>62.285969</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>527</th>\n      <td>29.859179</td>\n      <td>16.056555</td>\n      <td>129.547562</td>\n      <td>1</td>\n      <td>14.332130</td>\n      <td>23.574633</td>\n      <td>11.045143</td>\n      <td>18.351897</td>\n      <td>16.785585</td>\n      <td>19.659401</td>\n      <td>...</td>\n      <td>31.533846</td>\n      <td>55.828495</td>\n      <td>45.141456</td>\n      <td>50.233665</td>\n      <td>45.187841</td>\n      <td>55.899282</td>\n      <td>19.821091</td>\n      <td>39.923730</td>\n      <td>47.636457</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>528</th>\n      <td>29.536491</td>\n      <td>12.767765</td>\n      <td>122.175057</td>\n      <td>0</td>\n      <td>16.018351</td>\n      <td>17.909770</td>\n      <td>13.825752</td>\n      <td>14.791642</td>\n      <td>18.013545</td>\n      <td>20.968432</td>\n      <td>...</td>\n      <td>86.092666</td>\n      <td>53.775009</td>\n      <td>92.172863</td>\n      <td>90.911438</td>\n      <td>45.939741</td>\n      <td>88.578716</td>\n      <td>42.796943</td>\n      <td>85.889060</td>\n      <td>55.994294</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>529</th>\n      <td>27.580821</td>\n      <td>16.000000</td>\n      <td>114.744465</td>\n      <td>1</td>\n      <td>23.887229</td>\n      <td>23.936216</td>\n      <td>16.964908</td>\n      <td>23.727482</td>\n      <td>27.984030</td>\n      <td>22.996285</td>\n      <td>...</td>\n      <td>65.154648</td>\n      <td>48.855452</td>\n      <td>65.079410</td>\n      <td>69.760110</td>\n      <td>47.340168</td>\n      <td>68.406285</td>\n      <td>31.144459</td>\n      <td>94.580466</td>\n      <td>36.281022</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>530</th>\n      <td>21.262096</td>\n      <td>12.877715</td>\n      <td>104.755431</td>\n      <td>1</td>\n      <td>41.877695</td>\n      <td>44.607081</td>\n      <td>34.711693</td>\n      <td>28.247273</td>\n      <td>35.248240</td>\n      <td>36.466592</td>\n      <td>...</td>\n      <td>68.056056</td>\n      <td>70.364430</td>\n      <td>73.989504</td>\n      <td>74.344268</td>\n      <td>53.351700</td>\n      <td>72.497074</td>\n      <td>48.287194</td>\n      <td>79.983862</td>\n      <td>64.618663</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>531</th>\n      <td>28.310714</td>\n      <td>16.000000</td>\n      <td>120.808442</td>\n      <td>1</td>\n      <td>12.657177</td>\n      <td>13.350691</td>\n      <td>11.046637</td>\n      <td>11.577865</td>\n      <td>18.406045</td>\n      <td>18.524563</td>\n      <td>...</td>\n      <td>61.770018</td>\n      <td>41.084526</td>\n      <td>58.721209</td>\n      <td>83.692158</td>\n      <td>48.329199</td>\n      <td>73.856083</td>\n      <td>48.508317</td>\n      <td>76.775900</td>\n      <td>64.945388</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>532 rows × 1145 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"id":"aaac7799","cell_type":"code","source":"sets_binaries = {}\n\nfor disorder, bin_data in binary_datasets.items():\n    df_bin = bin_data['data'].copy()\n    \n    X_bin = df_bin.drop(columns=[target_name,'age', 'education', 'IQ', 'sex'], errors='ignore') \n    X_quantitative = df_bin.loc[:, ['age', 'education', 'IQ', 'sex']]\n    y_bin = df_bin[target_name] if target_name in df_bin.columns else df_bin.iloc[:, -1]\n\n    sets = Sets(dataframe=X_bin)\n\n    sets_binaries[disorder] = {\n        'sets': sets,\n        'labels': bin_data['labels'],\n        'X_quantitative': X_quantitative,\n        'y': y_bin\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:21.946078Z","iopub.execute_input":"2025-06-10T03:19:21.946347Z","iopub.status.idle":"2025-06-10T03:19:22.016512Z","shell.execute_reply.started":"2025-06-10T03:19:21.946326Z","shell.execute_reply":"2025-06-10T03:19:22.015951Z"}},"outputs":[],"execution_count":17},{"id":"3f015d7e","cell_type":"code","source":"for disorder in sets_binaries:\n    sets = sets_binaries[disorder]['sets']\n\n    dfs_bands_psd = sets.create_dfs_bands(df=sets.df_ab_psd)\n    dfs_bands_fc = sets.create_dfs_bands(df=sets.df_coh_fc)\n    dfs_bands_psd_fc = sets.create_dfs_bands(df=sets.df_ab_psd_coh_fc)\n\n    sets_binaries[disorder].update({\n        'dfs_bands_psd': dfs_bands_psd,\n        'dfs_bands_fc': dfs_bands_fc,\n        'dfs_bands_psd_fc': dfs_bands_psd_fc,\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:22.017336Z","iopub.execute_input":"2025-06-10T03:19:22.017535Z","iopub.status.idle":"2025-06-10T03:19:22.110810Z","shell.execute_reply.started":"2025-06-10T03:19:22.017519Z","shell.execute_reply":"2025-06-10T03:19:22.110274Z"}},"outputs":[],"execution_count":18},{"id":"93fe8a7f","cell_type":"code","source":"for disorder, data in sets_binaries.items():\n    print(f\"Disorder: {disorder}\")\n    print(f\"Labels: {data['labels']}\")\n    print(f\"Number of samples: {len(data['y'])}\")\n    print(f\"Quantitative features shape: {data['X_quantitative'].shape}\")\n    print(f\"PSD bands shape: {data['sets'].df_ab_psd.shape}\")\n    print(f\"FC bands shape: {data['sets'].df_coh_fc.shape}\")\n    print(f\"PSD + FC bands shape: {data['sets'].df_ab_psd_coh_fc.shape}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:22.111584Z","iopub.execute_input":"2025-06-10T03:19:22.111931Z","iopub.status.idle":"2025-06-10T03:19:22.118282Z","shell.execute_reply.started":"2025-06-10T03:19:22.111897Z","shell.execute_reply":"2025-06-10T03:19:22.117381Z"}},"outputs":[{"name":"stdout","text":"Disorder: Addictive disorder\nLabels: ['Healthy control', 'Addictive disorder']\nNumber of samples: 372\nQuantitative features shape: (372, 4)\nPSD bands shape: (372, 114)\nFC bands shape: (372, 1026)\nPSD + FC bands shape: (372, 1140)\n\nDisorder: Anxiety disorder\nLabels: ['Healthy control', 'Anxiety disorder']\nNumber of samples: 214\nQuantitative features shape: (214, 4)\nPSD bands shape: (214, 114)\nFC bands shape: (214, 1026)\nPSD + FC bands shape: (214, 1140)\n\nDisorder: Mood disorder\nLabels: ['Healthy control', 'Mood disorder']\nNumber of samples: 532\nQuantitative features shape: (532, 4)\nPSD bands shape: (532, 114)\nFC bands shape: (532, 1026)\nPSD + FC bands shape: (532, 1140)\n\nDisorder: Obsessive compulsive disorder\nLabels: ['Healthy control', 'Obsessive compulsive disorder']\nNumber of samples: 190\nQuantitative features shape: (190, 4)\nPSD bands shape: (190, 114)\nFC bands shape: (190, 1026)\nPSD + FC bands shape: (190, 1140)\n\nDisorder: Schizophrenia\nLabels: ['Healthy control', 'Schizophrenia']\nNumber of samples: 234\nQuantitative features shape: (234, 4)\nPSD bands shape: (234, 114)\nFC bands shape: (234, 1026)\nPSD + FC bands shape: (234, 1140)\n\nDisorder: Trauma and stress related disorder\nLabels: ['Healthy control', 'Trauma and stress related disorder']\nNumber of samples: 256\nQuantitative features shape: (256, 4)\nPSD bands shape: (256, 114)\nFC bands shape: (256, 1026)\nPSD + FC bands shape: (256, 1140)\n\n","output_type":"stream"}],"execution_count":19},{"id":"5acc5bb2","cell_type":"code","source":"dfs_dicts_binaries = {}\n\nfor disorder, data in sets_binaries.items():\n    dfs_psd = data['dfs_bands_psd']\n    dfs_fc = data['dfs_bands_fc']\n    dfs_psd_fc = data['dfs_bands_psd_fc']\n    sets = data['sets']\n\n    df_dict_psd_all_bands = {\n        'psd_all_bands': sets.df_ab_psd,\n    }\n\n    df_dict_fc_all_bands = {\n        'fc_all_bands': sets.df_coh_fc,\n    }\n\n    df_dict_psd_fc_all_bands = {\n        'psd_fc_all_bands': sets.df_ab_psd_coh_fc,\n    }\n\n    df_dict_psd_band = {\n        f'psd_{band}': dfs_psd[band] for band in dfs_psd\n    }\n\n    df_dict_fc_band = {\n        f'fc_{band}': dfs_fc[band] for band in dfs_fc\n    }\n\n    df_dict_psd_fc_band = {\n        f'psd_fc_{band}': dfs_psd_fc[band] for band in dfs_psd_fc\n    }\n\n    dfs_dicts_binaries[disorder] = {\n        'df_dict_psd_all_bands': df_dict_psd_all_bands,\n        'df_dict_fc_all_bands': df_dict_fc_all_bands,\n        'df_dict_psd_fc_all_bands': df_dict_psd_fc_all_bands,\n        'df_dict_psd_band': df_dict_psd_band,\n        'df_dict_fc_band': df_dict_fc_band,\n        'df_dict_psd_fc_band': df_dict_psd_fc_band,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:22.119175Z","iopub.execute_input":"2025-06-10T03:19:22.119427Z","iopub.status.idle":"2025-06-10T03:19:22.138147Z","shell.execute_reply.started":"2025-06-10T03:19:22.119397Z","shell.execute_reply":"2025-06-10T03:19:22.137358Z"}},"outputs":[],"execution_count":20},{"id":"d147f176","cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport rtdl\nfrom rtdl import FTTransformer\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:22.138837Z","iopub.execute_input":"2025-06-10T03:19:22.139082Z","iopub.status.idle":"2025-06-10T03:19:22.157108Z","shell.execute_reply.started":"2025-06-10T03:19:22.139054Z","shell.execute_reply":"2025-06-10T03:19:22.156346Z"}},"outputs":[],"execution_count":21},{"id":"a35a82de","cell_type":"code","source":"def fit(model, train_loader, epochs=200, lr=1e-3, weight_decay=1e-4, device='cpu', patience=20):\n        model.to(device)\n        criterion = nn.BCEWithLogitsLoss()\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n\n        best_acc = 0\n        patience_counter = 0\n        history = {'train_loss': [], 'train_acc': []}\n        \n        for epoch in range(epochs):\n            model.train()\n            running_loss = 0.0\n            correct, total = 0, 0\n            \n            for inputs, targets in train_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                \n                assert set(targets.cpu().tolist()).issubset({0.0, 1.0}), \"Targets must be binary (0 or 1)\"\n                \n                optimizer.zero_grad()\n                outputs = model(inputs, None)\n                \n                # shape\n                outputs = outputs.view(-1).float()\n                targets = targets.view(-1).float() \n                \n                assert outputs.shape == targets.shape, \"Outputs and targets must have the same shape\"\n                assert outputs.dtype == torch.float32 and targets.dtype == torch.float32, \"Outputs and targets must be float32\"\n                assert outputs.dim() == 1 and targets.dim() == 1, \"Outputs and targets must be 1-dimensional\"\n                \n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n                \n                preds = (torch.sigmoid(outputs) > 0.5).long()\n                correct += (preds == targets.long()).sum().item()\n                total += targets.size(0)\n                running_loss += loss.item() * inputs.size(0)\n                \n            train_acc = correct / total\n            epoch_loss = running_loss / len(train_loader.dataset)\n            history['train_acc'].append(train_acc)\n            history['train_loss'].append(epoch_loss)\n            \n            if train_acc > best_acc:\n                best_acc = train_acc\n                patience_counter = 0\n            else:\n                patience_counter += 1\n\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n            \n            scheduler.step(epoch_loss)\n            \n            print(\n                f\"Epoch [{epoch+1}/{epochs}] | \"\n                f\"Train Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n            )\n        \n        return history\n\n\ndef test(model, test_loader, device='cpu'):\n    model.to(device)\n    model.eval()\n    \n    all_preds, all_probas, all_targets = [], [], []\n    correct, total = 0, 0\n    test_loss = 0.0\n    criterion = nn.BCEWithLogitsLoss()\n    history = {'test_loss': [], 'test_acc': [], 'test_auc': []}\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            outputs = model(inputs, None)\n            \n            outputs = outputs.view(-1).float()\n            targets = targets.view(-1).float() \n            \n            loss = criterion(outputs, targets)\n            \n            proba = torch.sigmoid(outputs)\n            pred = (proba > 0.5).long()\n\n            all_preds.append(pred.cpu())\n            all_probas.append(proba.cpu())\n            all_targets.append(targets.cpu())\n            \n            correct += (pred == targets.long()).sum().item()\n            total += targets.size(0)\n            test_loss += loss.item() * inputs.size(0)\n    \n    test_acc = correct / total\n    test_loss /= len(test_loader.dataset)\n    history['test_loss'].append(test_loss)\n    history['test_acc'].append(test_acc)\n    \n    y_true = torch.cat(all_targets).numpy().astype(int)\n    y_pred = torch.cat(all_preds).numpy().astype(int)\n    y_proba = torch.cat(all_probas).numpy().astype(float)\n\n    try:\n        auc = roc_auc_score(y_true, y_proba)\n    except ValueError:\n        auc = float('nan')\n\n    report = classification_report(y_true, y_pred, output_dict=True)\n    history['test_auc'].append(auc)\n\n    print(\n        f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test AUC: {auc:.4f}\"\n        )\n    \n    return test_acc, auc, report, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:22.157940Z","iopub.execute_input":"2025-06-10T03:19:22.158145Z","iopub.status.idle":"2025-06-10T03:19:22.173499Z","shell.execute_reply.started":"2025-06-10T03:19:22.158130Z","shell.execute_reply":"2025-06-10T03:19:22.172658Z"}},"outputs":[],"execution_count":22},{"id":"ee074deb","cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:22.174293Z","iopub.execute_input":"2025-06-10T03:19:22.174703Z","iopub.status.idle":"2025-06-10T03:19:22.191648Z","shell.execute_reply.started":"2025-06-10T03:19:22.174679Z","shell.execute_reply":"2025-06-10T03:19:22.191022Z"}},"outputs":[],"execution_count":23},{"id":"845bbf18","cell_type":"code","source":"all_results = {}\n\nfor disorder, dicts in dfs_dicts_binaries.items():\n    print(f\"\\n=== Disorder: {disorder} ===\")\n    \n    y_binary = sets_binaries[disorder]['y']\n    X_quantitative = sets_binaries[disorder]['X_quantitative']\n    labels = sets_binaries[disorder]['labels']\n    \n    all_results[disorder] = {}\n    \n    for dict_name, df_dict in dicts.items():\n        if not dict_name.startswith('df_dict_'):\n            continue\n        \n        for df_name, df in df_dict.items():\n            print(f\"\\n--- Processing: {df_name} ---\")\n            print(\"Shape:\", df.shape)\n            print(\"Missing values:\", df.isna().sum().sum())\n            \n            df_full = pd.concat([X_quantitative, df], axis=1)\n            assert all(df_full.index == y_binary.index), 'Misaligned indexes'\n            \n            X_train, X_test, y_train, y_test = train_test_split(\n                df_full,\n                y_binary,\n                train_size=0.80,\n                stratify=y_binary,\n                random_state=42\n            )\n            \n            X_cols = X_train.drop(columns=['sex']).columns\n            \n            scaler = RobustScaler()\n            X_train_scaled = scaler.fit_transform(X_train[X_cols])\n            X_test_scaled = scaler.transform(X_test[X_cols])\n            \n            X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_cols, index=X_train.index)\n            X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_cols, index=X_test.index)\n\n            X_train_final = pd.concat([X_train_scaled_df, X_train['sex']], axis=1)\n            X_test_final = pd.concat([X_test_scaled_df, X_test['sex']], axis=1)\n\n            X_train_array = X_train_final.to_numpy()\n            X_test_array = X_test_final.to_numpy()\n            y_train_array = y_train.to_numpy()\n            y_test_array = y_test.to_numpy()\n            \n            # tensors\n            X_train_tensor = torch.tensor(X_train_array, dtype=torch.float32).to(device)\n            y_train_tensor = torch.tensor(y_train_array, dtype=torch.long).to(device)\n            X_test_tensor = torch.tensor(X_test_array, dtype=torch.float32).to(device)\n            y_test_tensor = torch.tensor(y_test_array, dtype=torch.long).to(device)\n            \n            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n            test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n            test_loader = DataLoader(test_dataset, batch_size=32)\n            \n            model = FTTransformer.make_default(\n                n_num_features=X_train_array.shape[1],\n                cat_cardinalities=None,\n                last_layer_query_idx=[-1],\n                d_out=1,  # binary\n            )\n            \n            fit_history = fit(\n                model=model,\n                train_loader=train_loader,\n                epochs=200,\n                lr=1e-3,\n                weight_decay=1e-4,\n                device=device,\n                patience=20\n            )\n            \n            acc, auc, report, test_history = test(\n                model=model,\n                test_loader=test_loader,\n                device=device\n            )\n            \n            all_results.setdefault(disorder, {}).setdefault(dict_name, {})[df_name] = {\n                'accuracy': acc,\n                'auc': auc,\n                'classification_report': report,\n                'features': list(X_train_final.columns),\n                'X_train_shape': X_train_tensor.shape,\n                'X_test_shape': X_test_tensor.shape,\n                'history': {\n                    'fit': fit_history,\n                    'test': test_history\n                }\n            }\n            \n            # memory cleaning\n            del model\n            del X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:19:22.192489Z","iopub.execute_input":"2025-06-10T03:19:22.192965Z"}},"outputs":[{"name":"stdout","text":"\n=== Disorder: Addictive disorder ===\n\n--- Processing: psd_all_bands ---\nShape: (372, 114)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7246, Train Acc: 0.5185 | \nEpoch [2/200] | Train Loss: 0.6970, Train Acc: 0.5286 | \nEpoch [3/200] | Train Loss: 0.6957, Train Acc: 0.4983 | \nEpoch [4/200] | Train Loss: 0.6883, Train Acc: 0.5387 | \nEpoch [5/200] | Train Loss: 0.6726, Train Acc: 0.5926 | \nEpoch [6/200] | Train Loss: 0.5658, Train Acc: 0.7340 | \nEpoch [7/200] | Train Loss: 0.5724, Train Acc: 0.6768 | \nEpoch [8/200] | Train Loss: 0.5268, Train Acc: 0.7374 | \nEpoch [9/200] | Train Loss: 0.4473, Train Acc: 0.8047 | \nEpoch [10/200] | Train Loss: 0.5451, Train Acc: 0.7710 | \nEpoch [11/200] | Train Loss: 0.4532, Train Acc: 0.8047 | \nEpoch [12/200] | Train Loss: 0.4359, Train Acc: 0.7980 | \nEpoch [13/200] | Train Loss: 0.3977, Train Acc: 0.8350 | \nEpoch [14/200] | Train Loss: 0.5103, Train Acc: 0.7744 | \nEpoch [15/200] | Train Loss: 0.4825, Train Acc: 0.7912 | \nEpoch [16/200] | Train Loss: 0.3822, Train Acc: 0.8451 | \nEpoch [17/200] | Train Loss: 0.3228, Train Acc: 0.8687 | \nEpoch [18/200] | Train Loss: 0.3617, Train Acc: 0.8552 | \nEpoch [19/200] | Train Loss: 0.4359, Train Acc: 0.8316 | \nEpoch [20/200] | Train Loss: 0.2989, Train Acc: 0.8889 | \nEpoch [21/200] | Train Loss: 0.3010, Train Acc: 0.8990 | \nEpoch [22/200] | Train Loss: 0.2988, Train Acc: 0.8855 | \nEpoch [23/200] | Train Loss: 0.3686, Train Acc: 0.8586 | \nEpoch [24/200] | Train Loss: 0.3471, Train Acc: 0.8451 | \nEpoch [25/200] | Train Loss: 0.3022, Train Acc: 0.8923 | \nEpoch [26/200] | Train Loss: 0.2703, Train Acc: 0.9024 | \nEpoch [27/200] | Train Loss: 0.2775, Train Acc: 0.8822 | \nEpoch [28/200] | Train Loss: 0.2403, Train Acc: 0.8990 | \nEpoch [29/200] | Train Loss: 0.2270, Train Acc: 0.9226 | \nEpoch [30/200] | Train Loss: 0.2848, Train Acc: 0.8754 | \nEpoch [31/200] | Train Loss: 0.4071, Train Acc: 0.8418 | \nEpoch [32/200] | Train Loss: 0.4008, Train Acc: 0.8148 | \nEpoch [33/200] | Train Loss: 0.3164, Train Acc: 0.8653 | \nEpoch [34/200] | Train Loss: 0.2691, Train Acc: 0.8923 | \nEpoch [35/200] | Train Loss: 0.2534, Train Acc: 0.8923 | \nEpoch [36/200] | Train Loss: 0.2694, Train Acc: 0.8923 | \nEpoch [37/200] | Train Loss: 0.2502, Train Acc: 0.9024 | \nEpoch [38/200] | Train Loss: 0.2374, Train Acc: 0.9091 | \nEpoch [39/200] | Train Loss: 0.2627, Train Acc: 0.8990 | \nEpoch [40/200] | Train Loss: 0.2191, Train Acc: 0.9125 | \nEpoch [41/200] | Train Loss: 0.2329, Train Acc: 0.9192 | \nEpoch [42/200] | Train Loss: 0.2309, Train Acc: 0.9024 | \nEpoch [43/200] | Train Loss: 0.2557, Train Acc: 0.9091 | \nEpoch [44/200] | Train Loss: 0.2067, Train Acc: 0.9259 | \nEpoch [45/200] | Train Loss: 0.2182, Train Acc: 0.9125 | \nEpoch [46/200] | Train Loss: 0.2268, Train Acc: 0.9125 | \nEpoch [47/200] | Train Loss: 0.2804, Train Acc: 0.8788 | \nEpoch [48/200] | Train Loss: 0.2527, Train Acc: 0.9057 | \nEpoch [49/200] | Train Loss: 0.2488, Train Acc: 0.8923 | \nEpoch [50/200] | Train Loss: 0.2215, Train Acc: 0.9192 | \nEpoch [51/200] | Train Loss: 0.2366, Train Acc: 0.9091 | \nEpoch [52/200] | Train Loss: 0.1788, Train Acc: 0.9394 | \nEpoch [53/200] | Train Loss: 0.1688, Train Acc: 0.9428 | \nEpoch [54/200] | Train Loss: 0.1617, Train Acc: 0.9428 | \nEpoch [55/200] | Train Loss: 0.1673, Train Acc: 0.9428 | \nEpoch [56/200] | Train Loss: 0.1601, Train Acc: 0.9461 | \nEpoch [57/200] | Train Loss: 0.1771, Train Acc: 0.9327 | \nEpoch [58/200] | Train Loss: 0.1906, Train Acc: 0.9360 | \nEpoch [59/200] | Train Loss: 0.1928, Train Acc: 0.9293 | \nEpoch [60/200] | Train Loss: 0.3725, Train Acc: 0.8249 | \nEpoch [61/200] | Train Loss: 0.3522, Train Acc: 0.8384 | \nEpoch [62/200] | Train Loss: 0.2664, Train Acc: 0.8956 | \nEpoch [63/200] | Train Loss: 0.2411, Train Acc: 0.8956 | \nEpoch [64/200] | Train Loss: 0.2410, Train Acc: 0.9125 | \nEpoch [65/200] | Train Loss: 0.2877, Train Acc: 0.8822 | \nEpoch [66/200] | Train Loss: 0.2495, Train Acc: 0.9057 | \nEpoch 00067: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [67/200] | Train Loss: 0.2261, Train Acc: 0.9125 | \nEpoch [68/200] | Train Loss: 0.1964, Train Acc: 0.9293 | \nEpoch [69/200] | Train Loss: 0.1889, Train Acc: 0.9360 | \nEpoch [70/200] | Train Loss: 0.1767, Train Acc: 0.9360 | \nEpoch [71/200] | Train Loss: 0.1628, Train Acc: 0.9461 | \nEpoch [72/200] | Train Loss: 0.1639, Train Acc: 0.9461 | \nEpoch [73/200] | Train Loss: 0.1503, Train Acc: 0.9596 | \nEpoch [74/200] | Train Loss: 0.1544, Train Acc: 0.9562 | \nEpoch [75/200] | Train Loss: 0.1525, Train Acc: 0.9562 | \nEpoch [76/200] | Train Loss: 0.1493, Train Acc: 0.9562 | \nEpoch [77/200] | Train Loss: 0.1436, Train Acc: 0.9596 | \nEpoch [78/200] | Train Loss: 0.1590, Train Acc: 0.9529 | \nEpoch [79/200] | Train Loss: 0.1411, Train Acc: 0.9562 | \nEpoch [80/200] | Train Loss: 0.1446, Train Acc: 0.9562 | \nEpoch [81/200] | Train Loss: 0.1392, Train Acc: 0.9596 | \nEpoch [82/200] | Train Loss: 0.1364, Train Acc: 0.9596 | \nEpoch [83/200] | Train Loss: 0.1415, Train Acc: 0.9596 | \nEpoch [84/200] | Train Loss: 0.1279, Train Acc: 0.9630 | \nEpoch [85/200] | Train Loss: 0.1399, Train Acc: 0.9596 | \nEpoch [86/200] | Train Loss: 0.1353, Train Acc: 0.9630 | \nEpoch [87/200] | Train Loss: 0.1397, Train Acc: 0.9596 | \nEpoch [88/200] | Train Loss: 0.1281, Train Acc: 0.9596 | \nEpoch [89/200] | Train Loss: 0.1410, Train Acc: 0.9495 | \nEpoch [90/200] | Train Loss: 0.1299, Train Acc: 0.9562 | \nEpoch [91/200] | Train Loss: 0.1314, Train Acc: 0.9562 | \nEpoch [92/200] | Train Loss: 0.1305, Train Acc: 0.9596 | \nEpoch [93/200] | Train Loss: 0.1203, Train Acc: 0.9663 | \nEpoch [94/200] | Train Loss: 0.1280, Train Acc: 0.9630 | \nEpoch [95/200] | Train Loss: 0.1447, Train Acc: 0.9529 | \nEpoch [96/200] | Train Loss: 0.1223, Train Acc: 0.9663 | \nEpoch [97/200] | Train Loss: 0.1268, Train Acc: 0.9630 | \nEpoch [98/200] | Train Loss: 0.1354, Train Acc: 0.9495 | \nEpoch [99/200] | Train Loss: 0.1257, Train Acc: 0.9663 | \nEpoch [100/200] | Train Loss: 0.1358, Train Acc: 0.9630 | \nEpoch [101/200] | Train Loss: 0.1165, Train Acc: 0.9697 | \nEpoch [102/200] | Train Loss: 0.1203, Train Acc: 0.9663 | \nEpoch [103/200] | Train Loss: 0.1441, Train Acc: 0.9596 | \nEpoch [104/200] | Train Loss: 0.1348, Train Acc: 0.9663 | \nEpoch [105/200] | Train Loss: 0.1178, Train Acc: 0.9697 | \nEpoch [106/200] | Train Loss: 0.1328, Train Acc: 0.9562 | \nEpoch [107/200] | Train Loss: 0.1317, Train Acc: 0.9529 | \nEpoch [108/200] | Train Loss: 0.1290, Train Acc: 0.9596 | \nEpoch [109/200] | Train Loss: 0.1135, Train Acc: 0.9596 | \nEpoch [110/200] | Train Loss: 0.1118, Train Acc: 0.9697 | \nEpoch [111/200] | Train Loss: 0.1103, Train Acc: 0.9731 | \nEpoch [112/200] | Train Loss: 0.1056, Train Acc: 0.9697 | \nEpoch [113/200] | Train Loss: 0.1119, Train Acc: 0.9663 | \nEpoch [114/200] | Train Loss: 0.1080, Train Acc: 0.9697 | \nEpoch [115/200] | Train Loss: 0.1268, Train Acc: 0.9630 | \nEpoch [116/200] | Train Loss: 0.1149, Train Acc: 0.9663 | \nEpoch [117/200] | Train Loss: 0.1000, Train Acc: 0.9697 | \nEpoch [118/200] | Train Loss: 0.1112, Train Acc: 0.9697 | \nEpoch [119/200] | Train Loss: 0.1240, Train Acc: 0.9596 | \nEpoch [120/200] | Train Loss: 0.1038, Train Acc: 0.9697 | \nEpoch [121/200] | Train Loss: 0.1148, Train Acc: 0.9630 | \nEpoch [122/200] | Train Loss: 0.1085, Train Acc: 0.9697 | \nEpoch [123/200] | Train Loss: 0.1075, Train Acc: 0.9697 | \nEpoch [124/200] | Train Loss: 0.1087, Train Acc: 0.9663 | \nEpoch [125/200] | Train Loss: 0.0952, Train Acc: 0.9764 | \nEpoch [126/200] | Train Loss: 0.1224, Train Acc: 0.9663 | \nEpoch [127/200] | Train Loss: 0.1037, Train Acc: 0.9663 | \nEpoch [128/200] | Train Loss: 0.1365, Train Acc: 0.9596 | \nEpoch [129/200] | Train Loss: 0.1118, Train Acc: 0.9630 | \nEpoch [130/200] | Train Loss: 0.1068, Train Acc: 0.9697 | \nEpoch [131/200] | Train Loss: 0.1051, Train Acc: 0.9731 | \nEpoch [132/200] | Train Loss: 0.1038, Train Acc: 0.9697 | \nEpoch [133/200] | Train Loss: 0.0953, Train Acc: 0.9731 | \nEpoch [134/200] | Train Loss: 0.1203, Train Acc: 0.9630 | \nEpoch [135/200] | Train Loss: 0.1053, Train Acc: 0.9663 | \nEpoch 00136: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [136/200] | Train Loss: 0.1111, Train Acc: 0.9663 | \nEpoch [137/200] | Train Loss: 0.1075, Train Acc: 0.9697 | \nEpoch [138/200] | Train Loss: 0.1017, Train Acc: 0.9731 | \nEpoch [139/200] | Train Loss: 0.0985, Train Acc: 0.9697 | \nEpoch [140/200] | Train Loss: 0.0999, Train Acc: 0.9663 | \nEpoch [141/200] | Train Loss: 0.1097, Train Acc: 0.9663 | \nEpoch [142/200] | Train Loss: 0.0945, Train Acc: 0.9731 | \nEpoch [143/200] | Train Loss: 0.0974, Train Acc: 0.9697 | \nEpoch [144/200] | Train Loss: 0.0932, Train Acc: 0.9731 | \nEarly stopping triggered.\nTest Loss: 0.2932, Test Accuracy: 0.9200, Test AUC: 0.9445\n\n--- Processing: fc_all_bands ---\nShape: (372, 1026)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7817, Train Acc: 0.4680 | \nEpoch [2/200] | Train Loss: 0.6932, Train Acc: 0.5185 | \nEpoch [3/200] | Train Loss: 0.6982, Train Acc: 0.4882 | \nEpoch [4/200] | Train Loss: 0.7035, Train Acc: 0.4848 | \nEpoch [5/200] | Train Loss: 0.6948, Train Acc: 0.4949 | \nEpoch [6/200] | Train Loss: 0.6912, Train Acc: 0.5522 | \nEpoch [7/200] | Train Loss: 0.6796, Train Acc: 0.5488 | \nEpoch [8/200] | Train Loss: 0.6621, Train Acc: 0.6128 | \nEpoch [9/200] | Train Loss: 0.5750, Train Acc: 0.6936 | \nEpoch [10/200] | Train Loss: 0.5593, Train Acc: 0.6970 | \nEpoch [11/200] | Train Loss: 0.5222, Train Acc: 0.7205 | \nEpoch [12/200] | Train Loss: 0.4225, Train Acc: 0.7879 | \nEpoch [13/200] | Train Loss: 0.3885, Train Acc: 0.8283 | \nEpoch [14/200] | Train Loss: 0.4471, Train Acc: 0.7946 | \nEpoch [15/200] | Train Loss: 0.4426, Train Acc: 0.8182 | \nEpoch [16/200] | Train Loss: 0.3505, Train Acc: 0.8519 | \nEpoch [17/200] | Train Loss: 0.3086, Train Acc: 0.8754 | \nEpoch [18/200] | Train Loss: 0.2504, Train Acc: 0.8956 | \nEpoch [19/200] | Train Loss: 0.2536, Train Acc: 0.9091 | \nEpoch [20/200] | Train Loss: 0.3797, Train Acc: 0.8451 | \nEpoch [21/200] | Train Loss: 0.2635, Train Acc: 0.9057 | \nEpoch [22/200] | Train Loss: 0.2189, Train Acc: 0.9259 | \nEpoch [23/200] | Train Loss: 0.2254, Train Acc: 0.8990 | \nEpoch [24/200] | Train Loss: 0.2116, Train Acc: 0.9192 | \nEpoch [25/200] | Train Loss: 0.2043, Train Acc: 0.9057 | \nEpoch [26/200] | Train Loss: 0.1812, Train Acc: 0.9192 | \nEpoch [27/200] | Train Loss: 0.1830, Train Acc: 0.9192 | \nEpoch [28/200] | Train Loss: 0.2071, Train Acc: 0.9091 | \nEpoch [29/200] | Train Loss: 0.2154, Train Acc: 0.9259 | \nEpoch [30/200] | Train Loss: 0.1465, Train Acc: 0.9495 | \nEpoch [31/200] | Train Loss: 0.1540, Train Acc: 0.9327 | \nEpoch [32/200] | Train Loss: 0.4371, Train Acc: 0.8013 | \nEpoch [33/200] | Train Loss: 0.3694, Train Acc: 0.8283 | \nEpoch [34/200] | Train Loss: 0.2887, Train Acc: 0.8889 | \nEpoch [35/200] | Train Loss: 0.1954, Train Acc: 0.9158 | \nEpoch [36/200] | Train Loss: 0.1902, Train Acc: 0.9461 | \nEpoch [37/200] | Train Loss: 0.2076, Train Acc: 0.9091 | \nEpoch [38/200] | Train Loss: 0.1718, Train Acc: 0.9259 | \nEpoch [39/200] | Train Loss: 0.1322, Train Acc: 0.9562 | \nEpoch [40/200] | Train Loss: 0.1178, Train Acc: 0.9562 | \nEpoch [41/200] | Train Loss: 0.1135, Train Acc: 0.9562 | \nEpoch [42/200] | Train Loss: 0.0920, Train Acc: 0.9697 | \nEpoch [43/200] | Train Loss: 0.1943, Train Acc: 0.9259 | \nEpoch [44/200] | Train Loss: 0.1077, Train Acc: 0.9663 | \nEpoch [45/200] | Train Loss: 0.1604, Train Acc: 0.9259 | \nEpoch [46/200] | Train Loss: 0.1410, Train Acc: 0.9394 | \nEpoch [47/200] | Train Loss: 0.1792, Train Acc: 0.9259 | \nEpoch [48/200] | Train Loss: 0.1215, Train Acc: 0.9596 | \nEpoch [49/200] | Train Loss: 0.1119, Train Acc: 0.9461 | \nEpoch [50/200] | Train Loss: 0.1314, Train Acc: 0.9394 | \nEpoch [51/200] | Train Loss: 0.1191, Train Acc: 0.9630 | \nEpoch [52/200] | Train Loss: 0.0756, Train Acc: 0.9764 | \nEpoch [53/200] | Train Loss: 0.0755, Train Acc: 0.9596 | \nEpoch [54/200] | Train Loss: 0.2074, Train Acc: 0.9327 | \nEpoch [55/200] | Train Loss: 0.1667, Train Acc: 0.9226 | \nEpoch [56/200] | Train Loss: 0.2863, Train Acc: 0.9057 | \nEpoch [57/200] | Train Loss: 0.1773, Train Acc: 0.9360 | \nEpoch [58/200] | Train Loss: 0.1617, Train Acc: 0.9428 | \nEpoch [59/200] | Train Loss: 0.1404, Train Acc: 0.9529 | \nEpoch [60/200] | Train Loss: 0.1391, Train Acc: 0.9630 | \nEpoch [61/200] | Train Loss: 0.1010, Train Acc: 0.9663 | \nEpoch [62/200] | Train Loss: 0.1639, Train Acc: 0.9293 | \nEpoch [63/200] | Train Loss: 0.1374, Train Acc: 0.9562 | \nEpoch 00064: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [64/200] | Train Loss: 0.0941, Train Acc: 0.9562 | \nEpoch [65/200] | Train Loss: 0.0844, Train Acc: 0.9832 | \nEpoch [66/200] | Train Loss: 0.0507, Train Acc: 0.9764 | \nEpoch [67/200] | Train Loss: 0.0407, Train Acc: 0.9832 | \nEpoch [68/200] | Train Loss: 0.0390, Train Acc: 0.9865 | \nEpoch [69/200] | Train Loss: 0.0200, Train Acc: 0.9966 | \nEpoch [70/200] | Train Loss: 0.0336, Train Acc: 0.9966 | \nEpoch [71/200] | Train Loss: 0.0197, Train Acc: 1.0000 | \nEpoch [72/200] | Train Loss: 0.0181, Train Acc: 0.9966 | \nEpoch [73/200] | Train Loss: 0.0151, Train Acc: 1.0000 | \nEpoch [74/200] | Train Loss: 0.0192, Train Acc: 0.9966 | \nEpoch [75/200] | Train Loss: 0.0164, Train Acc: 0.9966 | \nEpoch [76/200] | Train Loss: 0.0150, Train Acc: 0.9966 | \nEpoch [77/200] | Train Loss: 0.0128, Train Acc: 1.0000 | \nEpoch [78/200] | Train Loss: 0.0205, Train Acc: 0.9933 | \nEpoch [79/200] | Train Loss: 0.0165, Train Acc: 0.9966 | \nEpoch [80/200] | Train Loss: 0.0129, Train Acc: 0.9966 | \nEpoch [81/200] | Train Loss: 0.0102, Train Acc: 1.0000 | \nEpoch [82/200] | Train Loss: 0.0152, Train Acc: 0.9933 | \nEpoch [83/200] | Train Loss: 0.0111, Train Acc: 1.0000 | \nEpoch [84/200] | Train Loss: 0.0075, Train Acc: 1.0000 | \nEpoch [85/200] | Train Loss: 0.0089, Train Acc: 1.0000 | \nEpoch [86/200] | Train Loss: 0.0078, Train Acc: 1.0000 | \nEpoch [87/200] | Train Loss: 0.0063, Train Acc: 1.0000 | \nEpoch [88/200] | Train Loss: 0.0071, Train Acc: 1.0000 | \nEpoch [89/200] | Train Loss: 0.0063, Train Acc: 1.0000 | \nEpoch [90/200] | Train Loss: 0.0057, Train Acc: 1.0000 | \nEarly stopping triggered.\nTest Loss: 0.8938, Test Accuracy: 0.8267, Test AUC: 0.8755\n\n--- Processing: psd_fc_all_bands ---\nShape: (372, 1140)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7284, Train Acc: 0.5017 | \nEpoch [2/200] | Train Loss: 0.7097, Train Acc: 0.5017 | \nEpoch [3/200] | Train Loss: 0.6994, Train Acc: 0.5051 | \nEpoch [4/200] | Train Loss: 0.6939, Train Acc: 0.5152 | \nEpoch [5/200] | Train Loss: 0.6946, Train Acc: 0.4781 | \nEpoch [6/200] | Train Loss: 0.7110, Train Acc: 0.4377 | \nEpoch [7/200] | Train Loss: 0.6870, Train Acc: 0.5926 | \nEpoch [8/200] | Train Loss: 0.6938, Train Acc: 0.5253 | \nEpoch [9/200] | Train Loss: 0.7035, Train Acc: 0.5320 | \nEpoch [10/200] | Train Loss: 0.6694, Train Acc: 0.5623 | \nEpoch [11/200] | Train Loss: 0.6565, Train Acc: 0.6128 | \nEpoch [12/200] | Train Loss: 0.6060, Train Acc: 0.6902 | \nEpoch [13/200] | Train Loss: 0.5845, Train Acc: 0.7037 | \nEpoch [14/200] | Train Loss: 0.5837, Train Acc: 0.7003 | \nEpoch [15/200] | Train Loss: 0.5285, Train Acc: 0.7576 | \nEpoch [16/200] | Train Loss: 0.5022, Train Acc: 0.7609 | \nEpoch [17/200] | Train Loss: 0.4973, Train Acc: 0.7677 | \nEpoch [18/200] | Train Loss: 0.4946, Train Acc: 0.7643 | \nEpoch [19/200] | Train Loss: 0.4337, Train Acc: 0.8114 | \nEpoch [20/200] | Train Loss: 0.4232, Train Acc: 0.8182 | \nEpoch [21/200] | Train Loss: 0.3771, Train Acc: 0.8418 | \nEpoch [22/200] | Train Loss: 0.4335, Train Acc: 0.8148 | \nEpoch [23/200] | Train Loss: 0.3980, Train Acc: 0.8316 | \nEpoch [24/200] | Train Loss: 0.3654, Train Acc: 0.8384 | \nEpoch [25/200] | Train Loss: 0.3299, Train Acc: 0.8586 | \nEpoch [26/200] | Train Loss: 0.2941, Train Acc: 0.8822 | \nEpoch [27/200] | Train Loss: 0.8500, Train Acc: 0.6027 | \nEpoch [28/200] | Train Loss: 0.6323, Train Acc: 0.6263 | \nEpoch [29/200] | Train Loss: 0.5108, Train Acc: 0.7340 | \nEpoch [30/200] | Train Loss: 0.4329, Train Acc: 0.8114 | \nEpoch [31/200] | Train Loss: 0.4546, Train Acc: 0.7778 | \nEpoch [32/200] | Train Loss: 0.3987, Train Acc: 0.7980 | \nEpoch [33/200] | Train Loss: 0.3611, Train Acc: 0.8081 | \nEpoch [34/200] | Train Loss: 0.3415, Train Acc: 0.8485 | \nEpoch [35/200] | Train Loss: 0.2901, Train Acc: 0.8923 | \nEpoch [36/200] | Train Loss: 0.2719, Train Acc: 0.8822 | \nEpoch [37/200] | Train Loss: 0.3131, Train Acc: 0.8788 | \nEpoch [38/200] | Train Loss: 0.2690, Train Acc: 0.8889 | \nEpoch [39/200] | Train Loss: 0.2096, Train Acc: 0.9259 | \nEpoch [40/200] | Train Loss: 0.2797, Train Acc: 0.8855 | \nEpoch [41/200] | Train Loss: 0.3795, Train Acc: 0.8316 | \nEpoch [42/200] | Train Loss: 0.2573, Train Acc: 0.8855 | \nEpoch [43/200] | Train Loss: 0.1810, Train Acc: 0.9327 | \nEpoch [44/200] | Train Loss: 0.1473, Train Acc: 0.9461 | \nEpoch [45/200] | Train Loss: 0.2276, Train Acc: 0.9057 | \nEpoch [46/200] | Train Loss: 0.3206, Train Acc: 0.8519 | \nEpoch [47/200] | Train Loss: 0.3527, Train Acc: 0.8418 | \nEpoch [48/200] | Train Loss: 0.2674, Train Acc: 0.8889 | \nEpoch [49/200] | Train Loss: 0.1795, Train Acc: 0.9226 | \nEpoch [50/200] | Train Loss: 0.1891, Train Acc: 0.9091 | \nEpoch [51/200] | Train Loss: 0.1688, Train Acc: 0.9461 | \nEpoch [52/200] | Train Loss: 0.2501, Train Acc: 0.8923 | \nEpoch [53/200] | Train Loss: 0.2209, Train Acc: 0.9293 | \nEpoch [54/200] | Train Loss: 0.2469, Train Acc: 0.8956 | \nEpoch 00055: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [55/200] | Train Loss: 0.1630, Train Acc: 0.9394 | \nEpoch [56/200] | Train Loss: 0.1636, Train Acc: 0.9360 | \nEpoch [57/200] | Train Loss: 0.1234, Train Acc: 0.9630 | \nEpoch [58/200] | Train Loss: 0.1134, Train Acc: 0.9731 | \nEpoch [59/200] | Train Loss: 0.1031, Train Acc: 0.9764 | \nEpoch [60/200] | Train Loss: 0.1041, Train Acc: 0.9764 | \nEpoch [61/200] | Train Loss: 0.0952, Train Acc: 0.9798 | \nEpoch [62/200] | Train Loss: 0.1078, Train Acc: 0.9663 | \nEpoch [63/200] | Train Loss: 0.0889, Train Acc: 0.9798 | \nEpoch [64/200] | Train Loss: 0.0891, Train Acc: 0.9697 | \nEpoch [65/200] | Train Loss: 0.0886, Train Acc: 0.9764 | \nEpoch [66/200] | Train Loss: 0.0728, Train Acc: 0.9798 | \nEpoch [67/200] | Train Loss: 0.0723, Train Acc: 0.9832 | \nEpoch [68/200] | Train Loss: 0.0811, Train Acc: 0.9798 | \nEpoch [69/200] | Train Loss: 0.0637, Train Acc: 0.9899 | \nEpoch [70/200] | Train Loss: 0.0759, Train Acc: 0.9865 | \nEpoch [71/200] | Train Loss: 0.0679, Train Acc: 0.9832 | \nEpoch [72/200] | Train Loss: 0.0576, Train Acc: 0.9865 | \nEpoch [73/200] | Train Loss: 0.0628, Train Acc: 0.9832 | \nEpoch [74/200] | Train Loss: 0.0713, Train Acc: 0.9798 | \nEpoch [75/200] | Train Loss: 0.0599, Train Acc: 0.9865 | \nEpoch [76/200] | Train Loss: 0.0676, Train Acc: 0.9832 | \nEpoch [77/200] | Train Loss: 0.0550, Train Acc: 0.9899 | \nEpoch [78/200] | Train Loss: 0.0614, Train Acc: 0.9899 | \nEpoch [79/200] | Train Loss: 0.0555, Train Acc: 0.9865 | \nEpoch [80/200] | Train Loss: 0.0557, Train Acc: 0.9832 | \nEpoch [81/200] | Train Loss: 0.0564, Train Acc: 0.9865 | \nEpoch [82/200] | Train Loss: 0.0475, Train Acc: 0.9933 | \nEpoch [83/200] | Train Loss: 0.0577, Train Acc: 0.9865 | \nEpoch [84/200] | Train Loss: 0.0468, Train Acc: 0.9899 | \nEpoch [85/200] | Train Loss: 0.0441, Train Acc: 0.9899 | \nEpoch [86/200] | Train Loss: 0.0439, Train Acc: 0.9933 | \nEpoch [87/200] | Train Loss: 0.0562, Train Acc: 0.9899 | \nEpoch [88/200] | Train Loss: 0.0478, Train Acc: 0.9899 | \nEpoch [89/200] | Train Loss: 0.0456, Train Acc: 0.9933 | \nEpoch [90/200] | Train Loss: 0.0442, Train Acc: 0.9899 | \nEpoch [91/200] | Train Loss: 0.0490, Train Acc: 0.9832 | \nEpoch [92/200] | Train Loss: 0.0451, Train Acc: 0.9933 | \nEpoch [93/200] | Train Loss: 0.0404, Train Acc: 0.9933 | \nEpoch [94/200] | Train Loss: 0.0384, Train Acc: 0.9899 | \nEpoch [95/200] | Train Loss: 0.0401, Train Acc: 0.9933 | \nEpoch [96/200] | Train Loss: 0.0394, Train Acc: 0.9899 | \nEpoch [97/200] | Train Loss: 0.0415, Train Acc: 0.9933 | \nEpoch [98/200] | Train Loss: 0.0386, Train Acc: 0.9899 | \nEpoch [99/200] | Train Loss: 0.0387, Train Acc: 0.9899 | \nEpoch [100/200] | Train Loss: 0.0320, Train Acc: 0.9966 | \nEpoch [101/200] | Train Loss: 0.0379, Train Acc: 0.9899 | \nEpoch [102/200] | Train Loss: 0.0288, Train Acc: 0.9966 | \nEpoch [103/200] | Train Loss: 0.0326, Train Acc: 0.9933 | \nEpoch [104/200] | Train Loss: 0.0298, Train Acc: 0.9966 | \nEpoch [105/200] | Train Loss: 0.0259, Train Acc: 0.9966 | \nEpoch [106/200] | Train Loss: 0.0271, Train Acc: 0.9966 | \nEpoch [107/200] | Train Loss: 0.0288, Train Acc: 0.9933 | \nEpoch [108/200] | Train Loss: 0.0328, Train Acc: 0.9966 | \nEpoch [109/200] | Train Loss: 0.0283, Train Acc: 0.9933 | \nEpoch [110/200] | Train Loss: 0.0269, Train Acc: 0.9966 | \nEpoch [111/200] | Train Loss: 0.0280, Train Acc: 0.9966 | \nEpoch [112/200] | Train Loss: 0.0436, Train Acc: 0.9899 | \nEpoch [113/200] | Train Loss: 0.0405, Train Acc: 0.9832 | \nEpoch [114/200] | Train Loss: 0.0287, Train Acc: 0.9899 | \nEpoch [115/200] | Train Loss: 0.0269, Train Acc: 0.9966 | \nEpoch 00116: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [116/200] | Train Loss: 0.0310, Train Acc: 0.9933 | \nEpoch [117/200] | Train Loss: 0.0266, Train Acc: 0.9966 | \nEpoch [118/200] | Train Loss: 0.0261, Train Acc: 0.9966 | \nEpoch [119/200] | Train Loss: 0.0254, Train Acc: 0.9966 | \nEarly stopping triggered.\nTest Loss: 0.8121, Test Accuracy: 0.8133, Test AUC: 0.8535\n\n--- Processing: psd_delta ---\nShape: (372, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7025, Train Acc: 0.5421 | \nEpoch [2/200] | Train Loss: 0.6962, Train Acc: 0.5421 | \nEpoch [3/200] | Train Loss: 0.6164, Train Acc: 0.6869 | \nEpoch [4/200] | Train Loss: 0.5924, Train Acc: 0.6700 | \nEpoch [5/200] | Train Loss: 0.5240, Train Acc: 0.7475 | \nEpoch [6/200] | Train Loss: 0.4517, Train Acc: 0.7845 | \nEpoch [7/200] | Train Loss: 0.4398, Train Acc: 0.7912 | \nEpoch [8/200] | Train Loss: 0.4289, Train Acc: 0.8047 | \nEpoch [9/200] | Train Loss: 0.3813, Train Acc: 0.8182 | \nEpoch [10/200] | Train Loss: 0.4146, Train Acc: 0.8148 | \nEpoch [11/200] | Train Loss: 0.3814, Train Acc: 0.8283 | \nEpoch [12/200] | Train Loss: 0.3534, Train Acc: 0.8485 | \nEpoch [13/200] | Train Loss: 0.4125, Train Acc: 0.8114 | \nEpoch [14/200] | Train Loss: 0.4650, Train Acc: 0.7912 | \nEpoch [15/200] | Train Loss: 0.3927, Train Acc: 0.8047 | \nEpoch [16/200] | Train Loss: 0.3699, Train Acc: 0.8316 | \nEpoch [17/200] | Train Loss: 0.3643, Train Acc: 0.8350 | \nEpoch [18/200] | Train Loss: 0.3743, Train Acc: 0.8215 | \nEpoch [19/200] | Train Loss: 0.3188, Train Acc: 0.8586 | \nEpoch [20/200] | Train Loss: 0.3334, Train Acc: 0.8451 | \nEpoch [21/200] | Train Loss: 0.3859, Train Acc: 0.8249 | \nEpoch [22/200] | Train Loss: 0.3865, Train Acc: 0.8114 | \nEpoch [23/200] | Train Loss: 0.3648, Train Acc: 0.8384 | \nEpoch [24/200] | Train Loss: 0.3453, Train Acc: 0.8519 | \nEpoch [25/200] | Train Loss: 0.3178, Train Acc: 0.8586 | \nEpoch [26/200] | Train Loss: 0.2900, Train Acc: 0.8687 | \nEpoch [27/200] | Train Loss: 0.3356, Train Acc: 0.8586 | \nEpoch [28/200] | Train Loss: 0.2964, Train Acc: 0.8788 | \nEpoch [29/200] | Train Loss: 0.3309, Train Acc: 0.8586 | \nEpoch [30/200] | Train Loss: 0.2932, Train Acc: 0.8788 | \nEpoch [31/200] | Train Loss: 0.3845, Train Acc: 0.8316 | \nEpoch [32/200] | Train Loss: 0.3647, Train Acc: 0.8283 | \nEpoch [33/200] | Train Loss: 0.3533, Train Acc: 0.8485 | \nEpoch [34/200] | Train Loss: 0.3293, Train Acc: 0.8485 | \nEpoch [35/200] | Train Loss: 0.4010, Train Acc: 0.8350 | \nEpoch [36/200] | Train Loss: 0.3257, Train Acc: 0.8620 | \nEpoch 00037: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [37/200] | Train Loss: 0.3161, Train Acc: 0.8620 | \nEpoch [38/200] | Train Loss: 0.3161, Train Acc: 0.8620 | \nEpoch [39/200] | Train Loss: 0.2815, Train Acc: 0.8721 | \nEpoch [40/200] | Train Loss: 0.2765, Train Acc: 0.8822 | \nEpoch [41/200] | Train Loss: 0.2544, Train Acc: 0.8754 | \nEpoch [42/200] | Train Loss: 0.2375, Train Acc: 0.9057 | \nEpoch [43/200] | Train Loss: 0.2386, Train Acc: 0.8923 | \nEpoch [44/200] | Train Loss: 0.2371, Train Acc: 0.8990 | \nEpoch [45/200] | Train Loss: 0.2306, Train Acc: 0.9057 | \nEpoch [46/200] | Train Loss: 0.2212, Train Acc: 0.9024 | \nEpoch [47/200] | Train Loss: 0.2268, Train Acc: 0.9057 | \nEpoch [48/200] | Train Loss: 0.2033, Train Acc: 0.9158 | \nEpoch [49/200] | Train Loss: 0.2261, Train Acc: 0.9024 | \nEpoch [50/200] | Train Loss: 0.2100, Train Acc: 0.9057 | \nEpoch [51/200] | Train Loss: 0.1856, Train Acc: 0.9158 | \nEpoch [52/200] | Train Loss: 0.1918, Train Acc: 0.9091 | \nEpoch [53/200] | Train Loss: 0.1949, Train Acc: 0.9192 | \nEpoch [54/200] | Train Loss: 0.2077, Train Acc: 0.8956 | \nEpoch [55/200] | Train Loss: 0.1786, Train Acc: 0.9226 | \nEpoch [56/200] | Train Loss: 0.1840, Train Acc: 0.9192 | \nEpoch [57/200] | Train Loss: 0.1729, Train Acc: 0.9293 | \nEpoch [58/200] | Train Loss: 0.1750, Train Acc: 0.9293 | \nEpoch [59/200] | Train Loss: 0.1741, Train Acc: 0.9192 | \nEpoch [60/200] | Train Loss: 0.1544, Train Acc: 0.9327 | \nEpoch [61/200] | Train Loss: 0.1430, Train Acc: 0.9327 | \nEpoch [62/200] | Train Loss: 0.1471, Train Acc: 0.9495 | \nEpoch [63/200] | Train Loss: 0.1443, Train Acc: 0.9428 | \nEpoch [64/200] | Train Loss: 0.1242, Train Acc: 0.9562 | \nEpoch [65/200] | Train Loss: 0.1362, Train Acc: 0.9461 | \nEpoch [66/200] | Train Loss: 0.1355, Train Acc: 0.9495 | \nEpoch [67/200] | Train Loss: 0.1462, Train Acc: 0.9495 | \nEpoch [68/200] | Train Loss: 0.1347, Train Acc: 0.9562 | \nEpoch [69/200] | Train Loss: 0.1209, Train Acc: 0.9697 | \nEpoch [70/200] | Train Loss: 0.1379, Train Acc: 0.9461 | \nEpoch [71/200] | Train Loss: 0.1361, Train Acc: 0.9529 | \nEpoch [72/200] | Train Loss: 0.1372, Train Acc: 0.9495 | \nEpoch [73/200] | Train Loss: 0.1055, Train Acc: 0.9562 | \nEpoch [74/200] | Train Loss: 0.0985, Train Acc: 0.9697 | \nEpoch [75/200] | Train Loss: 0.1174, Train Acc: 0.9495 | \nEpoch [76/200] | Train Loss: 0.1225, Train Acc: 0.9461 | \nEpoch [77/200] | Train Loss: 0.1073, Train Acc: 0.9529 | \nEpoch [78/200] | Train Loss: 0.1087, Train Acc: 0.9495 | \nEpoch [79/200] | Train Loss: 0.1131, Train Acc: 0.9394 | \nEpoch [80/200] | Train Loss: 0.0926, Train Acc: 0.9630 | \nEpoch [81/200] | Train Loss: 0.0912, Train Acc: 0.9731 | \nEpoch [82/200] | Train Loss: 0.1318, Train Acc: 0.9529 | \nEpoch [83/200] | Train Loss: 0.1028, Train Acc: 0.9697 | \nEpoch [84/200] | Train Loss: 0.0769, Train Acc: 0.9832 | \nEpoch [85/200] | Train Loss: 0.0905, Train Acc: 0.9697 | \nEpoch [86/200] | Train Loss: 0.0896, Train Acc: 0.9731 | \nEpoch [87/200] | Train Loss: 0.0697, Train Acc: 0.9798 | \nEpoch [88/200] | Train Loss: 0.1095, Train Acc: 0.9630 | \nEpoch [89/200] | Train Loss: 0.1007, Train Acc: 0.9596 | \nEpoch [90/200] | Train Loss: 0.0952, Train Acc: 0.9562 | \nEpoch [91/200] | Train Loss: 0.0829, Train Acc: 0.9731 | \nEpoch [92/200] | Train Loss: 0.0809, Train Acc: 0.9731 | \nEpoch [93/200] | Train Loss: 0.0962, Train Acc: 0.9697 | \nEpoch [94/200] | Train Loss: 0.0783, Train Acc: 0.9663 | \nEpoch [95/200] | Train Loss: 0.0675, Train Acc: 0.9697 | \nEpoch [96/200] | Train Loss: 0.0895, Train Acc: 0.9731 | \nEpoch [97/200] | Train Loss: 0.1045, Train Acc: 0.9630 | \nEpoch [98/200] | Train Loss: 0.0996, Train Acc: 0.9697 | \nEpoch [99/200] | Train Loss: 0.0592, Train Acc: 0.9798 | \nEpoch [100/200] | Train Loss: 0.0642, Train Acc: 0.9798 | \nEpoch [101/200] | Train Loss: 0.0561, Train Acc: 0.9798 | \nEpoch [102/200] | Train Loss: 0.0619, Train Acc: 0.9764 | \nEpoch [103/200] | Train Loss: 0.0997, Train Acc: 0.9731 | \nEpoch [104/200] | Train Loss: 0.0475, Train Acc: 0.9865 | \nEpoch [105/200] | Train Loss: 0.0937, Train Acc: 0.9529 | \nEpoch [106/200] | Train Loss: 0.0634, Train Acc: 0.9697 | \nEpoch [107/200] | Train Loss: 0.0599, Train Acc: 0.9764 | \nEpoch [108/200] | Train Loss: 0.0639, Train Acc: 0.9663 | \nEpoch [109/200] | Train Loss: 0.0576, Train Acc: 0.9764 | \nEpoch [110/200] | Train Loss: 0.1033, Train Acc: 0.9529 | \nEpoch [111/200] | Train Loss: 0.0447, Train Acc: 0.9865 | \nEpoch [112/200] | Train Loss: 0.0465, Train Acc: 0.9832 | \nEpoch [113/200] | Train Loss: 0.0514, Train Acc: 0.9764 | \nEpoch [114/200] | Train Loss: 0.0545, Train Acc: 0.9865 | \nEpoch [115/200] | Train Loss: 0.0555, Train Acc: 0.9764 | \nEpoch [116/200] | Train Loss: 0.0540, Train Acc: 0.9798 | \nEpoch [117/200] | Train Loss: 0.0651, Train Acc: 0.9663 | \nEpoch [118/200] | Train Loss: 0.0465, Train Acc: 0.9798 | \nEpoch [119/200] | Train Loss: 0.0318, Train Acc: 0.9933 | \nEpoch [120/200] | Train Loss: 0.0446, Train Acc: 0.9798 | \nEpoch [121/200] | Train Loss: 0.0603, Train Acc: 0.9798 | \nEpoch [122/200] | Train Loss: 0.0556, Train Acc: 0.9899 | \nEpoch [123/200] | Train Loss: 0.0425, Train Acc: 0.9865 | \nEpoch [124/200] | Train Loss: 0.0676, Train Acc: 0.9697 | \nEpoch [125/200] | Train Loss: 0.0672, Train Acc: 0.9764 | \nEpoch [126/200] | Train Loss: 0.0804, Train Acc: 0.9731 | \nEpoch [127/200] | Train Loss: 0.0982, Train Acc: 0.9562 | \nEpoch [128/200] | Train Loss: 0.0524, Train Acc: 0.9798 | \nEpoch [129/200] | Train Loss: 0.1018, Train Acc: 0.9663 | \nEpoch 00130: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [130/200] | Train Loss: 0.0600, Train Acc: 0.9764 | \nEpoch [131/200] | Train Loss: 0.0765, Train Acc: 0.9663 | \nEpoch [132/200] | Train Loss: 0.0468, Train Acc: 0.9798 | \nEpoch [133/200] | Train Loss: 0.0712, Train Acc: 0.9663 | \nEpoch [134/200] | Train Loss: 0.0465, Train Acc: 0.9899 | \nEpoch [135/200] | Train Loss: 0.0798, Train Acc: 0.9697 | \nEpoch [136/200] | Train Loss: 0.0571, Train Acc: 0.9731 | \nEpoch [137/200] | Train Loss: 0.0485, Train Acc: 0.9832 | \nEpoch [138/200] | Train Loss: 0.0456, Train Acc: 0.9832 | \nEarly stopping triggered.\nTest Loss: 0.2346, Test Accuracy: 0.9333, Test AUC: 0.9694\n\n--- Processing: psd_theta ---\nShape: (372, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7573, Train Acc: 0.5253 | \nEpoch [2/200] | Train Loss: 0.7145, Train Acc: 0.5118 | \nEpoch [3/200] | Train Loss: 0.6396, Train Acc: 0.6498 | \nEpoch [4/200] | Train Loss: 0.6007, Train Acc: 0.6700 | \nEpoch [5/200] | Train Loss: 0.5282, Train Acc: 0.7205 | \nEpoch [6/200] | Train Loss: 0.5214, Train Acc: 0.7205 | \nEpoch [7/200] | Train Loss: 0.5315, Train Acc: 0.7205 | \nEpoch [8/200] | Train Loss: 0.5382, Train Acc: 0.7239 | \nEpoch [9/200] | Train Loss: 0.5044, Train Acc: 0.7576 | \nEpoch [10/200] | Train Loss: 0.5255, Train Acc: 0.7306 | \nEpoch [11/200] | Train Loss: 0.5045, Train Acc: 0.7542 | \nEpoch [12/200] | Train Loss: 0.4693, Train Acc: 0.7946 | \nEpoch [13/200] | Train Loss: 0.4548, Train Acc: 0.7912 | \nEpoch [14/200] | Train Loss: 0.4382, Train Acc: 0.7980 | \nEpoch [15/200] | Train Loss: 0.4366, Train Acc: 0.7845 | \nEpoch [16/200] | Train Loss: 0.4546, Train Acc: 0.7811 | \nEpoch [17/200] | Train Loss: 0.4207, Train Acc: 0.7980 | \nEpoch [18/200] | Train Loss: 0.4145, Train Acc: 0.7845 | \nEpoch [19/200] | Train Loss: 0.4462, Train Acc: 0.7946 | \nEpoch [20/200] | Train Loss: 0.4332, Train Acc: 0.7946 | \nEpoch [21/200] | Train Loss: 0.4216, Train Acc: 0.8047 | \nEpoch [22/200] | Train Loss: 0.4450, Train Acc: 0.7811 | \nEpoch [23/200] | Train Loss: 0.3946, Train Acc: 0.8047 | \nEpoch [24/200] | Train Loss: 0.4199, Train Acc: 0.7879 | \nEpoch [25/200] | Train Loss: 0.4511, Train Acc: 0.7677 | \nEpoch [26/200] | Train Loss: 0.3743, Train Acc: 0.8013 | \nEpoch [27/200] | Train Loss: 0.3725, Train Acc: 0.8148 | \nEpoch [28/200] | Train Loss: 0.3787, Train Acc: 0.8215 | \nEpoch [29/200] | Train Loss: 0.3980, Train Acc: 0.8081 | \nEpoch [30/200] | Train Loss: 0.3816, Train Acc: 0.8047 | \nEpoch [31/200] | Train Loss: 0.4056, Train Acc: 0.8249 | \nEpoch [32/200] | Train Loss: 0.3694, Train Acc: 0.8182 | \nEpoch [33/200] | Train Loss: 0.3518, Train Acc: 0.8249 | \nEpoch [34/200] | Train Loss: 0.4534, Train Acc: 0.8047 | \nEpoch [35/200] | Train Loss: 0.4077, Train Acc: 0.8013 | \nEpoch [36/200] | Train Loss: 0.3789, Train Acc: 0.8350 | \nEpoch [37/200] | Train Loss: 0.3744, Train Acc: 0.8148 | \nEpoch [38/200] | Train Loss: 0.3960, Train Acc: 0.8013 | \nEpoch [39/200] | Train Loss: 0.3636, Train Acc: 0.8148 | \nEpoch [40/200] | Train Loss: 0.3896, Train Acc: 0.8182 | \nEpoch [41/200] | Train Loss: 0.3678, Train Acc: 0.8215 | \nEpoch [42/200] | Train Loss: 0.3670, Train Acc: 0.8215 | \nEpoch [43/200] | Train Loss: 0.3803, Train Acc: 0.8081 | \nEpoch 00044: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [44/200] | Train Loss: 0.3843, Train Acc: 0.8148 | \nEpoch [45/200] | Train Loss: 0.3476, Train Acc: 0.8283 | \nEpoch [46/200] | Train Loss: 0.3484, Train Acc: 0.8519 | \nEpoch [47/200] | Train Loss: 0.3168, Train Acc: 0.8519 | \nEpoch [48/200] | Train Loss: 0.3290, Train Acc: 0.8283 | \nEpoch [49/200] | Train Loss: 0.2990, Train Acc: 0.8653 | \nEpoch [50/200] | Train Loss: 0.3023, Train Acc: 0.8485 | \nEpoch [51/200] | Train Loss: 0.3053, Train Acc: 0.8653 | \nEpoch [52/200] | Train Loss: 0.3031, Train Acc: 0.8586 | \nEpoch [53/200] | Train Loss: 0.2910, Train Acc: 0.8653 | \nEpoch [54/200] | Train Loss: 0.2801, Train Acc: 0.8687 | \nEpoch [55/200] | Train Loss: 0.3010, Train Acc: 0.8485 | \nEpoch [56/200] | Train Loss: 0.2848, Train Acc: 0.8653 | \nEpoch [57/200] | Train Loss: 0.2703, Train Acc: 0.8855 | \nEpoch [58/200] | Train Loss: 0.2802, Train Acc: 0.8620 | \nEpoch [59/200] | Train Loss: 0.2821, Train Acc: 0.8485 | \nEpoch [60/200] | Train Loss: 0.2646, Train Acc: 0.8586 | \nEpoch [61/200] | Train Loss: 0.2743, Train Acc: 0.8687 | \nEpoch [62/200] | Train Loss: 0.2624, Train Acc: 0.8653 | \nEpoch [63/200] | Train Loss: 0.2487, Train Acc: 0.8754 | \nEpoch [64/200] | Train Loss: 0.2406, Train Acc: 0.8889 | \nEpoch [65/200] | Train Loss: 0.2478, Train Acc: 0.8620 | \nEpoch [66/200] | Train Loss: 0.2575, Train Acc: 0.8822 | \nEpoch [67/200] | Train Loss: 0.2538, Train Acc: 0.8788 | \nEpoch [68/200] | Train Loss: 0.2384, Train Acc: 0.8956 | \nEpoch [69/200] | Train Loss: 0.2320, Train Acc: 0.9024 | \nEpoch [70/200] | Train Loss: 0.2418, Train Acc: 0.8956 | \nEpoch [71/200] | Train Loss: 0.2384, Train Acc: 0.9057 | \nEpoch [72/200] | Train Loss: 0.2594, Train Acc: 0.8788 | \nEpoch [73/200] | Train Loss: 0.2283, Train Acc: 0.9125 | \nEpoch [74/200] | Train Loss: 0.2283, Train Acc: 0.8956 | \nEpoch [75/200] | Train Loss: 0.2195, Train Acc: 0.9125 | \nEpoch [76/200] | Train Loss: 0.2149, Train Acc: 0.9057 | \nEpoch [77/200] | Train Loss: 0.2418, Train Acc: 0.9057 | \nEpoch [78/200] | Train Loss: 0.1971, Train Acc: 0.9125 | \nEpoch [79/200] | Train Loss: 0.2433, Train Acc: 0.8721 | \nEpoch [80/200] | Train Loss: 0.2163, Train Acc: 0.8990 | \nEpoch [81/200] | Train Loss: 0.2178, Train Acc: 0.8923 | \nEpoch [82/200] | Train Loss: 0.2015, Train Acc: 0.9125 | \nEpoch [83/200] | Train Loss: 0.1894, Train Acc: 0.9259 | \nEpoch [84/200] | Train Loss: 0.2110, Train Acc: 0.9057 | \nEpoch [85/200] | Train Loss: 0.2128, Train Acc: 0.8990 | \nEpoch [86/200] | Train Loss: 0.1987, Train Acc: 0.9125 | \nEpoch [87/200] | Train Loss: 0.2088, Train Acc: 0.9259 | \nEpoch [88/200] | Train Loss: 0.1832, Train Acc: 0.9360 | \nEpoch [89/200] | Train Loss: 0.1708, Train Acc: 0.9259 | \nEpoch [90/200] | Train Loss: 0.1979, Train Acc: 0.9158 | \nEpoch [91/200] | Train Loss: 0.1658, Train Acc: 0.9259 | \nEpoch [92/200] | Train Loss: 0.2025, Train Acc: 0.9226 | \nEpoch [93/200] | Train Loss: 0.1988, Train Acc: 0.9192 | \nEpoch [94/200] | Train Loss: 0.1665, Train Acc: 0.9360 | \nEpoch [95/200] | Train Loss: 0.1534, Train Acc: 0.9428 | \nEpoch [96/200] | Train Loss: 0.1823, Train Acc: 0.9158 | \nEpoch [97/200] | Train Loss: 0.1598, Train Acc: 0.9327 | \nEpoch [98/200] | Train Loss: 0.1674, Train Acc: 0.9360 | \nEpoch [99/200] | Train Loss: 0.1642, Train Acc: 0.9360 | \nEpoch [100/200] | Train Loss: 0.1654, Train Acc: 0.9259 | \nEpoch [101/200] | Train Loss: 0.1728, Train Acc: 0.9259 | \nEpoch [102/200] | Train Loss: 0.2028, Train Acc: 0.9226 | \nEpoch [103/200] | Train Loss: 0.2274, Train Acc: 0.8889 | \nEpoch [104/200] | Train Loss: 0.2236, Train Acc: 0.9125 | \nEpoch [105/200] | Train Loss: 0.1981, Train Acc: 0.9226 | \nEpoch 00106: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [106/200] | Train Loss: 0.1621, Train Acc: 0.9259 | \nEpoch [107/200] | Train Loss: 0.1547, Train Acc: 0.9293 | \nEpoch [108/200] | Train Loss: 0.1835, Train Acc: 0.9158 | \nEpoch [109/200] | Train Loss: 0.1698, Train Acc: 0.9360 | \nEpoch [110/200] | Train Loss: 0.1500, Train Acc: 0.9461 | \nEpoch [111/200] | Train Loss: 0.1525, Train Acc: 0.9428 | \nEpoch [112/200] | Train Loss: 0.1360, Train Acc: 0.9529 | \nEpoch [113/200] | Train Loss: 0.1625, Train Acc: 0.9394 | \nEpoch [114/200] | Train Loss: 0.1375, Train Acc: 0.9495 | \nEpoch [115/200] | Train Loss: 0.1354, Train Acc: 0.9562 | \nEpoch [116/200] | Train Loss: 0.1507, Train Acc: 0.9293 | \nEpoch [117/200] | Train Loss: 0.1514, Train Acc: 0.9428 | \nEpoch [118/200] | Train Loss: 0.1672, Train Acc: 0.9226 | \nEpoch [119/200] | Train Loss: 0.1369, Train Acc: 0.9596 | \nEpoch [120/200] | Train Loss: 0.1384, Train Acc: 0.9495 | \nEpoch [121/200] | Train Loss: 0.1487, Train Acc: 0.9360 | \nEpoch [122/200] | Train Loss: 0.1660, Train Acc: 0.9293 | \nEpoch [123/200] | Train Loss: 0.1255, Train Acc: 0.9596 | \nEpoch [124/200] | Train Loss: 0.1323, Train Acc: 0.9529 | \nEpoch [125/200] | Train Loss: 0.1716, Train Acc: 0.9428 | \nEpoch [126/200] | Train Loss: 0.1319, Train Acc: 0.9461 | \nEpoch [127/200] | Train Loss: 0.1454, Train Acc: 0.9360 | \nEpoch [128/200] | Train Loss: 0.1430, Train Acc: 0.9529 | \nEpoch [129/200] | Train Loss: 0.1220, Train Acc: 0.9697 | \nEpoch [130/200] | Train Loss: 0.1548, Train Acc: 0.9293 | \nEpoch [131/200] | Train Loss: 0.1301, Train Acc: 0.9529 | \nEpoch [132/200] | Train Loss: 0.1487, Train Acc: 0.9495 | \nEpoch [133/200] | Train Loss: 0.1155, Train Acc: 0.9529 | \nEpoch [134/200] | Train Loss: 0.1482, Train Acc: 0.9428 | \nEpoch [135/200] | Train Loss: 0.1336, Train Acc: 0.9630 | \nEpoch [136/200] | Train Loss: 0.1269, Train Acc: 0.9529 | \nEpoch [137/200] | Train Loss: 0.1443, Train Acc: 0.9529 | \nEpoch [138/200] | Train Loss: 0.1253, Train Acc: 0.9529 | \nEpoch [139/200] | Train Loss: 0.1235, Train Acc: 0.9529 | \nEpoch [140/200] | Train Loss: 0.1435, Train Acc: 0.9394 | \nEpoch [141/200] | Train Loss: 0.1305, Train Acc: 0.9461 | \nEpoch [142/200] | Train Loss: 0.1167, Train Acc: 0.9663 | \nEpoch [143/200] | Train Loss: 0.1331, Train Acc: 0.9596 | \nEpoch 00144: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [144/200] | Train Loss: 0.1273, Train Acc: 0.9529 | \nEpoch [145/200] | Train Loss: 0.1160, Train Acc: 0.9630 | \nEpoch [146/200] | Train Loss: 0.1252, Train Acc: 0.9562 | \nEpoch [147/200] | Train Loss: 0.1139, Train Acc: 0.9630 | \nEpoch [148/200] | Train Loss: 0.1136, Train Acc: 0.9562 | \nEarly stopping triggered.\nTest Loss: 0.3500, Test Accuracy: 0.9067, Test AUC: 0.9395\n\n--- Processing: psd_alpha ---\nShape: (372, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7603, Train Acc: 0.4848 | \nEpoch [2/200] | Train Loss: 0.6972, Train Acc: 0.5084 | \nEpoch [3/200] | Train Loss: 0.6760, Train Acc: 0.5455 | \nEpoch [4/200] | Train Loss: 0.5799, Train Acc: 0.7138 | \nEpoch [5/200] | Train Loss: 0.5492, Train Acc: 0.7239 | \nEpoch [6/200] | Train Loss: 0.5185, Train Acc: 0.7508 | \nEpoch [7/200] | Train Loss: 0.5056, Train Acc: 0.7374 | \nEpoch [8/200] | Train Loss: 0.4736, Train Acc: 0.7744 | \nEpoch [9/200] | Train Loss: 0.4249, Train Acc: 0.7778 | \nEpoch [10/200] | Train Loss: 0.3905, Train Acc: 0.8047 | \nEpoch [11/200] | Train Loss: 0.3924, Train Acc: 0.8148 | \nEpoch [12/200] | Train Loss: 0.4444, Train Acc: 0.7912 | \nEpoch [13/200] | Train Loss: 0.4527, Train Acc: 0.7980 | \nEpoch [14/200] | Train Loss: 0.3654, Train Acc: 0.8316 | \nEpoch [15/200] | Train Loss: 0.5096, Train Acc: 0.7778 | \nEpoch [16/200] | Train Loss: 0.4686, Train Acc: 0.7744 | \nEpoch [17/200] | Train Loss: 0.3761, Train Acc: 0.8114 | \nEpoch [18/200] | Train Loss: 0.3670, Train Acc: 0.8148 | \nEpoch [19/200] | Train Loss: 0.3425, Train Acc: 0.8350 | \nEpoch [20/200] | Train Loss: 0.3207, Train Acc: 0.8350 | \nEpoch [21/200] | Train Loss: 0.3382, Train Acc: 0.8485 | \nEpoch [22/200] | Train Loss: 0.3156, Train Acc: 0.8519 | \nEpoch [23/200] | Train Loss: 0.3753, Train Acc: 0.8384 | \nEpoch [24/200] | Train Loss: 0.4049, Train Acc: 0.8047 | \nEpoch [25/200] | Train Loss: 0.3994, Train Acc: 0.7778 | \nEpoch [26/200] | Train Loss: 0.3616, Train Acc: 0.8384 | \nEpoch [27/200] | Train Loss: 0.3944, Train Acc: 0.8316 | \nEpoch [28/200] | Train Loss: 0.3799, Train Acc: 0.8182 | \nEpoch [29/200] | Train Loss: 0.3210, Train Acc: 0.8485 | \nEpoch [30/200] | Train Loss: 0.3218, Train Acc: 0.8215 | \nEpoch [31/200] | Train Loss: 0.3322, Train Acc: 0.8283 | \nEpoch [32/200] | Train Loss: 0.3028, Train Acc: 0.8519 | \nEpoch [33/200] | Train Loss: 0.2829, Train Acc: 0.8721 | \nEpoch [34/200] | Train Loss: 0.2744, Train Acc: 0.8889 | \nEpoch [35/200] | Train Loss: 0.2978, Train Acc: 0.8519 | \nEpoch [36/200] | Train Loss: 0.3151, Train Acc: 0.8653 | \nEpoch [37/200] | Train Loss: 0.3559, Train Acc: 0.8384 | \nEpoch [38/200] | Train Loss: 0.3105, Train Acc: 0.8552 | \nEpoch [39/200] | Train Loss: 0.2964, Train Acc: 0.8721 | \nEpoch [40/200] | Train Loss: 0.3345, Train Acc: 0.8586 | \nEpoch [41/200] | Train Loss: 0.2798, Train Acc: 0.8788 | \nEpoch [42/200] | Train Loss: 0.2600, Train Acc: 0.8822 | \nEpoch [43/200] | Train Loss: 0.3711, Train Acc: 0.8418 | \nEpoch [44/200] | Train Loss: 0.3376, Train Acc: 0.8316 | \nEpoch [45/200] | Train Loss: 0.2971, Train Acc: 0.8552 | \nEpoch [46/200] | Train Loss: 0.2640, Train Acc: 0.8754 | \nEpoch [47/200] | Train Loss: 0.2953, Train Acc: 0.8552 | \nEpoch [48/200] | Train Loss: 0.2520, Train Acc: 0.8754 | \nEpoch [49/200] | Train Loss: 0.3048, Train Acc: 0.8552 | \nEpoch [50/200] | Train Loss: 0.3306, Train Acc: 0.8552 | \nEpoch [51/200] | Train Loss: 0.3419, Train Acc: 0.8721 | \nEpoch [52/200] | Train Loss: 0.2805, Train Acc: 0.8721 | \nEpoch [53/200] | Train Loss: 0.2897, Train Acc: 0.8552 | \nEarly stopping triggered.\nTest Loss: 0.4920, Test Accuracy: 0.8267, Test AUC: 0.8656\n\n--- Processing: psd_beta ---\nShape: (372, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7401, Train Acc: 0.4848 | \nEpoch [2/200] | Train Loss: 0.6691, Train Acc: 0.5657 | \nEpoch [3/200] | Train Loss: 0.5294, Train Acc: 0.7239 | \nEpoch [4/200] | Train Loss: 0.6499, Train Acc: 0.6633 | \nEpoch [5/200] | Train Loss: 0.5676, Train Acc: 0.6835 | \nEpoch [6/200] | Train Loss: 0.5300, Train Acc: 0.7306 | \nEpoch [7/200] | Train Loss: 0.4835, Train Acc: 0.7542 | \nEpoch [8/200] | Train Loss: 0.4711, Train Acc: 0.7576 | \nEpoch [9/200] | Train Loss: 0.4760, Train Acc: 0.8047 | \nEpoch [10/200] | Train Loss: 0.4944, Train Acc: 0.7374 | \nEpoch [11/200] | Train Loss: 0.4435, Train Acc: 0.8013 | \nEpoch [12/200] | Train Loss: 0.4423, Train Acc: 0.7912 | \nEpoch [13/200] | Train Loss: 0.4741, Train Acc: 0.7475 | \nEpoch [14/200] | Train Loss: 0.4187, Train Acc: 0.8215 | \nEpoch [15/200] | Train Loss: 0.3563, Train Acc: 0.8620 | \nEpoch [16/200] | Train Loss: 0.3838, Train Acc: 0.8182 | \nEpoch [17/200] | Train Loss: 0.3820, Train Acc: 0.8316 | \nEpoch [18/200] | Train Loss: 0.3543, Train Acc: 0.8519 | \nEpoch [19/200] | Train Loss: 0.3598, Train Acc: 0.8451 | \nEpoch [20/200] | Train Loss: 0.4842, Train Acc: 0.7811 | \nEpoch [21/200] | Train Loss: 0.5158, Train Acc: 0.7508 | \nEpoch [22/200] | Train Loss: 0.4954, Train Acc: 0.7441 | \nEpoch [23/200] | Train Loss: 0.4678, Train Acc: 0.7744 | \nEpoch [24/200] | Train Loss: 0.4157, Train Acc: 0.7912 | \nEpoch [25/200] | Train Loss: 0.4015, Train Acc: 0.8114 | \nEpoch [26/200] | Train Loss: 0.3919, Train Acc: 0.8182 | \nEpoch [27/200] | Train Loss: 0.4050, Train Acc: 0.8148 | \nEpoch [28/200] | Train Loss: 0.3591, Train Acc: 0.8384 | \nEpoch [29/200] | Train Loss: 0.3174, Train Acc: 0.8620 | \nEpoch [30/200] | Train Loss: 0.3478, Train Acc: 0.8552 | \nEpoch [31/200] | Train Loss: 0.3473, Train Acc: 0.8620 | \nEpoch [32/200] | Train Loss: 0.3150, Train Acc: 0.8586 | \nEpoch [33/200] | Train Loss: 0.3200, Train Acc: 0.8687 | \nEpoch [34/200] | Train Loss: 0.3065, Train Acc: 0.8788 | \nEpoch [35/200] | Train Loss: 0.3187, Train Acc: 0.8485 | \nEpoch [36/200] | Train Loss: 0.3554, Train Acc: 0.8519 | \nEpoch [37/200] | Train Loss: 0.3554, Train Acc: 0.8418 | \nEpoch [38/200] | Train Loss: 0.3608, Train Acc: 0.8485 | \nEpoch [39/200] | Train Loss: 0.3413, Train Acc: 0.8485 | \nEpoch [40/200] | Train Loss: 0.3240, Train Acc: 0.8687 | \nEpoch [41/200] | Train Loss: 0.2939, Train Acc: 0.8754 | \nEpoch [42/200] | Train Loss: 0.2923, Train Acc: 0.8687 | \nEpoch [43/200] | Train Loss: 0.3383, Train Acc: 0.8485 | \nEpoch [44/200] | Train Loss: 0.3269, Train Acc: 0.8519 | \nEpoch [45/200] | Train Loss: 0.3014, Train Acc: 0.8855 | \nEpoch [46/200] | Train Loss: 0.3009, Train Acc: 0.8586 | \nEpoch [47/200] | Train Loss: 0.3348, Train Acc: 0.8620 | \nEpoch [48/200] | Train Loss: 0.2901, Train Acc: 0.8754 | \nEpoch [49/200] | Train Loss: 0.3431, Train Acc: 0.8485 | \nEpoch [50/200] | Train Loss: 0.3033, Train Acc: 0.8687 | \nEpoch [51/200] | Train Loss: 0.3412, Train Acc: 0.8552 | \nEpoch [52/200] | Train Loss: 0.3328, Train Acc: 0.8485 | \nEpoch [53/200] | Train Loss: 0.2920, Train Acc: 0.8687 | \nEpoch [54/200] | Train Loss: 0.3564, Train Acc: 0.8485 | \nEpoch [55/200] | Train Loss: 0.3192, Train Acc: 0.8586 | \nEpoch [56/200] | Train Loss: 0.3070, Train Acc: 0.8721 | \nEpoch [57/200] | Train Loss: 0.2912, Train Acc: 0.8687 | \nEpoch [58/200] | Train Loss: 0.3088, Train Acc: 0.8485 | \nEpoch 00059: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [59/200] | Train Loss: 0.2968, Train Acc: 0.8687 | \nEpoch [60/200] | Train Loss: 0.2737, Train Acc: 0.8687 | \nEpoch [61/200] | Train Loss: 0.2775, Train Acc: 0.8721 | \nEpoch [62/200] | Train Loss: 0.2527, Train Acc: 0.8855 | \nEpoch [63/200] | Train Loss: 0.2514, Train Acc: 0.8923 | \nEpoch [64/200] | Train Loss: 0.2473, Train Acc: 0.8889 | \nEpoch [65/200] | Train Loss: 0.2611, Train Acc: 0.8721 | \nEpoch [66/200] | Train Loss: 0.2447, Train Acc: 0.8889 | \nEpoch [67/200] | Train Loss: 0.2439, Train Acc: 0.8990 | \nEpoch [68/200] | Train Loss: 0.2420, Train Acc: 0.8923 | \nEpoch [69/200] | Train Loss: 0.2527, Train Acc: 0.8788 | \nEpoch [70/200] | Train Loss: 0.2365, Train Acc: 0.8956 | \nEpoch [71/200] | Train Loss: 0.2418, Train Acc: 0.8990 | \nEpoch [72/200] | Train Loss: 0.2356, Train Acc: 0.8990 | \nEpoch [73/200] | Train Loss: 0.2431, Train Acc: 0.8889 | \nEpoch [74/200] | Train Loss: 0.2273, Train Acc: 0.9024 | \nEpoch [75/200] | Train Loss: 0.2536, Train Acc: 0.8754 | \nEpoch [76/200] | Train Loss: 0.2358, Train Acc: 0.8923 | \nEpoch [77/200] | Train Loss: 0.2335, Train Acc: 0.8956 | \nEpoch [78/200] | Train Loss: 0.2344, Train Acc: 0.8889 | \nEpoch [79/200] | Train Loss: 0.2312, Train Acc: 0.8956 | \nEpoch [80/200] | Train Loss: 0.2233, Train Acc: 0.9091 | \nEpoch [81/200] | Train Loss: 0.2256, Train Acc: 0.9024 | \nEpoch [82/200] | Train Loss: 0.2323, Train Acc: 0.8956 | \nEpoch [83/200] | Train Loss: 0.2191, Train Acc: 0.8990 | \nEpoch [84/200] | Train Loss: 0.2191, Train Acc: 0.9091 | \nEpoch [85/200] | Train Loss: 0.2232, Train Acc: 0.9158 | \nEpoch [86/200] | Train Loss: 0.2320, Train Acc: 0.8990 | \nEpoch [87/200] | Train Loss: 0.2365, Train Acc: 0.8889 | \nEpoch [88/200] | Train Loss: 0.2264, Train Acc: 0.8990 | \nEpoch [89/200] | Train Loss: 0.2366, Train Acc: 0.8889 | \nEpoch [90/200] | Train Loss: 0.2220, Train Acc: 0.9091 | \nEpoch [91/200] | Train Loss: 0.2139, Train Acc: 0.9192 | \nEpoch [92/200] | Train Loss: 0.2068, Train Acc: 0.9057 | \nEpoch [93/200] | Train Loss: 0.2380, Train Acc: 0.9024 | \nEpoch [94/200] | Train Loss: 0.2315, Train Acc: 0.8855 | \nEpoch [95/200] | Train Loss: 0.2290, Train Acc: 0.9057 | \nEpoch [96/200] | Train Loss: 0.2130, Train Acc: 0.9226 | \nEpoch [97/200] | Train Loss: 0.2277, Train Acc: 0.8889 | \nEpoch [98/200] | Train Loss: 0.1999, Train Acc: 0.9259 | \nEpoch [99/200] | Train Loss: 0.2167, Train Acc: 0.9057 | \nEpoch [100/200] | Train Loss: 0.2155, Train Acc: 0.9091 | \nEpoch [101/200] | Train Loss: 0.2088, Train Acc: 0.9091 | \nEpoch [102/200] | Train Loss: 0.2277, Train Acc: 0.9024 | \nEpoch [103/200] | Train Loss: 0.1949, Train Acc: 0.9125 | \nEpoch [104/200] | Train Loss: 0.2172, Train Acc: 0.8923 | \nEpoch [105/200] | Train Loss: 0.2033, Train Acc: 0.9125 | \nEpoch [106/200] | Train Loss: 0.2403, Train Acc: 0.8990 | \nEpoch [107/200] | Train Loss: 0.2075, Train Acc: 0.9024 | \nEpoch [108/200] | Train Loss: 0.2100, Train Acc: 0.9091 | \nEpoch [109/200] | Train Loss: 0.2200, Train Acc: 0.9057 | \nEpoch [110/200] | Train Loss: 0.2052, Train Acc: 0.9057 | \nEpoch [111/200] | Train Loss: 0.1983, Train Acc: 0.9125 | \nEpoch [112/200] | Train Loss: 0.1903, Train Acc: 0.9259 | \nEpoch [113/200] | Train Loss: 0.2042, Train Acc: 0.9057 | \nEpoch [114/200] | Train Loss: 0.1913, Train Acc: 0.9226 | \nEpoch [115/200] | Train Loss: 0.1976, Train Acc: 0.9192 | \nEpoch [116/200] | Train Loss: 0.1960, Train Acc: 0.9158 | \nEpoch [117/200] | Train Loss: 0.2273, Train Acc: 0.9057 | \nEarly stopping triggered.\nTest Loss: 0.2153, Test Accuracy: 0.9067, Test AUC: 0.9630\n\n--- Processing: psd_highbeta ---\nShape: (372, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7378, Train Acc: 0.5051 | \nEpoch [2/200] | Train Loss: 0.7323, Train Acc: 0.5152 | \nEpoch [3/200] | Train Loss: 0.6401, Train Acc: 0.6902 | \nEpoch [4/200] | Train Loss: 0.5467, Train Acc: 0.7239 | \nEpoch [5/200] | Train Loss: 0.5100, Train Acc: 0.7576 | \nEpoch [6/200] | Train Loss: 0.4648, Train Acc: 0.8081 | \nEpoch [7/200] | Train Loss: 0.4343, Train Acc: 0.8013 | \nEpoch [8/200] | Train Loss: 0.5492, Train Acc: 0.7239 | \nEpoch [9/200] | Train Loss: 0.4626, Train Acc: 0.7609 | \nEpoch [10/200] | Train Loss: 0.4134, Train Acc: 0.8081 | \nEpoch [11/200] | Train Loss: 0.3755, Train Acc: 0.8418 | \nEpoch [12/200] | Train Loss: 0.3792, Train Acc: 0.8418 | \nEpoch [13/200] | Train Loss: 0.3144, Train Acc: 0.8721 | \nEpoch [14/200] | Train Loss: 0.4231, Train Acc: 0.8013 | \nEpoch [15/200] | Train Loss: 0.4051, Train Acc: 0.7845 | \nEpoch [16/200] | Train Loss: 0.3787, Train Acc: 0.8148 | \nEpoch [17/200] | Train Loss: 0.3272, Train Acc: 0.8721 | \nEpoch [18/200] | Train Loss: 0.2976, Train Acc: 0.8855 | \nEpoch [19/200] | Train Loss: 0.3422, Train Acc: 0.8519 | \nEpoch [20/200] | Train Loss: 0.3404, Train Acc: 0.8418 | \nEpoch [21/200] | Train Loss: 0.3768, Train Acc: 0.8283 | \nEpoch [22/200] | Train Loss: 0.3518, Train Acc: 0.8384 | \nEpoch [23/200] | Train Loss: 0.3807, Train Acc: 0.8215 | \nEpoch [24/200] | Train Loss: 0.3620, Train Acc: 0.8485 | \nEpoch [25/200] | Train Loss: 0.3160, Train Acc: 0.8687 | \nEpoch [26/200] | Train Loss: 0.3242, Train Acc: 0.8687 | \nEpoch [27/200] | Train Loss: 0.3000, Train Acc: 0.8956 | \nEpoch [28/200] | Train Loss: 0.2686, Train Acc: 0.9057 | \nEpoch [29/200] | Train Loss: 0.2982, Train Acc: 0.8721 | \nEpoch [30/200] | Train Loss: 0.3343, Train Acc: 0.8586 | \nEpoch [31/200] | Train Loss: 0.3861, Train Acc: 0.8148 | \nEpoch [32/200] | Train Loss: 0.2910, Train Acc: 0.8754 | \nEpoch [33/200] | Train Loss: 0.2984, Train Acc: 0.8653 | \nEpoch [34/200] | Train Loss: 0.3197, Train Acc: 0.8653 | \nEpoch [35/200] | Train Loss: 0.3310, Train Acc: 0.8384 | \nEpoch [36/200] | Train Loss: 0.3113, Train Acc: 0.8653 | \nEpoch [37/200] | Train Loss: 0.2952, Train Acc: 0.8653 | \nEpoch [38/200] | Train Loss: 0.2979, Train Acc: 0.8889 | \nEpoch 00039: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [39/200] | Train Loss: 0.2823, Train Acc: 0.8687 | \nEpoch [40/200] | Train Loss: 0.3256, Train Acc: 0.8519 | \nEpoch [41/200] | Train Loss: 0.2658, Train Acc: 0.8889 | \nEpoch [42/200] | Train Loss: 0.2523, Train Acc: 0.8956 | \nEpoch [43/200] | Train Loss: 0.2562, Train Acc: 0.8822 | \nEpoch [44/200] | Train Loss: 0.2261, Train Acc: 0.8956 | \nEpoch [45/200] | Train Loss: 0.2125, Train Acc: 0.9125 | \nEpoch [46/200] | Train Loss: 0.2048, Train Acc: 0.9091 | \nEpoch [47/200] | Train Loss: 0.2037, Train Acc: 0.9158 | \nEpoch [48/200] | Train Loss: 0.1962, Train Acc: 0.9158 | \nEpoch [49/200] | Train Loss: 0.2009, Train Acc: 0.9158 | \nEpoch [50/200] | Train Loss: 0.1717, Train Acc: 0.9394 | \nEpoch [51/200] | Train Loss: 0.1940, Train Acc: 0.9158 | \nEpoch [52/200] | Train Loss: 0.1753, Train Acc: 0.9192 | \nEpoch [53/200] | Train Loss: 0.1694, Train Acc: 0.9293 | \nEpoch [54/200] | Train Loss: 0.1611, Train Acc: 0.9428 | \nEpoch [55/200] | Train Loss: 0.1874, Train Acc: 0.9226 | \nEpoch [56/200] | Train Loss: 0.1545, Train Acc: 0.9327 | \nEpoch [57/200] | Train Loss: 0.1759, Train Acc: 0.9192 | \nEpoch [58/200] | Train Loss: 0.1573, Train Acc: 0.9428 | \nEpoch [59/200] | Train Loss: 0.1441, Train Acc: 0.9428 | \nEpoch [60/200] | Train Loss: 0.1631, Train Acc: 0.9226 | \nEpoch [61/200] | Train Loss: 0.1351, Train Acc: 0.9461 | \nEpoch [62/200] | Train Loss: 0.1602, Train Acc: 0.9226 | \nEpoch [63/200] | Train Loss: 0.1449, Train Acc: 0.9192 | \nEpoch [64/200] | Train Loss: 0.1513, Train Acc: 0.9360 | \nEpoch [65/200] | Train Loss: 0.1405, Train Acc: 0.9293 | \nEpoch [66/200] | Train Loss: 0.1412, Train Acc: 0.9259 | \nEpoch [67/200] | Train Loss: 0.1085, Train Acc: 0.9529 | \nEpoch [68/200] | Train Loss: 0.1379, Train Acc: 0.9226 | \nEpoch [69/200] | Train Loss: 0.1360, Train Acc: 0.9529 | \nEpoch [70/200] | Train Loss: 0.1390, Train Acc: 0.9360 | \nEpoch [71/200] | Train Loss: 0.1515, Train Acc: 0.9293 | \nEpoch [72/200] | Train Loss: 0.1046, Train Acc: 0.9529 | \nEpoch [73/200] | Train Loss: 0.1207, Train Acc: 0.9596 | \nEpoch [74/200] | Train Loss: 0.1244, Train Acc: 0.9529 | \nEpoch [75/200] | Train Loss: 0.1412, Train Acc: 0.9360 | \nEpoch [76/200] | Train Loss: 0.1465, Train Acc: 0.9360 | \nEpoch [77/200] | Train Loss: 0.1300, Train Acc: 0.9394 | \nEpoch [78/200] | Train Loss: 0.1268, Train Acc: 0.9428 | \nEpoch [79/200] | Train Loss: 0.1142, Train Acc: 0.9529 | \nEpoch [80/200] | Train Loss: 0.1204, Train Acc: 0.9394 | \nEpoch [81/200] | Train Loss: 0.1416, Train Acc: 0.9394 | \nEpoch [82/200] | Train Loss: 0.1215, Train Acc: 0.9461 | \nEpoch 00083: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [83/200] | Train Loss: 0.1222, Train Acc: 0.9461 | \nEpoch [84/200] | Train Loss: 0.1068, Train Acc: 0.9529 | \nEpoch [85/200] | Train Loss: 0.1039, Train Acc: 0.9562 | \nEpoch [86/200] | Train Loss: 0.1071, Train Acc: 0.9461 | \nEpoch [87/200] | Train Loss: 0.0859, Train Acc: 0.9663 | \nEpoch [88/200] | Train Loss: 0.1095, Train Acc: 0.9663 | \nEpoch [89/200] | Train Loss: 0.0999, Train Acc: 0.9630 | \nEpoch [90/200] | Train Loss: 0.1046, Train Acc: 0.9596 | \nEpoch [91/200] | Train Loss: 0.1215, Train Acc: 0.9529 | \nEpoch [92/200] | Train Loss: 0.1075, Train Acc: 0.9630 | \nEpoch [93/200] | Train Loss: 0.1088, Train Acc: 0.9461 | \nEpoch [94/200] | Train Loss: 0.0936, Train Acc: 0.9596 | \nEpoch [95/200] | Train Loss: 0.1083, Train Acc: 0.9428 | \nEpoch [96/200] | Train Loss: 0.1143, Train Acc: 0.9394 | \nEpoch [97/200] | Train Loss: 0.1062, Train Acc: 0.9630 | \nEpoch 00098: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [98/200] | Train Loss: 0.0898, Train Acc: 0.9663 | \nEpoch [99/200] | Train Loss: 0.1007, Train Acc: 0.9562 | \nEpoch [100/200] | Train Loss: 0.1120, Train Acc: 0.9562 | \nEpoch [101/200] | Train Loss: 0.1088, Train Acc: 0.9495 | \nEpoch [102/200] | Train Loss: 0.1000, Train Acc: 0.9562 | \nEpoch [103/200] | Train Loss: 0.0925, Train Acc: 0.9663 | \nEpoch [104/200] | Train Loss: 0.0989, Train Acc: 0.9562 | \nEpoch [105/200] | Train Loss: 0.0858, Train Acc: 0.9697 | \nEpoch [106/200] | Train Loss: 0.0876, Train Acc: 0.9697 | \nEpoch [107/200] | Train Loss: 0.0900, Train Acc: 0.9697 | \nEpoch [108/200] | Train Loss: 0.0898, Train Acc: 0.9562 | \nEpoch [109/200] | Train Loss: 0.1037, Train Acc: 0.9663 | \nEpoch [110/200] | Train Loss: 0.1012, Train Acc: 0.9596 | \nEpoch [111/200] | Train Loss: 0.1128, Train Acc: 0.9630 | \nEpoch [112/200] | Train Loss: 0.0953, Train Acc: 0.9596 | \nEpoch [113/200] | Train Loss: 0.0913, Train Acc: 0.9596 | \nEpoch [114/200] | Train Loss: 0.1069, Train Acc: 0.9630 | \nEpoch [115/200] | Train Loss: 0.1062, Train Acc: 0.9529 | \nEpoch 00116: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [116/200] | Train Loss: 0.1109, Train Acc: 0.9461 | \nEpoch [117/200] | Train Loss: 0.0954, Train Acc: 0.9663 | \nEpoch [118/200] | Train Loss: 0.0908, Train Acc: 0.9697 | \nEpoch [119/200] | Train Loss: 0.0869, Train Acc: 0.9663 | \nEpoch [120/200] | Train Loss: 0.0954, Train Acc: 0.9596 | \nEpoch [121/200] | Train Loss: 0.0955, Train Acc: 0.9562 | \nEpoch [122/200] | Train Loss: 0.0843, Train Acc: 0.9663 | \nEpoch [123/200] | Train Loss: 0.0910, Train Acc: 0.9731 | \nEpoch [124/200] | Train Loss: 0.1281, Train Acc: 0.9529 | \nEpoch [125/200] | Train Loss: 0.0970, Train Acc: 0.9630 | \nEpoch [126/200] | Train Loss: 0.1068, Train Acc: 0.9630 | \nEpoch [127/200] | Train Loss: 0.1020, Train Acc: 0.9596 | \nEpoch [128/200] | Train Loss: 0.0983, Train Acc: 0.9562 | \nEpoch [129/200] | Train Loss: 0.0991, Train Acc: 0.9562 | \nEpoch [130/200] | Train Loss: 0.0971, Train Acc: 0.9596 | \nEpoch [131/200] | Train Loss: 0.0940, Train Acc: 0.9663 | \nEpoch [132/200] | Train Loss: 0.0944, Train Acc: 0.9630 | \nEpoch 00133: reducing learning rate of group 0 to 1.0000e-08.\nEpoch [133/200] | Train Loss: 0.1049, Train Acc: 0.9562 | \nEpoch [134/200] | Train Loss: 0.1151, Train Acc: 0.9596 | \nEpoch [135/200] | Train Loss: 0.0984, Train Acc: 0.9663 | \nEpoch [136/200] | Train Loss: 0.1073, Train Acc: 0.9630 | \nEpoch [137/200] | Train Loss: 0.0979, Train Acc: 0.9562 | \nEpoch [138/200] | Train Loss: 0.1207, Train Acc: 0.9428 | \nEpoch [139/200] | Train Loss: 0.0924, Train Acc: 0.9663 | \nEpoch [140/200] | Train Loss: 0.1022, Train Acc: 0.9596 | \nEpoch [141/200] | Train Loss: 0.0931, Train Acc: 0.9663 | \nEpoch [142/200] | Train Loss: 0.1013, Train Acc: 0.9529 | \nEarly stopping triggered.\nTest Loss: 0.5073, Test Accuracy: 0.8133, Test AUC: 0.9161\n\n--- Processing: psd_gamma ---\nShape: (372, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7829, Train Acc: 0.4613 | \nEpoch [2/200] | Train Loss: 0.7052, Train Acc: 0.4512 | \nEpoch [3/200] | Train Loss: 0.6689, Train Acc: 0.5960 | \nEpoch [4/200] | Train Loss: 0.6015, Train Acc: 0.6869 | \nEpoch [5/200] | Train Loss: 0.5166, Train Acc: 0.7441 | \nEpoch [6/200] | Train Loss: 0.5193, Train Acc: 0.7542 | \nEpoch [7/200] | Train Loss: 0.5709, Train Acc: 0.6970 | \nEpoch [8/200] | Train Loss: 0.4881, Train Acc: 0.7542 | \nEpoch [9/200] | Train Loss: 0.4755, Train Acc: 0.7778 | \nEpoch [10/200] | Train Loss: 0.5061, Train Acc: 0.7677 | \nEpoch [11/200] | Train Loss: 0.4522, Train Acc: 0.7542 | \nEpoch [12/200] | Train Loss: 0.3977, Train Acc: 0.8215 | \nEpoch [13/200] | Train Loss: 0.3737, Train Acc: 0.8418 | \nEpoch [14/200] | Train Loss: 0.3902, Train Acc: 0.8148 | \nEpoch [15/200] | Train Loss: 0.3734, Train Acc: 0.8384 | \nEpoch [16/200] | Train Loss: 0.3383, Train Acc: 0.8552 | \nEpoch [17/200] | Train Loss: 0.3240, Train Acc: 0.8485 | \nEpoch [18/200] | Train Loss: 0.4494, Train Acc: 0.7744 | \nEpoch [19/200] | Train Loss: 0.3891, Train Acc: 0.8081 | \nEpoch [20/200] | Train Loss: 0.3608, Train Acc: 0.8148 | \nEpoch [21/200] | Train Loss: 0.3402, Train Acc: 0.8451 | \nEpoch [22/200] | Train Loss: 0.3363, Train Acc: 0.8451 | \nEpoch [23/200] | Train Loss: 0.2678, Train Acc: 0.8889 | \nEpoch [24/200] | Train Loss: 0.3136, Train Acc: 0.8586 | \nEpoch [25/200] | Train Loss: 0.3106, Train Acc: 0.8687 | \nEpoch [26/200] | Train Loss: 0.3227, Train Acc: 0.8519 | \nEpoch [27/200] | Train Loss: 0.2940, Train Acc: 0.8754 | \nEpoch [28/200] | Train Loss: 0.3011, Train Acc: 0.8754 | \nEpoch [29/200] | Train Loss: 0.3366, Train Acc: 0.8451 | \nEpoch [30/200] | Train Loss: 0.2919, Train Acc: 0.8889 | \nEpoch [31/200] | Train Loss: 0.2912, Train Acc: 0.8822 | \nEpoch [32/200] | Train Loss: 0.3194, Train Acc: 0.8384 | \nEpoch [33/200] | Train Loss: 0.2956, Train Acc: 0.8889 | \nEpoch 00034: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [34/200] | Train Loss: 0.2862, Train Acc: 0.8721 | \nEpoch [35/200] | Train Loss: 0.2493, Train Acc: 0.8822 | \nEpoch [36/200] | Train Loss: 0.2268, Train Acc: 0.8956 | \nEpoch [37/200] | Train Loss: 0.2276, Train Acc: 0.8990 | \nEpoch [38/200] | Train Loss: 0.2061, Train Acc: 0.9226 | \nEpoch [39/200] | Train Loss: 0.2035, Train Acc: 0.9259 | \nEpoch [40/200] | Train Loss: 0.2211, Train Acc: 0.9158 | \nEpoch [41/200] | Train Loss: 0.2177, Train Acc: 0.9158 | \nEpoch [42/200] | Train Loss: 0.2272, Train Acc: 0.9226 | \nEpoch [43/200] | Train Loss: 0.2013, Train Acc: 0.9192 | \nEpoch [44/200] | Train Loss: 0.2159, Train Acc: 0.9091 | \nEpoch [45/200] | Train Loss: 0.1857, Train Acc: 0.9259 | \nEpoch [46/200] | Train Loss: 0.1842, Train Acc: 0.9327 | \nEpoch [47/200] | Train Loss: 0.1898, Train Acc: 0.9226 | \nEpoch [48/200] | Train Loss: 0.1871, Train Acc: 0.9293 | \nEpoch [49/200] | Train Loss: 0.2019, Train Acc: 0.9125 | \nEpoch [50/200] | Train Loss: 0.1710, Train Acc: 0.9394 | \nEpoch [51/200] | Train Loss: 0.1855, Train Acc: 0.9259 | \nEpoch [52/200] | Train Loss: 0.2017, Train Acc: 0.9158 | \nEpoch [53/200] | Train Loss: 0.2070, Train Acc: 0.9125 | \nEpoch [54/200] | Train Loss: 0.1803, Train Acc: 0.9327 | \nEpoch [55/200] | Train Loss: 0.1862, Train Acc: 0.9192 | \nEpoch [56/200] | Train Loss: 0.1833, Train Acc: 0.9327 | \nEpoch [57/200] | Train Loss: 0.1980, Train Acc: 0.9327 | \nEpoch [58/200] | Train Loss: 0.1895, Train Acc: 0.9192 | \nEpoch [59/200] | Train Loss: 0.1741, Train Acc: 0.9327 | \nEpoch [60/200] | Train Loss: 0.1607, Train Acc: 0.9394 | \nEpoch [61/200] | Train Loss: 0.1663, Train Acc: 0.9327 | \nEpoch [62/200] | Train Loss: 0.1815, Train Acc: 0.9293 | \nEpoch [63/200] | Train Loss: 0.1551, Train Acc: 0.9529 | \nEpoch [64/200] | Train Loss: 0.1841, Train Acc: 0.9327 | \nEpoch [65/200] | Train Loss: 0.1578, Train Acc: 0.9461 | \nEpoch [66/200] | Train Loss: 0.1527, Train Acc: 0.9461 | \nEpoch [67/200] | Train Loss: 0.1500, Train Acc: 0.9394 | \nEpoch [68/200] | Train Loss: 0.1518, Train Acc: 0.9428 | \nEpoch [69/200] | Train Loss: 0.1763, Train Acc: 0.9293 | \nEpoch [70/200] | Train Loss: 0.1776, Train Acc: 0.9327 | \nEpoch [71/200] | Train Loss: 0.1725, Train Acc: 0.9394 | \nEpoch [72/200] | Train Loss: 0.1819, Train Acc: 0.9360 | \nEpoch [73/200] | Train Loss: 0.1637, Train Acc: 0.9394 | \nEpoch [74/200] | Train Loss: 0.1567, Train Acc: 0.9495 | \nEpoch [75/200] | Train Loss: 0.1736, Train Acc: 0.9259 | \nEpoch [76/200] | Train Loss: 0.1648, Train Acc: 0.9360 | \nEpoch [77/200] | Train Loss: 0.1548, Train Acc: 0.9428 | \nEpoch 00078: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [78/200] | Train Loss: 0.1592, Train Acc: 0.9360 | \nEpoch [79/200] | Train Loss: 0.1630, Train Acc: 0.9461 | \nEpoch [80/200] | Train Loss: 0.1518, Train Acc: 0.9327 | \nEpoch [81/200] | Train Loss: 0.1389, Train Acc: 0.9461 | \nEpoch [82/200] | Train Loss: 0.1602, Train Acc: 0.9360 | \nEarly stopping triggered.\nTest Loss: 0.3818, Test Accuracy: 0.8533, Test AUC: 0.9381\n\n--- Processing: fc_delta ---\nShape: (372, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.8025, Train Acc: 0.4815 | \nEpoch [2/200] | Train Loss: 0.6959, Train Acc: 0.4916 | \nEpoch [3/200] | Train Loss: 0.7016, Train Acc: 0.4646 | \nEpoch [4/200] | Train Loss: 0.6925, Train Acc: 0.5017 | \nEpoch [5/200] | Train Loss: 0.6595, Train Acc: 0.6397 | \nEpoch [6/200] | Train Loss: 0.5641, Train Acc: 0.7239 | \nEpoch [7/200] | Train Loss: 0.5475, Train Acc: 0.7273 | \nEpoch [8/200] | Train Loss: 0.4752, Train Acc: 0.7677 | \nEpoch [9/200] | Train Loss: 0.4106, Train Acc: 0.8182 | \nEpoch [10/200] | Train Loss: 0.4613, Train Acc: 0.7744 | \nEpoch [11/200] | Train Loss: 0.4034, Train Acc: 0.8350 | \nEpoch [12/200] | Train Loss: 0.3981, Train Acc: 0.8114 | \nEpoch [13/200] | Train Loss: 0.3683, Train Acc: 0.8350 | \nEpoch [14/200] | Train Loss: 0.3881, Train Acc: 0.8114 | \nEpoch [15/200] | Train Loss: 0.3139, Train Acc: 0.8822 | \nEpoch [16/200] | Train Loss: 0.2785, Train Acc: 0.8889 | \nEpoch [17/200] | Train Loss: 0.2897, Train Acc: 0.8822 | \nEpoch [18/200] | Train Loss: 0.3009, Train Acc: 0.8687 | \nEpoch [19/200] | Train Loss: 0.2751, Train Acc: 0.8923 | \nEpoch [20/200] | Train Loss: 0.3047, Train Acc: 0.8687 | \nEpoch [21/200] | Train Loss: 0.3208, Train Acc: 0.8418 | \nEpoch [22/200] | Train Loss: 0.2354, Train Acc: 0.8956 | \nEpoch [23/200] | Train Loss: 0.2707, Train Acc: 0.8923 | \nEpoch [24/200] | Train Loss: 0.2126, Train Acc: 0.9057 | \nEpoch [25/200] | Train Loss: 0.2560, Train Acc: 0.8956 | \nEpoch [26/200] | Train Loss: 0.3131, Train Acc: 0.8620 | \nEpoch [27/200] | Train Loss: 0.2635, Train Acc: 0.8956 | \nEpoch [28/200] | Train Loss: 0.2130, Train Acc: 0.8956 | \nEpoch [29/200] | Train Loss: 0.2326, Train Acc: 0.9125 | \nEpoch [30/200] | Train Loss: 0.2130, Train Acc: 0.8923 | \nEpoch [31/200] | Train Loss: 0.1545, Train Acc: 0.9327 | \nEpoch [32/200] | Train Loss: 0.2186, Train Acc: 0.9125 | \nEpoch [33/200] | Train Loss: 0.1813, Train Acc: 0.9293 | \nEpoch [34/200] | Train Loss: 0.1891, Train Acc: 0.9327 | \nEpoch [35/200] | Train Loss: 0.1533, Train Acc: 0.9327 | \nEpoch [36/200] | Train Loss: 0.1631, Train Acc: 0.9394 | \nEpoch [37/200] | Train Loss: 0.1762, Train Acc: 0.9192 | \nEpoch [38/200] | Train Loss: 0.1676, Train Acc: 0.9226 | \nEpoch [39/200] | Train Loss: 0.2497, Train Acc: 0.8822 | \nEpoch [40/200] | Train Loss: 0.2095, Train Acc: 0.8990 | \nEpoch [41/200] | Train Loss: 0.1952, Train Acc: 0.9226 | \nEpoch [42/200] | Train Loss: 0.2429, Train Acc: 0.9091 | \nEpoch [43/200] | Train Loss: 0.1831, Train Acc: 0.9226 | \nEpoch [44/200] | Train Loss: 0.1586, Train Acc: 0.9529 | \nEpoch [45/200] | Train Loss: 0.1598, Train Acc: 0.9394 | \nEpoch 00046: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [46/200] | Train Loss: 0.1595, Train Acc: 0.9259 | \nEpoch [47/200] | Train Loss: 0.1646, Train Acc: 0.9495 | \nEpoch [48/200] | Train Loss: 0.1102, Train Acc: 0.9495 | \nEpoch [49/200] | Train Loss: 0.0936, Train Acc: 0.9663 | \nEpoch [50/200] | Train Loss: 0.1057, Train Acc: 0.9495 | \nEpoch [51/200] | Train Loss: 0.0880, Train Acc: 0.9697 | \nEpoch [52/200] | Train Loss: 0.0889, Train Acc: 0.9596 | \nEpoch [53/200] | Train Loss: 0.0584, Train Acc: 0.9832 | \nEpoch [54/200] | Train Loss: 0.0698, Train Acc: 0.9764 | \nEpoch [55/200] | Train Loss: 0.0527, Train Acc: 0.9865 | \nEpoch [56/200] | Train Loss: 0.0700, Train Acc: 0.9731 | \nEpoch [57/200] | Train Loss: 0.0604, Train Acc: 0.9764 | \nEpoch [58/200] | Train Loss: 0.0414, Train Acc: 0.9966 | \nEpoch [59/200] | Train Loss: 0.0583, Train Acc: 0.9764 | \nEpoch [60/200] | Train Loss: 0.0552, Train Acc: 0.9865 | \nEpoch [61/200] | Train Loss: 0.0336, Train Acc: 0.9933 | \nEpoch [62/200] | Train Loss: 0.0247, Train Acc: 0.9933 | \nEpoch [63/200] | Train Loss: 0.0341, Train Acc: 0.9899 | \nEpoch [64/200] | Train Loss: 0.0320, Train Acc: 0.9899 | \nEpoch [65/200] | Train Loss: 0.0270, Train Acc: 0.9966 | \nEpoch [66/200] | Train Loss: 0.0259, Train Acc: 0.9966 | \nEpoch [67/200] | Train Loss: 0.0214, Train Acc: 0.9966 | \nEpoch [68/200] | Train Loss: 0.0144, Train Acc: 1.0000 | \nEpoch [69/200] | Train Loss: 0.0261, Train Acc: 0.9899 | \nEpoch [70/200] | Train Loss: 0.0272, Train Acc: 0.9899 | \nEpoch [71/200] | Train Loss: 0.0309, Train Acc: 0.9865 | \nEpoch [72/200] | Train Loss: 0.0306, Train Acc: 0.9899 | \nEpoch [73/200] | Train Loss: 0.0107, Train Acc: 1.0000 | \nEpoch [74/200] | Train Loss: 0.0132, Train Acc: 0.9966 | \nEpoch [75/200] | Train Loss: 0.0107, Train Acc: 1.0000 | \nEpoch [76/200] | Train Loss: 0.0212, Train Acc: 0.9899 | \nEpoch [77/200] | Train Loss: 0.0638, Train Acc: 0.9798 | \nEpoch [78/200] | Train Loss: 0.0135, Train Acc: 0.9966 | \nEpoch [79/200] | Train Loss: 0.0474, Train Acc: 0.9832 | \nEpoch [80/200] | Train Loss: 0.0417, Train Acc: 0.9832 | \nEpoch [81/200] | Train Loss: 0.0462, Train Acc: 0.9865 | \nEpoch [82/200] | Train Loss: 0.0193, Train Acc: 0.9899 | \nEpoch [83/200] | Train Loss: 0.0125, Train Acc: 1.0000 | \nEpoch [84/200] | Train Loss: 0.0180, Train Acc: 0.9899 | \nEpoch [85/200] | Train Loss: 0.0161, Train Acc: 0.9966 | \nEpoch [86/200] | Train Loss: 0.0096, Train Acc: 0.9966 | \nEpoch [87/200] | Train Loss: 0.0087, Train Acc: 0.9966 | \nEarly stopping triggered.\nTest Loss: 0.7217, Test Accuracy: 0.8800, Test AUC: 0.8684\n\n--- Processing: fc_theta ---\nShape: (372, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7170, Train Acc: 0.5556 | \nEpoch [2/200] | Train Loss: 0.6989, Train Acc: 0.5017 | \nEpoch [3/200] | Train Loss: 0.7252, Train Acc: 0.4747 | \nEpoch [4/200] | Train Loss: 0.7050, Train Acc: 0.5253 | \nEpoch [5/200] | Train Loss: 0.6562, Train Acc: 0.5556 | \nEpoch [6/200] | Train Loss: 0.6019, Train Acc: 0.6801 | \nEpoch [7/200] | Train Loss: 0.5619, Train Acc: 0.7104 | \nEpoch [8/200] | Train Loss: 0.5123, Train Acc: 0.7374 | \nEpoch [9/200] | Train Loss: 0.5086, Train Acc: 0.7710 | \nEpoch [10/200] | Train Loss: 0.4314, Train Acc: 0.7946 | \nEpoch [11/200] | Train Loss: 0.3859, Train Acc: 0.8384 | \nEpoch [12/200] | Train Loss: 0.4014, Train Acc: 0.8047 | \nEpoch [13/200] | Train Loss: 0.3539, Train Acc: 0.8384 | \nEpoch [14/200] | Train Loss: 0.3365, Train Acc: 0.8754 | \nEpoch [15/200] | Train Loss: 0.3413, Train Acc: 0.8485 | \nEpoch [16/200] | Train Loss: 0.4790, Train Acc: 0.7542 | \nEpoch [17/200] | Train Loss: 0.4220, Train Acc: 0.7845 | \nEpoch [18/200] | Train Loss: 0.3303, Train Acc: 0.8653 | \nEpoch [19/200] | Train Loss: 0.2883, Train Acc: 0.8687 | \nEpoch [20/200] | Train Loss: 0.3289, Train Acc: 0.8519 | \nEpoch [21/200] | Train Loss: 0.2838, Train Acc: 0.8788 | \nEpoch [22/200] | Train Loss: 0.2967, Train Acc: 0.8653 | \nEpoch [23/200] | Train Loss: 0.2755, Train Acc: 0.8855 | \nEpoch [24/200] | Train Loss: 0.2882, Train Acc: 0.8687 | \nEpoch [25/200] | Train Loss: 0.2641, Train Acc: 0.8889 | \nEpoch [26/200] | Train Loss: 0.2330, Train Acc: 0.8822 | \nEpoch [27/200] | Train Loss: 0.2661, Train Acc: 0.8754 | \nEpoch [28/200] | Train Loss: 0.2286, Train Acc: 0.9158 | \nEpoch [29/200] | Train Loss: 0.2943, Train Acc: 0.8923 | \nEpoch [30/200] | Train Loss: 0.2670, Train Acc: 0.9057 | \nEpoch [31/200] | Train Loss: 0.2473, Train Acc: 0.8822 | \nEpoch [32/200] | Train Loss: 0.2409, Train Acc: 0.8990 | \nEpoch [33/200] | Train Loss: 0.3835, Train Acc: 0.8384 | \nEpoch [34/200] | Train Loss: 0.3713, Train Acc: 0.8114 | \nEpoch [35/200] | Train Loss: 0.3491, Train Acc: 0.8451 | \nEpoch [36/200] | Train Loss: 0.3041, Train Acc: 0.8687 | \nEpoch [37/200] | Train Loss: 0.2751, Train Acc: 0.8754 | \nEpoch [38/200] | Train Loss: 0.2634, Train Acc: 0.8788 | \nEpoch 00039: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [39/200] | Train Loss: 0.2408, Train Acc: 0.8956 | \nEpoch [40/200] | Train Loss: 0.1863, Train Acc: 0.9192 | \nEpoch [41/200] | Train Loss: 0.1816, Train Acc: 0.9158 | \nEpoch [42/200] | Train Loss: 0.1612, Train Acc: 0.9428 | \nEpoch [43/200] | Train Loss: 0.1565, Train Acc: 0.9394 | \nEpoch [44/200] | Train Loss: 0.1574, Train Acc: 0.9293 | \nEpoch [45/200] | Train Loss: 0.1514, Train Acc: 0.9394 | \nEpoch [46/200] | Train Loss: 0.1420, Train Acc: 0.9529 | \nEpoch [47/200] | Train Loss: 0.1295, Train Acc: 0.9562 | \nEpoch [48/200] | Train Loss: 0.1280, Train Acc: 0.9428 | \nEpoch [49/200] | Train Loss: 0.1286, Train Acc: 0.9428 | \nEpoch [50/200] | Train Loss: 0.1317, Train Acc: 0.9495 | \nEpoch [51/200] | Train Loss: 0.1084, Train Acc: 0.9630 | \nEpoch [52/200] | Train Loss: 0.1077, Train Acc: 0.9630 | \nEpoch [53/200] | Train Loss: 0.0979, Train Acc: 0.9731 | \nEpoch [54/200] | Train Loss: 0.0935, Train Acc: 0.9663 | \nEpoch [55/200] | Train Loss: 0.0919, Train Acc: 0.9697 | \nEpoch [56/200] | Train Loss: 0.0883, Train Acc: 0.9731 | \nEpoch [57/200] | Train Loss: 0.1095, Train Acc: 0.9562 | \nEpoch [58/200] | Train Loss: 0.0846, Train Acc: 0.9663 | \nEpoch [59/200] | Train Loss: 0.0727, Train Acc: 0.9798 | \nEpoch [60/200] | Train Loss: 0.1124, Train Acc: 0.9697 | \nEpoch [61/200] | Train Loss: 0.0829, Train Acc: 0.9731 | \nEpoch [62/200] | Train Loss: 0.0882, Train Acc: 0.9697 | \nEpoch [63/200] | Train Loss: 0.0638, Train Acc: 0.9832 | \nEpoch [64/200] | Train Loss: 0.0810, Train Acc: 0.9832 | \nEpoch [65/200] | Train Loss: 0.0608, Train Acc: 0.9832 | \nEpoch [66/200] | Train Loss: 0.0725, Train Acc: 0.9764 | \nEpoch [67/200] | Train Loss: 0.0641, Train Acc: 0.9798 | \nEpoch [68/200] | Train Loss: 0.0531, Train Acc: 0.9899 | \nEpoch [69/200] | Train Loss: 0.0746, Train Acc: 0.9731 | \nEpoch [70/200] | Train Loss: 0.0458, Train Acc: 0.9832 | \nEpoch [71/200] | Train Loss: 0.0415, Train Acc: 0.9933 | \nEpoch [72/200] | Train Loss: 0.0425, Train Acc: 0.9899 | \nEpoch [73/200] | Train Loss: 0.0434, Train Acc: 0.9933 | \nEpoch [74/200] | Train Loss: 0.0552, Train Acc: 0.9899 | \nEpoch [75/200] | Train Loss: 0.0471, Train Acc: 0.9899 | \nEpoch [76/200] | Train Loss: 0.0516, Train Acc: 0.9865 | \nEpoch [77/200] | Train Loss: 0.0512, Train Acc: 0.9832 | \nEpoch [78/200] | Train Loss: 0.0559, Train Acc: 0.9832 | \nEpoch [79/200] | Train Loss: 0.0428, Train Acc: 0.9899 | \nEpoch [80/200] | Train Loss: 0.0424, Train Acc: 0.9899 | \nEpoch [81/200] | Train Loss: 0.0532, Train Acc: 0.9899 | \nEpoch [82/200] | Train Loss: 0.0395, Train Acc: 0.9899 | \nEpoch [83/200] | Train Loss: 0.0482, Train Acc: 0.9832 | \nEpoch [84/200] | Train Loss: 0.0319, Train Acc: 0.9933 | \nEpoch [85/200] | Train Loss: 0.0287, Train Acc: 0.9933 | \nEpoch [86/200] | Train Loss: 0.0437, Train Acc: 0.9865 | \nEpoch [87/200] | Train Loss: 0.0283, Train Acc: 0.9966 | \nEpoch [88/200] | Train Loss: 0.0346, Train Acc: 0.9933 | \nEpoch [89/200] | Train Loss: 0.0360, Train Acc: 0.9933 | \nEpoch [90/200] | Train Loss: 0.0322, Train Acc: 0.9933 | \nEpoch [91/200] | Train Loss: 0.0340, Train Acc: 0.9933 | \nEpoch [92/200] | Train Loss: 0.0234, Train Acc: 0.9966 | \nEpoch [93/200] | Train Loss: 0.0264, Train Acc: 0.9933 | \nEpoch [94/200] | Train Loss: 0.0232, Train Acc: 0.9966 | \nEpoch [95/200] | Train Loss: 0.0380, Train Acc: 0.9899 | \nEpoch [96/200] | Train Loss: 0.0444, Train Acc: 0.9899 | \nEpoch [97/200] | Train Loss: 0.0340, Train Acc: 0.9933 | \nEpoch [98/200] | Train Loss: 0.0431, Train Acc: 0.9832 | \nEpoch [99/200] | Train Loss: 0.0394, Train Acc: 0.9899 | \nEpoch [100/200] | Train Loss: 0.0328, Train Acc: 0.9899 | \nEpoch [101/200] | Train Loss: 0.0265, Train Acc: 0.9966 | \nEpoch [102/200] | Train Loss: 0.0265, Train Acc: 0.9933 | \nEpoch [103/200] | Train Loss: 0.0223, Train Acc: 0.9966 | \nEpoch [104/200] | Train Loss: 0.0215, Train Acc: 0.9966 | \nEpoch [105/200] | Train Loss: 0.0241, Train Acc: 0.9933 | \nEpoch [106/200] | Train Loss: 0.0227, Train Acc: 0.9966 | \nEarly stopping triggered.\nTest Loss: 0.6940, Test Accuracy: 0.8267, Test AUC: 0.9097\n\n--- Processing: fc_alpha ---\nShape: (372, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7179, Train Acc: 0.5387 | \nEpoch [2/200] | Train Loss: 0.7023, Train Acc: 0.5455 | \nEpoch [3/200] | Train Loss: 0.7039, Train Acc: 0.5185 | \nEpoch [4/200] | Train Loss: 0.6941, Train Acc: 0.5185 | \nEpoch [5/200] | Train Loss: 0.6667, Train Acc: 0.6465 | \nEpoch [6/200] | Train Loss: 0.6296, Train Acc: 0.6465 | \nEpoch [7/200] | Train Loss: 0.5590, Train Acc: 0.6902 | \nEpoch [8/200] | Train Loss: 0.6207, Train Acc: 0.6633 | \nEpoch [9/200] | Train Loss: 0.5699, Train Acc: 0.6633 | \nEpoch [10/200] | Train Loss: 0.5225, Train Acc: 0.7306 | \nEpoch [11/200] | Train Loss: 0.4571, Train Acc: 0.7710 | \nEpoch [12/200] | Train Loss: 0.4978, Train Acc: 0.7239 | \nEpoch [13/200] | Train Loss: 0.5123, Train Acc: 0.7576 | \nEpoch [14/200] | Train Loss: 0.4282, Train Acc: 0.8013 | \nEpoch [15/200] | Train Loss: 0.4362, Train Acc: 0.7811 | \nEpoch [16/200] | Train Loss: 0.4191, Train Acc: 0.8215 | \nEpoch [17/200] | Train Loss: 0.3868, Train Acc: 0.8215 | \nEpoch [18/200] | Train Loss: 0.3752, Train Acc: 0.8249 | \nEpoch [19/200] | Train Loss: 0.4927, Train Acc: 0.7643 | \nEpoch [20/200] | Train Loss: 0.3468, Train Acc: 0.8653 | \nEpoch [21/200] | Train Loss: 0.3971, Train Acc: 0.8249 | \nEpoch [22/200] | Train Loss: 0.4263, Train Acc: 0.7946 | \nEpoch [23/200] | Train Loss: 0.3315, Train Acc: 0.8418 | \nEpoch [24/200] | Train Loss: 0.3082, Train Acc: 0.8855 | \nEpoch [25/200] | Train Loss: 0.3143, Train Acc: 0.8653 | \nEpoch [26/200] | Train Loss: 0.2943, Train Acc: 0.8586 | \nEpoch [27/200] | Train Loss: 0.2984, Train Acc: 0.8721 | \nEpoch [28/200] | Train Loss: 0.3631, Train Acc: 0.8586 | \nEpoch [29/200] | Train Loss: 0.3542, Train Acc: 0.8485 | \nEpoch [30/200] | Train Loss: 0.3234, Train Acc: 0.8620 | \nEpoch [31/200] | Train Loss: 0.3403, Train Acc: 0.8586 | \nEpoch [32/200] | Train Loss: 0.3040, Train Acc: 0.8687 | \nEpoch [33/200] | Train Loss: 0.2677, Train Acc: 0.8653 | \nEpoch [34/200] | Train Loss: 0.2848, Train Acc: 0.8889 | \nEpoch [35/200] | Train Loss: 0.2611, Train Acc: 0.9125 | \nEpoch [36/200] | Train Loss: 0.2145, Train Acc: 0.9192 | \nEpoch [37/200] | Train Loss: 0.2212, Train Acc: 0.8923 | \nEpoch [38/200] | Train Loss: 0.2508, Train Acc: 0.8956 | \nEpoch [39/200] | Train Loss: 0.4944, Train Acc: 0.8047 | \nEpoch [40/200] | Train Loss: 0.3795, Train Acc: 0.8384 | \nEpoch [41/200] | Train Loss: 0.3093, Train Acc: 0.8687 | \nEpoch [42/200] | Train Loss: 0.2685, Train Acc: 0.8889 | \nEpoch [43/200] | Train Loss: 0.2598, Train Acc: 0.8822 | \nEpoch [44/200] | Train Loss: 0.2315, Train Acc: 0.8855 | \nEpoch [45/200] | Train Loss: 0.2529, Train Acc: 0.8923 | \nEpoch [46/200] | Train Loss: 0.2538, Train Acc: 0.9024 | \nEpoch 00047: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [47/200] | Train Loss: 0.3680, Train Acc: 0.8215 | \nEpoch [48/200] | Train Loss: 0.3216, Train Acc: 0.8384 | \nEpoch [49/200] | Train Loss: 0.2633, Train Acc: 0.8956 | \nEpoch [50/200] | Train Loss: 0.2371, Train Acc: 0.9158 | \nEpoch [51/200] | Train Loss: 0.2114, Train Acc: 0.9192 | \nEpoch [52/200] | Train Loss: 0.2142, Train Acc: 0.9226 | \nEpoch [53/200] | Train Loss: 0.1941, Train Acc: 0.9259 | \nEpoch [54/200] | Train Loss: 0.1668, Train Acc: 0.9495 | \nEpoch [55/200] | Train Loss: 0.1574, Train Acc: 0.9461 | \nEpoch [56/200] | Train Loss: 0.1632, Train Acc: 0.9461 | \nEpoch [57/200] | Train Loss: 0.1590, Train Acc: 0.9428 | \nEpoch [58/200] | Train Loss: 0.1263, Train Acc: 0.9731 | \nEpoch [59/200] | Train Loss: 0.1324, Train Acc: 0.9529 | \nEpoch [60/200] | Train Loss: 0.1284, Train Acc: 0.9630 | \nEpoch [61/200] | Train Loss: 0.1076, Train Acc: 0.9663 | \nEpoch [62/200] | Train Loss: 0.0996, Train Acc: 0.9731 | \nEpoch [63/200] | Train Loss: 0.1027, Train Acc: 0.9663 | \nEpoch [64/200] | Train Loss: 0.0961, Train Acc: 0.9697 | \nEpoch [65/200] | Train Loss: 0.0888, Train Acc: 0.9663 | \nEpoch [66/200] | Train Loss: 0.1024, Train Acc: 0.9697 | \nEpoch [67/200] | Train Loss: 0.1099, Train Acc: 0.9697 | \nEpoch [68/200] | Train Loss: 0.0747, Train Acc: 0.9798 | \nEpoch [69/200] | Train Loss: 0.0838, Train Acc: 0.9596 | \nEpoch [70/200] | Train Loss: 0.0838, Train Acc: 0.9697 | \nEpoch [71/200] | Train Loss: 0.0783, Train Acc: 0.9697 | \nEpoch [72/200] | Train Loss: 0.0637, Train Acc: 0.9832 | \nEpoch [73/200] | Train Loss: 0.0590, Train Acc: 0.9798 | \nEpoch [74/200] | Train Loss: 0.0501, Train Acc: 0.9832 | \nEpoch [75/200] | Train Loss: 0.0586, Train Acc: 0.9798 | \nEpoch [76/200] | Train Loss: 0.0507, Train Acc: 0.9798 | \nEpoch [77/200] | Train Loss: 0.0483, Train Acc: 0.9832 | \nEpoch [78/200] | Train Loss: 0.0617, Train Acc: 0.9832 | \nEpoch [79/200] | Train Loss: 0.0507, Train Acc: 0.9832 | \nEpoch [80/200] | Train Loss: 0.0304, Train Acc: 0.9933 | \nEpoch [81/200] | Train Loss: 0.0371, Train Acc: 0.9865 | \nEpoch [82/200] | Train Loss: 0.0279, Train Acc: 0.9933 | \nEpoch [83/200] | Train Loss: 0.0468, Train Acc: 0.9865 | \nEpoch [84/200] | Train Loss: 0.0238, Train Acc: 0.9933 | \nEpoch [85/200] | Train Loss: 0.0296, Train Acc: 0.9899 | \nEpoch [86/200] | Train Loss: 0.0366, Train Acc: 0.9865 | \nEpoch [87/200] | Train Loss: 0.0335, Train Acc: 0.9899 | \nEpoch [88/200] | Train Loss: 0.0319, Train Acc: 0.9966 | \nEpoch [89/200] | Train Loss: 0.0194, Train Acc: 0.9933 | \nEpoch [90/200] | Train Loss: 0.0244, Train Acc: 0.9899 | \nEpoch [91/200] | Train Loss: 0.0564, Train Acc: 0.9832 | \nEpoch [92/200] | Train Loss: 0.0593, Train Acc: 0.9798 | \nEpoch [93/200] | Train Loss: 0.0536, Train Acc: 0.9832 | \nEpoch [94/200] | Train Loss: 0.0862, Train Acc: 0.9731 | \nEpoch [95/200] | Train Loss: 0.1105, Train Acc: 0.9562 | \nEpoch [96/200] | Train Loss: 0.0929, Train Acc: 0.9731 | \nEpoch [97/200] | Train Loss: 0.0773, Train Acc: 0.9630 | \nEpoch [98/200] | Train Loss: 0.0660, Train Acc: 0.9764 | \nEpoch [99/200] | Train Loss: 0.0658, Train Acc: 0.9832 | \nEpoch 00100: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [100/200] | Train Loss: 0.0421, Train Acc: 0.9832 | \nEpoch [101/200] | Train Loss: 0.0437, Train Acc: 0.9865 | \nEpoch [102/200] | Train Loss: 0.0339, Train Acc: 0.9933 | \nEpoch [103/200] | Train Loss: 0.0465, Train Acc: 0.9865 | \nEpoch [104/200] | Train Loss: 0.0300, Train Acc: 0.9933 | \nEpoch [105/200] | Train Loss: 0.0275, Train Acc: 0.9933 | \nEpoch [106/200] | Train Loss: 0.0244, Train Acc: 0.9933 | \nEpoch [107/200] | Train Loss: 0.0377, Train Acc: 0.9933 | \nEarly stopping triggered.\nTest Loss: 0.5541, Test Accuracy: 0.8533, Test AUC: 0.9218\n\n--- Processing: fc_beta ---\nShape: (372, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7812, Train Acc: 0.4882 | \nEpoch [2/200] | Train Loss: 0.7013, Train Acc: 0.5118 | \nEpoch [3/200] | Train Loss: 0.6970, Train Acc: 0.4983 | \nEpoch [4/200] | Train Loss: 0.6919, Train Acc: 0.5017 | \nEpoch [5/200] | Train Loss: 0.6923, Train Acc: 0.5084 | \nEpoch [6/200] | Train Loss: 0.6788, Train Acc: 0.5387 | \nEpoch [7/200] | Train Loss: 0.6176, Train Acc: 0.6700 | \nEpoch [8/200] | Train Loss: 0.5445, Train Acc: 0.7340 | \nEpoch [9/200] | Train Loss: 0.5464, Train Acc: 0.7071 | \nEpoch [10/200] | Train Loss: 0.4402, Train Acc: 0.8081 | \nEpoch [11/200] | Train Loss: 0.4303, Train Acc: 0.8182 | \nEpoch [12/200] | Train Loss: 0.4419, Train Acc: 0.8114 | \nEpoch [13/200] | Train Loss: 0.3637, Train Acc: 0.8384 | \nEpoch [14/200] | Train Loss: 0.3332, Train Acc: 0.8451 | \nEpoch [15/200] | Train Loss: 0.4088, Train Acc: 0.8215 | \nEpoch [16/200] | Train Loss: 0.3897, Train Acc: 0.8215 | \nEpoch [17/200] | Train Loss: 0.2900, Train Acc: 0.8923 | \nEpoch [18/200] | Train Loss: 0.2624, Train Acc: 0.8754 | \nEpoch [19/200] | Train Loss: 0.3008, Train Acc: 0.8687 | \nEpoch [20/200] | Train Loss: 0.3107, Train Acc: 0.8586 | \nEpoch [21/200] | Train Loss: 0.3180, Train Acc: 0.8519 | \nEpoch [22/200] | Train Loss: 0.3442, Train Acc: 0.8519 | \nEpoch [23/200] | Train Loss: 0.2649, Train Acc: 0.8855 | \nEpoch [24/200] | Train Loss: 0.3302, Train Acc: 0.8721 | \nEpoch [25/200] | Train Loss: 0.3331, Train Acc: 0.8384 | \nEpoch [26/200] | Train Loss: 0.2331, Train Acc: 0.8956 | \nEpoch [27/200] | Train Loss: 0.2126, Train Acc: 0.9091 | \nEpoch [28/200] | Train Loss: 0.3905, Train Acc: 0.8114 | \nEpoch [29/200] | Train Loss: 0.3593, Train Acc: 0.8316 | \nEpoch [30/200] | Train Loss: 0.3281, Train Acc: 0.8451 | \nEpoch [31/200] | Train Loss: 0.2087, Train Acc: 0.9158 | \nEpoch [32/200] | Train Loss: 0.2850, Train Acc: 0.8923 | \nEpoch [33/200] | Train Loss: 0.2193, Train Acc: 0.9125 | \nEpoch [34/200] | Train Loss: 0.1869, Train Acc: 0.9259 | \nEpoch [35/200] | Train Loss: 0.1994, Train Acc: 0.9192 | \nEpoch [36/200] | Train Loss: 0.5258, Train Acc: 0.7710 | \nEpoch [37/200] | Train Loss: 0.3912, Train Acc: 0.8249 | \nEpoch [38/200] | Train Loss: 0.3628, Train Acc: 0.8586 | \nEpoch [39/200] | Train Loss: 0.3125, Train Acc: 0.8889 | \nEpoch [40/200] | Train Loss: 0.3127, Train Acc: 0.8687 | \nEpoch [41/200] | Train Loss: 0.2759, Train Acc: 0.8586 | \nEpoch [42/200] | Train Loss: 0.2634, Train Acc: 0.8889 | \nEpoch [43/200] | Train Loss: 0.1953, Train Acc: 0.9394 | \nEpoch [44/200] | Train Loss: 0.2252, Train Acc: 0.9192 | \nEpoch 00045: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [45/200] | Train Loss: 0.2087, Train Acc: 0.9091 | \nEpoch [46/200] | Train Loss: 0.2518, Train Acc: 0.9259 | \nEpoch [47/200] | Train Loss: 0.1819, Train Acc: 0.9394 | \nEpoch [48/200] | Train Loss: 0.1625, Train Acc: 0.9360 | \nEpoch [49/200] | Train Loss: 0.1431, Train Acc: 0.9529 | \nEpoch [50/200] | Train Loss: 0.1344, Train Acc: 0.9461 | \nEpoch [51/200] | Train Loss: 0.1425, Train Acc: 0.9529 | \nEpoch [52/200] | Train Loss: 0.1263, Train Acc: 0.9529 | \nEpoch [53/200] | Train Loss: 0.1233, Train Acc: 0.9495 | \nEpoch [54/200] | Train Loss: 0.1207, Train Acc: 0.9529 | \nEpoch [55/200] | Train Loss: 0.1232, Train Acc: 0.9529 | \nEpoch [56/200] | Train Loss: 0.0911, Train Acc: 0.9731 | \nEpoch [57/200] | Train Loss: 0.1014, Train Acc: 0.9663 | \nEpoch [58/200] | Train Loss: 0.1138, Train Acc: 0.9529 | \nEpoch [59/200] | Train Loss: 0.0896, Train Acc: 0.9663 | \nEpoch [60/200] | Train Loss: 0.0911, Train Acc: 0.9697 | \nEpoch [61/200] | Train Loss: 0.0886, Train Acc: 0.9697 | \nEpoch [62/200] | Train Loss: 0.0709, Train Acc: 0.9798 | \nEpoch [63/200] | Train Loss: 0.0773, Train Acc: 0.9764 | \nEpoch [64/200] | Train Loss: 0.0851, Train Acc: 0.9731 | \nEpoch [65/200] | Train Loss: 0.0751, Train Acc: 0.9663 | \nEpoch [66/200] | Train Loss: 0.0803, Train Acc: 0.9630 | \nEpoch [67/200] | Train Loss: 0.0632, Train Acc: 0.9798 | \nEpoch [68/200] | Train Loss: 0.0582, Train Acc: 0.9832 | \nEpoch [69/200] | Train Loss: 0.0783, Train Acc: 0.9697 | \nEpoch [70/200] | Train Loss: 0.0553, Train Acc: 0.9832 | \nEpoch [71/200] | Train Loss: 0.0630, Train Acc: 0.9663 | \nEpoch [72/200] | Train Loss: 0.0600, Train Acc: 0.9865 | \nEpoch [73/200] | Train Loss: 0.0799, Train Acc: 0.9596 | \nEpoch [74/200] | Train Loss: 0.0539, Train Acc: 0.9865 | \nEpoch [75/200] | Train Loss: 0.0507, Train Acc: 0.9865 | \nEpoch [76/200] | Train Loss: 0.0596, Train Acc: 0.9764 | \nEpoch [77/200] | Train Loss: 0.0646, Train Acc: 0.9764 | \nEpoch [78/200] | Train Loss: 0.0487, Train Acc: 0.9798 | \nEpoch [79/200] | Train Loss: 0.0715, Train Acc: 0.9764 | \nEpoch [80/200] | Train Loss: 0.0623, Train Acc: 0.9764 | \nEpoch [81/200] | Train Loss: 0.0517, Train Acc: 0.9764 | \nEpoch [82/200] | Train Loss: 0.0499, Train Acc: 0.9865 | \nEpoch [83/200] | Train Loss: 0.0503, Train Acc: 0.9832 | \nEpoch [84/200] | Train Loss: 0.0747, Train Acc: 0.9764 | \nEpoch [85/200] | Train Loss: 0.0545, Train Acc: 0.9764 | \nEpoch [86/200] | Train Loss: 0.0418, Train Acc: 0.9832 | \nEpoch [87/200] | Train Loss: 0.0378, Train Acc: 0.9933 | \nEpoch [88/200] | Train Loss: 0.0840, Train Acc: 0.9697 | \nEpoch [89/200] | Train Loss: 0.0618, Train Acc: 0.9764 | \nEpoch [90/200] | Train Loss: 0.0318, Train Acc: 0.9933 | \nEpoch [91/200] | Train Loss: 0.0348, Train Acc: 0.9899 | \nEpoch [92/200] | Train Loss: 0.0322, Train Acc: 0.9933 | \nEpoch [93/200] | Train Loss: 0.0294, Train Acc: 0.9899 | \nEpoch [94/200] | Train Loss: 0.0662, Train Acc: 0.9697 | \nEpoch [95/200] | Train Loss: 0.0286, Train Acc: 0.9899 | \nEpoch [96/200] | Train Loss: 0.0303, Train Acc: 0.9899 | \nEpoch [97/200] | Train Loss: 0.0276, Train Acc: 0.9966 | \nEpoch [98/200] | Train Loss: 0.0340, Train Acc: 0.9933 | \nEpoch [99/200] | Train Loss: 0.0378, Train Acc: 0.9832 | \nEpoch [100/200] | Train Loss: 0.0358, Train Acc: 0.9865 | \nEpoch [101/200] | Train Loss: 0.0405, Train Acc: 0.9832 | \nEpoch [102/200] | Train Loss: 0.0386, Train Acc: 0.9865 | \nEpoch [103/200] | Train Loss: 0.0205, Train Acc: 0.9966 | \nEpoch [104/200] | Train Loss: 0.0226, Train Acc: 0.9933 | \nEpoch [105/200] | Train Loss: 0.0223, Train Acc: 0.9966 | \nEpoch [106/200] | Train Loss: 0.0238, Train Acc: 0.9899 | \nEpoch [107/200] | Train Loss: 0.0153, Train Acc: 0.9966 | \nEpoch [108/200] | Train Loss: 0.0172, Train Acc: 0.9933 | \nEpoch [109/200] | Train Loss: 0.0358, Train Acc: 0.9832 | \nEpoch [110/200] | Train Loss: 0.0468, Train Acc: 0.9832 | \nEpoch [111/200] | Train Loss: 0.0245, Train Acc: 0.9966 | \nEpoch [112/200] | Train Loss: 0.0139, Train Acc: 0.9966 | \nEpoch [113/200] | Train Loss: 0.0112, Train Acc: 1.0000 | \nEpoch [114/200] | Train Loss: 0.0125, Train Acc: 0.9966 | \nEpoch [115/200] | Train Loss: 0.0153, Train Acc: 0.9966 | \nEpoch [116/200] | Train Loss: 0.0180, Train Acc: 0.9966 | \nEpoch [117/200] | Train Loss: 0.0176, Train Acc: 0.9966 | \nEpoch [118/200] | Train Loss: 0.0466, Train Acc: 0.9933 | \nEpoch [119/200] | Train Loss: 0.0395, Train Acc: 0.9865 | \nEpoch [120/200] | Train Loss: 0.0384, Train Acc: 0.9865 | \nEpoch [121/200] | Train Loss: 0.0133, Train Acc: 0.9966 | \nEpoch [122/200] | Train Loss: 0.0197, Train Acc: 0.9966 | \nEpoch [123/200] | Train Loss: 0.0328, Train Acc: 0.9899 | \nEpoch 00124: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [124/200] | Train Loss: 0.0210, Train Acc: 0.9933 | \nEpoch [125/200] | Train Loss: 0.0212, Train Acc: 0.9966 | \nEpoch [126/200] | Train Loss: 0.0132, Train Acc: 0.9966 | \nEpoch [127/200] | Train Loss: 0.0072, Train Acc: 1.0000 | \nEpoch [128/200] | Train Loss: 0.0206, Train Acc: 0.9899 | \nEpoch [129/200] | Train Loss: 0.0122, Train Acc: 0.9966 | \nEpoch [130/200] | Train Loss: 0.0095, Train Acc: 1.0000 | \nEpoch [131/200] | Train Loss: 0.0071, Train Acc: 1.0000 | \nEpoch [132/200] | Train Loss: 0.0102, Train Acc: 0.9966 | \nEarly stopping triggered.\nTest Loss: 0.8074, Test Accuracy: 0.8133, Test AUC: 0.8976\n\n--- Processing: fc_highbeta ---\nShape: (372, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7438, Train Acc: 0.5185 | \nEpoch [2/200] | Train Loss: 0.6930, Train Acc: 0.5051 | \nEpoch [3/200] | Train Loss: 0.6863, Train Acc: 0.5724 | \nEpoch [4/200] | Train Loss: 0.6688, Train Acc: 0.6431 | \nEpoch [5/200] | Train Loss: 0.6310, Train Acc: 0.6465 | \nEpoch [6/200] | Train Loss: 0.5696, Train Acc: 0.7037 | \nEpoch [7/200] | Train Loss: 0.4608, Train Acc: 0.7811 | \nEpoch [8/200] | Train Loss: 0.4900, Train Acc: 0.7744 | \nEpoch [9/200] | Train Loss: 0.4967, Train Acc: 0.7744 | \nEpoch [10/200] | Train Loss: 0.4456, Train Acc: 0.8047 | \nEpoch [11/200] | Train Loss: 0.4599, Train Acc: 0.7677 | \nEpoch [12/200] | Train Loss: 0.4223, Train Acc: 0.7946 | \nEpoch [13/200] | Train Loss: 0.4981, Train Acc: 0.7744 | \nEpoch [14/200] | Train Loss: 0.4183, Train Acc: 0.8013 | \nEpoch [15/200] | Train Loss: 0.3459, Train Acc: 0.8653 | \nEpoch [16/200] | Train Loss: 0.3294, Train Acc: 0.8788 | \nEpoch [17/200] | Train Loss: 0.3034, Train Acc: 0.8754 | \nEpoch [18/200] | Train Loss: 0.3592, Train Acc: 0.8653 | \nEpoch [19/200] | Train Loss: 0.3542, Train Acc: 0.8485 | \nEpoch [20/200] | Train Loss: 0.2990, Train Acc: 0.8754 | \nEpoch [21/200] | Train Loss: 0.4019, Train Acc: 0.8249 | \nEpoch [22/200] | Train Loss: 0.2948, Train Acc: 0.8822 | \nEpoch [23/200] | Train Loss: 0.2846, Train Acc: 0.8855 | \nEpoch [24/200] | Train Loss: 0.2686, Train Acc: 0.8889 | \nEpoch [25/200] | Train Loss: 0.3088, Train Acc: 0.8788 | \nEpoch [26/200] | Train Loss: 0.3062, Train Acc: 0.8788 | \nEpoch [27/200] | Train Loss: 0.2756, Train Acc: 0.8956 | \nEpoch [28/200] | Train Loss: 0.2344, Train Acc: 0.9091 | \nEpoch [29/200] | Train Loss: 0.2189, Train Acc: 0.9259 | \nEpoch [30/200] | Train Loss: 0.3132, Train Acc: 0.8620 | \nEpoch [31/200] | Train Loss: 0.2745, Train Acc: 0.8923 | \nEpoch [32/200] | Train Loss: 0.3800, Train Acc: 0.8249 | \nEpoch [33/200] | Train Loss: 0.3000, Train Acc: 0.8620 | \nEpoch [34/200] | Train Loss: 0.2705, Train Acc: 0.9057 | \nEpoch [35/200] | Train Loss: 0.2237, Train Acc: 0.9091 | \nEpoch [36/200] | Train Loss: 0.3084, Train Acc: 0.8754 | \nEpoch [37/200] | Train Loss: 0.3069, Train Acc: 0.8620 | \nEpoch [38/200] | Train Loss: 0.2404, Train Acc: 0.8990 | \nEpoch [39/200] | Train Loss: 0.2424, Train Acc: 0.8990 | \nEpoch 00040: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [40/200] | Train Loss: 0.2329, Train Acc: 0.9158 | \nEpoch [41/200] | Train Loss: 0.2269, Train Acc: 0.8956 | \nEpoch [42/200] | Train Loss: 0.1793, Train Acc: 0.9259 | \nEpoch [43/200] | Train Loss: 0.1608, Train Acc: 0.9428 | \nEpoch [44/200] | Train Loss: 0.1610, Train Acc: 0.9428 | \nEpoch [45/200] | Train Loss: 0.1327, Train Acc: 0.9495 | \nEpoch [46/200] | Train Loss: 0.1181, Train Acc: 0.9630 | \nEpoch [47/200] | Train Loss: 0.1304, Train Acc: 0.9562 | \nEpoch [48/200] | Train Loss: 0.1074, Train Acc: 0.9630 | \nEpoch [49/200] | Train Loss: 0.1234, Train Acc: 0.9529 | \nEpoch [50/200] | Train Loss: 0.1142, Train Acc: 0.9596 | \nEpoch [51/200] | Train Loss: 0.1143, Train Acc: 0.9663 | \nEpoch [52/200] | Train Loss: 0.1045, Train Acc: 0.9663 | \nEpoch [53/200] | Train Loss: 0.1111, Train Acc: 0.9630 | \nEpoch [54/200] | Train Loss: 0.0980, Train Acc: 0.9697 | \nEpoch [55/200] | Train Loss: 0.0927, Train Acc: 0.9697 | \nEpoch [56/200] | Train Loss: 0.0952, Train Acc: 0.9731 | \nEpoch [57/200] | Train Loss: 0.1039, Train Acc: 0.9630 | \nEpoch [58/200] | Train Loss: 0.0843, Train Acc: 0.9731 | \nEpoch [59/200] | Train Loss: 0.0842, Train Acc: 0.9697 | \nEpoch [60/200] | Train Loss: 0.0887, Train Acc: 0.9697 | \nEpoch [61/200] | Train Loss: 0.0703, Train Acc: 0.9798 | \nEpoch [62/200] | Train Loss: 0.0736, Train Acc: 0.9798 | \nEpoch [63/200] | Train Loss: 0.0835, Train Acc: 0.9731 | \nEpoch [64/200] | Train Loss: 0.0663, Train Acc: 0.9731 | \nEpoch [65/200] | Train Loss: 0.0908, Train Acc: 0.9663 | \nEpoch [66/200] | Train Loss: 0.0539, Train Acc: 0.9865 | \nEpoch [67/200] | Train Loss: 0.0556, Train Acc: 0.9865 | \nEpoch [68/200] | Train Loss: 0.0509, Train Acc: 0.9865 | \nEpoch [69/200] | Train Loss: 0.0706, Train Acc: 0.9798 | \nEpoch [70/200] | Train Loss: 0.0659, Train Acc: 0.9798 | \nEpoch [71/200] | Train Loss: 0.0748, Train Acc: 0.9764 | \nEpoch [72/200] | Train Loss: 0.0568, Train Acc: 0.9832 | \nEpoch [73/200] | Train Loss: 0.0680, Train Acc: 0.9798 | \nEpoch [74/200] | Train Loss: 0.0686, Train Acc: 0.9731 | \nEpoch [75/200] | Train Loss: 0.0471, Train Acc: 0.9865 | \nEpoch [76/200] | Train Loss: 0.0419, Train Acc: 0.9933 | \nEpoch [77/200] | Train Loss: 0.0353, Train Acc: 0.9899 | \nEpoch [78/200] | Train Loss: 0.0490, Train Acc: 0.9865 | \nEpoch [79/200] | Train Loss: 0.0490, Train Acc: 0.9865 | \nEpoch [80/200] | Train Loss: 0.0408, Train Acc: 0.9933 | \nEpoch [81/200] | Train Loss: 0.0546, Train Acc: 0.9865 | \nEpoch [82/200] | Train Loss: 0.0358, Train Acc: 0.9933 | \nEpoch [83/200] | Train Loss: 0.0354, Train Acc: 0.9933 | \nEpoch [84/200] | Train Loss: 0.0303, Train Acc: 0.9933 | \nEpoch [85/200] | Train Loss: 0.0300, Train Acc: 0.9966 | \nEpoch [86/200] | Train Loss: 0.0490, Train Acc: 0.9865 | \nEpoch [87/200] | Train Loss: 0.0346, Train Acc: 0.9899 | \nEpoch [88/200] | Train Loss: 0.0368, Train Acc: 0.9865 | \nEpoch [89/200] | Train Loss: 0.0448, Train Acc: 0.9899 | \nEpoch [90/200] | Train Loss: 0.0362, Train Acc: 0.9933 | \nEpoch [91/200] | Train Loss: 0.0411, Train Acc: 0.9865 | \nEpoch [92/200] | Train Loss: 0.0439, Train Acc: 0.9865 | \nEpoch [93/200] | Train Loss: 0.0363, Train Acc: 0.9899 | \nEpoch [94/200] | Train Loss: 0.0423, Train Acc: 0.9899 | \nEpoch [95/200] | Train Loss: 0.0514, Train Acc: 0.9865 | \nEpoch 00096: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [96/200] | Train Loss: 0.0635, Train Acc: 0.9764 | \nEpoch [97/200] | Train Loss: 0.0508, Train Acc: 0.9865 | \nEpoch [98/200] | Train Loss: 0.0344, Train Acc: 0.9933 | \nEpoch [99/200] | Train Loss: 0.0310, Train Acc: 0.9899 | \nEpoch [100/200] | Train Loss: 0.0293, Train Acc: 0.9933 | \nEpoch [101/200] | Train Loss: 0.0284, Train Acc: 0.9966 | \nEpoch [102/200] | Train Loss: 0.0269, Train Acc: 0.9933 | \nEpoch [103/200] | Train Loss: 0.0299, Train Acc: 0.9933 | \nEpoch [104/200] | Train Loss: 0.0297, Train Acc: 0.9933 | \nEarly stopping triggered.\nTest Loss: 0.7118, Test Accuracy: 0.8667, Test AUC: 0.8898\n\n--- Processing: fc_gamma ---\nShape: (372, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7437, Train Acc: 0.5084 | \nEpoch [2/200] | Train Loss: 0.7038, Train Acc: 0.4646 | \nEpoch [3/200] | Train Loss: 0.6971, Train Acc: 0.4949 | \nEpoch [4/200] | Train Loss: 0.6936, Train Acc: 0.4983 | \nEpoch [5/200] | Train Loss: 0.6852, Train Acc: 0.5724 | \nEpoch [6/200] | Train Loss: 0.6758, Train Acc: 0.5791 | \nEpoch [7/200] | Train Loss: 0.5873, Train Acc: 0.7172 | \nEpoch [8/200] | Train Loss: 0.5654, Train Acc: 0.7441 | \nEpoch [9/200] | Train Loss: 0.5350, Train Acc: 0.7306 | \nEpoch [10/200] | Train Loss: 0.4602, Train Acc: 0.7879 | \nEpoch [11/200] | Train Loss: 0.4677, Train Acc: 0.7710 | \nEpoch [12/200] | Train Loss: 0.4072, Train Acc: 0.8148 | \nEpoch [13/200] | Train Loss: 0.3656, Train Acc: 0.8620 | \nEpoch [14/200] | Train Loss: 0.3727, Train Acc: 0.8384 | \nEpoch [15/200] | Train Loss: 0.3904, Train Acc: 0.8350 | \nEpoch [16/200] | Train Loss: 0.4152, Train Acc: 0.8148 | \nEpoch [17/200] | Train Loss: 0.3737, Train Acc: 0.8249 | \nEpoch [18/200] | Train Loss: 0.3214, Train Acc: 0.8687 | \nEpoch [19/200] | Train Loss: 0.3830, Train Acc: 0.8316 | \nEpoch [20/200] | Train Loss: 0.3692, Train Acc: 0.8384 | \nEpoch [21/200] | Train Loss: 0.3460, Train Acc: 0.8485 | \nEpoch [22/200] | Train Loss: 0.2964, Train Acc: 0.8788 | \nEpoch [23/200] | Train Loss: 0.2489, Train Acc: 0.9158 | \nEpoch [24/200] | Train Loss: 0.2759, Train Acc: 0.8923 | \nEpoch [25/200] | Train Loss: 0.4387, Train Acc: 0.8081 | \nEpoch [26/200] | Train Loss: 0.4156, Train Acc: 0.8081 | \nEpoch [27/200] | Train Loss: 0.3336, Train Acc: 0.8586 | \nEpoch [28/200] | Train Loss: 0.3096, Train Acc: 0.8687 | \nEpoch [29/200] | Train Loss: 0.3207, Train Acc: 0.8552 | \nEpoch [30/200] | Train Loss: 0.2727, Train Acc: 0.8822 | \nEpoch [31/200] | Train Loss: 0.2444, Train Acc: 0.9192 | \nEpoch [32/200] | Train Loss: 0.2161, Train Acc: 0.9226 | \nEpoch [33/200] | Train Loss: 0.2669, Train Acc: 0.9057 | \nEpoch [34/200] | Train Loss: 0.2643, Train Acc: 0.8822 | \nEpoch [35/200] | Train Loss: 0.2391, Train Acc: 0.9259 | \nEpoch [36/200] | Train Loss: 0.2681, Train Acc: 0.8889 | \nEpoch [37/200] | Train Loss: 0.2416, Train Acc: 0.9091 | \nEpoch [38/200] | Train Loss: 0.2284, Train Acc: 0.9125 | \nEpoch [39/200] | Train Loss: 0.1818, Train Acc: 0.9293 | \nEpoch [40/200] | Train Loss: 0.1747, Train Acc: 0.9461 | \nEpoch [41/200] | Train Loss: 0.2023, Train Acc: 0.9327 | \nEpoch [42/200] | Train Loss: 0.3620, Train Acc: 0.8586 | \nEpoch [43/200] | Train Loss: 0.3874, Train Acc: 0.8418 | \nEpoch [44/200] | Train Loss: 0.2760, Train Acc: 0.8956 | \nEpoch [45/200] | Train Loss: 0.2501, Train Acc: 0.8990 | \nEpoch [46/200] | Train Loss: 0.2075, Train Acc: 0.9226 | \nEpoch [47/200] | Train Loss: 0.1976, Train Acc: 0.9259 | \nEpoch [48/200] | Train Loss: 0.2616, Train Acc: 0.9024 | \nEpoch [49/200] | Train Loss: 0.2589, Train Acc: 0.9125 | \nEpoch [50/200] | Train Loss: 0.1930, Train Acc: 0.9327 | \nEpoch [51/200] | Train Loss: 0.1651, Train Acc: 0.9394 | \nEpoch [52/200] | Train Loss: 0.1342, Train Acc: 0.9562 | \nEpoch [53/200] | Train Loss: 0.1913, Train Acc: 0.9360 | \nEpoch [54/200] | Train Loss: 0.1583, Train Acc: 0.9428 | \nEpoch [55/200] | Train Loss: 0.1980, Train Acc: 0.9192 | \nEpoch [56/200] | Train Loss: 0.2570, Train Acc: 0.8956 | \nEpoch [57/200] | Train Loss: 0.1959, Train Acc: 0.9293 | \nEpoch [58/200] | Train Loss: 0.3149, Train Acc: 0.8754 | \nEpoch [59/200] | Train Loss: 0.3004, Train Acc: 0.8788 | \nEpoch [60/200] | Train Loss: 0.2284, Train Acc: 0.9259 | \nEpoch [61/200] | Train Loss: 0.2070, Train Acc: 0.9259 | \nEpoch [62/200] | Train Loss: 0.1781, Train Acc: 0.9360 | \nEpoch 00063: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [63/200] | Train Loss: 0.2011, Train Acc: 0.9226 | \nEpoch [64/200] | Train Loss: 0.1621, Train Acc: 0.9461 | \nEpoch [65/200] | Train Loss: 0.1450, Train Acc: 0.9495 | \nEpoch [66/200] | Train Loss: 0.1379, Train Acc: 0.9495 | \nEpoch [67/200] | Train Loss: 0.1267, Train Acc: 0.9596 | \nEpoch [68/200] | Train Loss: 0.1221, Train Acc: 0.9596 | \nEpoch [69/200] | Train Loss: 0.1171, Train Acc: 0.9697 | \nEpoch [70/200] | Train Loss: 0.1118, Train Acc: 0.9697 | \nEpoch [71/200] | Train Loss: 0.1140, Train Acc: 0.9630 | \nEpoch [72/200] | Train Loss: 0.1080, Train Acc: 0.9663 | \nEpoch [73/200] | Train Loss: 0.1037, Train Acc: 0.9731 | \nEpoch [74/200] | Train Loss: 0.0988, Train Acc: 0.9764 | \nEpoch [75/200] | Train Loss: 0.1073, Train Acc: 0.9663 | \nEpoch [76/200] | Train Loss: 0.1113, Train Acc: 0.9697 | \nEpoch [77/200] | Train Loss: 0.1087, Train Acc: 0.9697 | \nEpoch [78/200] | Train Loss: 0.0941, Train Acc: 0.9764 | \nEpoch [79/200] | Train Loss: 0.0971, Train Acc: 0.9697 | \nEpoch [80/200] | Train Loss: 0.0940, Train Acc: 0.9764 | \nEpoch [81/200] | Train Loss: 0.0904, Train Acc: 0.9731 | \nEpoch [82/200] | Train Loss: 0.0904, Train Acc: 0.9764 | \nEpoch [83/200] | Train Loss: 0.0885, Train Acc: 0.9798 | \nEpoch [84/200] | Train Loss: 0.0953, Train Acc: 0.9798 | \nEpoch [85/200] | Train Loss: 0.0959, Train Acc: 0.9731 | \nEpoch [86/200] | Train Loss: 0.0927, Train Acc: 0.9764 | \nEpoch [87/200] | Train Loss: 0.0877, Train Acc: 0.9798 | \nEpoch [88/200] | Train Loss: 0.0992, Train Acc: 0.9697 | \nEpoch [89/200] | Train Loss: 0.1019, Train Acc: 0.9764 | \nEpoch [90/200] | Train Loss: 0.0966, Train Acc: 0.9731 | \nEpoch [91/200] | Train Loss: 0.0967, Train Acc: 0.9731 | \nEpoch [92/200] | Train Loss: 0.0981, Train Acc: 0.9697 | \nEpoch [93/200] | Train Loss: 0.1040, Train Acc: 0.9731 | \nEpoch [94/200] | Train Loss: 0.1009, Train Acc: 0.9697 | \nEpoch [95/200] | Train Loss: 0.0896, Train Acc: 0.9798 | \nEpoch [96/200] | Train Loss: 0.0850, Train Acc: 0.9764 | \nEpoch [97/200] | Train Loss: 0.0971, Train Acc: 0.9731 | \nEpoch [98/200] | Train Loss: 0.0990, Train Acc: 0.9697 | \nEpoch [99/200] | Train Loss: 0.1031, Train Acc: 0.9697 | \nEpoch [100/200] | Train Loss: 0.1049, Train Acc: 0.9663 | \nEpoch [101/200] | Train Loss: 0.0875, Train Acc: 0.9764 | \nEpoch [102/200] | Train Loss: 0.0854, Train Acc: 0.9798 | \nEarly stopping triggered.\nTest Loss: 0.4159, Test Accuracy: 0.8933, Test AUC: 0.9410\n\n--- Processing: psd_fc_delta ---\nShape: (372, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7636, Train Acc: 0.4747 | \nEpoch [2/200] | Train Loss: 0.7254, Train Acc: 0.5051 | \nEpoch [3/200] | Train Loss: 0.7130, Train Acc: 0.5152 | \nEpoch [4/200] | Train Loss: 0.7209, Train Acc: 0.5017 | \nEpoch [5/200] | Train Loss: 0.6951, Train Acc: 0.5286 | \nEpoch [6/200] | Train Loss: 0.6781, Train Acc: 0.5993 | \nEpoch [7/200] | Train Loss: 0.6751, Train Acc: 0.5758 | \nEpoch [8/200] | Train Loss: 0.6107, Train Acc: 0.6936 | \nEpoch [9/200] | Train Loss: 0.5158, Train Acc: 0.7407 | \nEpoch [10/200] | Train Loss: 0.5843, Train Acc: 0.7037 | \nEpoch [11/200] | Train Loss: 0.5600, Train Acc: 0.7104 | \nEpoch [12/200] | Train Loss: 0.4829, Train Acc: 0.7778 | \nEpoch [13/200] | Train Loss: 0.4419, Train Acc: 0.8047 | \nEpoch [14/200] | Train Loss: 0.4522, Train Acc: 0.7980 | \nEpoch [15/200] | Train Loss: 0.4439, Train Acc: 0.8081 | \nEpoch [16/200] | Train Loss: 0.3933, Train Acc: 0.8114 | \nEpoch [17/200] | Train Loss: 0.4210, Train Acc: 0.7980 | \nEpoch [18/200] | Train Loss: 0.4518, Train Acc: 0.7912 | \nEpoch [19/200] | Train Loss: 0.4126, Train Acc: 0.8114 | \nEpoch [20/200] | Train Loss: 0.3873, Train Acc: 0.8418 | \nEpoch [21/200] | Train Loss: 0.3624, Train Acc: 0.8620 | \nEpoch [22/200] | Train Loss: 0.3465, Train Acc: 0.8384 | \nEpoch [23/200] | Train Loss: 0.3807, Train Acc: 0.8182 | \nEpoch [24/200] | Train Loss: 0.3670, Train Acc: 0.8384 | \nEpoch [25/200] | Train Loss: 0.3637, Train Acc: 0.8384 | \nEpoch [26/200] | Train Loss: 0.3266, Train Acc: 0.8485 | \nEpoch [27/200] | Train Loss: 0.3280, Train Acc: 0.8519 | \nEpoch [28/200] | Train Loss: 0.3088, Train Acc: 0.8721 | \nEpoch [29/200] | Train Loss: 0.3327, Train Acc: 0.8721 | \nEpoch [30/200] | Train Loss: 0.3018, Train Acc: 0.8519 | \nEpoch [31/200] | Train Loss: 0.2794, Train Acc: 0.8889 | \nEpoch [32/200] | Train Loss: 0.3674, Train Acc: 0.8384 | \nEpoch [33/200] | Train Loss: 0.3704, Train Acc: 0.8249 | \nEpoch [34/200] | Train Loss: 0.3406, Train Acc: 0.8754 | \nEpoch [35/200] | Train Loss: 0.4123, Train Acc: 0.8081 | \nEpoch [36/200] | Train Loss: 0.3469, Train Acc: 0.8519 | \nEpoch [37/200] | Train Loss: 0.3195, Train Acc: 0.8552 | \nEpoch [38/200] | Train Loss: 0.2730, Train Acc: 0.8822 | \nEpoch [39/200] | Train Loss: 0.2877, Train Acc: 0.8754 | \nEpoch [40/200] | Train Loss: 0.2794, Train Acc: 0.8653 | \nEpoch [41/200] | Train Loss: 0.2641, Train Acc: 0.8923 | \nEpoch [42/200] | Train Loss: 0.2834, Train Acc: 0.8788 | \nEpoch [43/200] | Train Loss: 0.3145, Train Acc: 0.8586 | \nEpoch [44/200] | Train Loss: 0.4036, Train Acc: 0.8283 | \nEpoch [45/200] | Train Loss: 0.3015, Train Acc: 0.8519 | \nEpoch [46/200] | Train Loss: 0.2999, Train Acc: 0.8721 | \nEpoch [47/200] | Train Loss: 0.2451, Train Acc: 0.8956 | \nEpoch [48/200] | Train Loss: 0.2452, Train Acc: 0.8923 | \nEpoch [49/200] | Train Loss: 0.3057, Train Acc: 0.8956 | \nEpoch [50/200] | Train Loss: 0.3247, Train Acc: 0.8451 | \nEpoch [51/200] | Train Loss: 0.2533, Train Acc: 0.8990 | \nEpoch [52/200] | Train Loss: 0.2401, Train Acc: 0.9125 | \nEpoch [53/200] | Train Loss: 0.2470, Train Acc: 0.8754 | \nEpoch [54/200] | Train Loss: 0.2232, Train Acc: 0.9024 | \nEpoch [55/200] | Train Loss: 0.3067, Train Acc: 0.8754 | \nEpoch [56/200] | Train Loss: 0.2374, Train Acc: 0.8956 | \nEpoch [57/200] | Train Loss: 0.1866, Train Acc: 0.9192 | \nEpoch [58/200] | Train Loss: 0.2400, Train Acc: 0.9057 | \nEpoch [59/200] | Train Loss: 0.2368, Train Acc: 0.8923 | \nEpoch [60/200] | Train Loss: 0.2464, Train Acc: 0.8855 | \nEpoch [61/200] | Train Loss: 0.2424, Train Acc: 0.8923 | \nEpoch [62/200] | Train Loss: 0.2138, Train Acc: 0.9057 | \nEpoch [63/200] | Train Loss: 0.2723, Train Acc: 0.8889 | \nEpoch [64/200] | Train Loss: 0.3642, Train Acc: 0.8519 | \nEpoch [65/200] | Train Loss: 0.2961, Train Acc: 0.8620 | \nEpoch [66/200] | Train Loss: 0.3031, Train Acc: 0.8653 | \nEpoch [67/200] | Train Loss: 0.2406, Train Acc: 0.8990 | \nEpoch 00068: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [68/200] | Train Loss: 0.2225, Train Acc: 0.9091 | \nEpoch [69/200] | Train Loss: 0.1937, Train Acc: 0.9259 | \nEpoch [70/200] | Train Loss: 0.1714, Train Acc: 0.9360 | \nEpoch [71/200] | Train Loss: 0.1551, Train Acc: 0.9495 | \nEpoch [72/200] | Train Loss: 0.1624, Train Acc: 0.9293 | \nEpoch [73/200] | Train Loss: 0.1408, Train Acc: 0.9562 | \nEpoch [74/200] | Train Loss: 0.1429, Train Acc: 0.9495 | \nEpoch [75/200] | Train Loss: 0.1310, Train Acc: 0.9461 | \nEpoch [76/200] | Train Loss: 0.1379, Train Acc: 0.9327 | \nEpoch [77/200] | Train Loss: 0.1325, Train Acc: 0.9327 | \nEpoch [78/200] | Train Loss: 0.1358, Train Acc: 0.9428 | \nEpoch [79/200] | Train Loss: 0.1322, Train Acc: 0.9428 | \nEpoch [80/200] | Train Loss: 0.1187, Train Acc: 0.9495 | \nEpoch [81/200] | Train Loss: 0.1227, Train Acc: 0.9428 | \nEpoch [82/200] | Train Loss: 0.1201, Train Acc: 0.9562 | \nEpoch [83/200] | Train Loss: 0.1070, Train Acc: 0.9562 | \nEpoch [84/200] | Train Loss: 0.1208, Train Acc: 0.9495 | \nEpoch [85/200] | Train Loss: 0.1159, Train Acc: 0.9495 | \nEpoch [86/200] | Train Loss: 0.1067, Train Acc: 0.9495 | \nEpoch [87/200] | Train Loss: 0.1043, Train Acc: 0.9495 | \nEpoch [88/200] | Train Loss: 0.1065, Train Acc: 0.9461 | \nEpoch [89/200] | Train Loss: 0.1044, Train Acc: 0.9495 | \nEpoch [90/200] | Train Loss: 0.1131, Train Acc: 0.9529 | \nEpoch [91/200] | Train Loss: 0.0974, Train Acc: 0.9697 | \nEpoch [92/200] | Train Loss: 0.1059, Train Acc: 0.9529 | \nEpoch [93/200] | Train Loss: 0.0960, Train Acc: 0.9562 | \nEpoch [94/200] | Train Loss: 0.0907, Train Acc: 0.9461 | \nEpoch [95/200] | Train Loss: 0.0870, Train Acc: 0.9630 | \nEpoch [96/200] | Train Loss: 0.0713, Train Acc: 0.9697 | \nEpoch [97/200] | Train Loss: 0.0786, Train Acc: 0.9663 | \nEpoch [98/200] | Train Loss: 0.0834, Train Acc: 0.9596 | \nEpoch [99/200] | Train Loss: 0.0788, Train Acc: 0.9596 | \nEpoch [100/200] | Train Loss: 0.0842, Train Acc: 0.9663 | \nEpoch [101/200] | Train Loss: 0.0755, Train Acc: 0.9663 | \nEpoch [102/200] | Train Loss: 0.0703, Train Acc: 0.9798 | \nEpoch [103/200] | Train Loss: 0.0541, Train Acc: 0.9865 | \nEpoch [104/200] | Train Loss: 0.0679, Train Acc: 0.9731 | \nEpoch [105/200] | Train Loss: 0.0564, Train Acc: 0.9798 | \nEpoch [106/200] | Train Loss: 0.0627, Train Acc: 0.9832 | \nEpoch [107/200] | Train Loss: 0.0518, Train Acc: 0.9832 | \nEpoch [108/200] | Train Loss: 0.0804, Train Acc: 0.9731 | \nEpoch [109/200] | Train Loss: 0.0736, Train Acc: 0.9731 | \nEpoch [110/200] | Train Loss: 0.0555, Train Acc: 0.9798 | \nEpoch [111/200] | Train Loss: 0.0706, Train Acc: 0.9697 | \nEpoch [112/200] | Train Loss: 0.0501, Train Acc: 0.9798 | \nEpoch [113/200] | Train Loss: 0.0439, Train Acc: 0.9865 | \nEpoch [114/200] | Train Loss: 0.0380, Train Acc: 0.9832 | \nEpoch [115/200] | Train Loss: 0.0565, Train Acc: 0.9832 | \nEpoch [116/200] | Train Loss: 0.0545, Train Acc: 0.9798 | \nEpoch [117/200] | Train Loss: 0.0554, Train Acc: 0.9798 | \nEpoch [118/200] | Train Loss: 0.0619, Train Acc: 0.9663 | \nEpoch [119/200] | Train Loss: 0.0384, Train Acc: 0.9933 | \nEpoch [120/200] | Train Loss: 0.0442, Train Acc: 0.9865 | \nEpoch [121/200] | Train Loss: 0.0622, Train Acc: 0.9731 | \nEpoch [122/200] | Train Loss: 0.0439, Train Acc: 0.9865 | \nEpoch [123/200] | Train Loss: 0.0656, Train Acc: 0.9663 | \nEpoch [124/200] | Train Loss: 0.0642, Train Acc: 0.9731 | \nEpoch 00125: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [125/200] | Train Loss: 0.0581, Train Acc: 0.9798 | \nEpoch [126/200] | Train Loss: 0.0452, Train Acc: 0.9798 | \nEpoch [127/200] | Train Loss: 0.0453, Train Acc: 0.9832 | \nEpoch [128/200] | Train Loss: 0.0295, Train Acc: 0.9899 | \nEpoch [129/200] | Train Loss: 0.0376, Train Acc: 0.9832 | \nEpoch [130/200] | Train Loss: 0.0303, Train Acc: 0.9899 | \nEpoch [131/200] | Train Loss: 0.0379, Train Acc: 0.9865 | \nEpoch [132/200] | Train Loss: 0.0384, Train Acc: 0.9966 | \nEpoch [133/200] | Train Loss: 0.0446, Train Acc: 0.9764 | \nEpoch [134/200] | Train Loss: 0.0489, Train Acc: 0.9832 | \nEpoch [135/200] | Train Loss: 0.0425, Train Acc: 0.9899 | \nEpoch [136/200] | Train Loss: 0.0383, Train Acc: 0.9933 | \nEpoch [137/200] | Train Loss: 0.0239, Train Acc: 0.9966 | \nEpoch [138/200] | Train Loss: 0.0350, Train Acc: 0.9899 | \nEpoch [139/200] | Train Loss: 0.0328, Train Acc: 0.9899 | \nEpoch [140/200] | Train Loss: 0.0320, Train Acc: 0.9933 | \nEpoch [141/200] | Train Loss: 0.0394, Train Acc: 0.9899 | \nEpoch [142/200] | Train Loss: 0.0299, Train Acc: 0.9933 | \nEpoch [143/200] | Train Loss: 0.0376, Train Acc: 0.9865 | \nEpoch [144/200] | Train Loss: 0.0417, Train Acc: 0.9798 | \nEpoch [145/200] | Train Loss: 0.0406, Train Acc: 0.9832 | \nEpoch [146/200] | Train Loss: 0.0397, Train Acc: 0.9865 | \nEpoch [147/200] | Train Loss: 0.0427, Train Acc: 0.9832 | \nEpoch 00148: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [148/200] | Train Loss: 0.0318, Train Acc: 0.9933 | \nEpoch [149/200] | Train Loss: 0.0332, Train Acc: 0.9865 | \nEpoch [150/200] | Train Loss: 0.0364, Train Acc: 0.9899 | \nEpoch [151/200] | Train Loss: 0.0285, Train Acc: 0.9899 | \nEarly stopping triggered.\nTest Loss: 1.3576, Test Accuracy: 0.7467, Test AUC: 0.7560\n\n--- Processing: psd_fc_theta ---\nShape: (372, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7324, Train Acc: 0.4781 | \nEpoch [2/200] | Train Loss: 0.7278, Train Acc: 0.4714 | \nEpoch [3/200] | Train Loss: 0.7009, Train Acc: 0.4781 | \nEpoch [4/200] | Train Loss: 0.6971, Train Acc: 0.5017 | \nEpoch [5/200] | Train Loss: 0.6899, Train Acc: 0.5421 | \nEpoch [6/200] | Train Loss: 0.6331, Train Acc: 0.7138 | \nEpoch [7/200] | Train Loss: 0.5194, Train Acc: 0.7576 | \nEpoch [8/200] | Train Loss: 0.5784, Train Acc: 0.7205 | \nEpoch [9/200] | Train Loss: 0.4922, Train Acc: 0.7845 | \nEpoch [10/200] | Train Loss: 0.4594, Train Acc: 0.8013 | \nEpoch [11/200] | Train Loss: 0.3752, Train Acc: 0.8485 | \nEpoch [12/200] | Train Loss: 0.3864, Train Acc: 0.8215 | \nEpoch [13/200] | Train Loss: 0.3335, Train Acc: 0.8552 | \nEpoch [14/200] | Train Loss: 0.3157, Train Acc: 0.8620 | \nEpoch [15/200] | Train Loss: 0.2539, Train Acc: 0.9158 | \nEpoch [16/200] | Train Loss: 0.3739, Train Acc: 0.8586 | \nEpoch [17/200] | Train Loss: 0.3534, Train Acc: 0.8485 | \nEpoch [18/200] | Train Loss: 0.2739, Train Acc: 0.8923 | \nEpoch [19/200] | Train Loss: 0.2933, Train Acc: 0.8990 | \nEpoch [20/200] | Train Loss: 0.2599, Train Acc: 0.8990 | \nEpoch [21/200] | Train Loss: 0.2815, Train Acc: 0.8923 | \nEpoch [22/200] | Train Loss: 0.2500, Train Acc: 0.9024 | \nEpoch [23/200] | Train Loss: 0.2678, Train Acc: 0.8956 | \nEpoch [24/200] | Train Loss: 0.2503, Train Acc: 0.9024 | \nEpoch [25/200] | Train Loss: 0.2151, Train Acc: 0.9259 | \nEpoch [26/200] | Train Loss: 0.2661, Train Acc: 0.8855 | \nEpoch [27/200] | Train Loss: 0.2815, Train Acc: 0.8956 | \nEpoch [28/200] | Train Loss: 0.2504, Train Acc: 0.8822 | \nEpoch [29/200] | Train Loss: 0.2064, Train Acc: 0.9226 | \nEpoch [30/200] | Train Loss: 0.2049, Train Acc: 0.9327 | \nEpoch [31/200] | Train Loss: 0.1570, Train Acc: 0.9596 | \nEpoch [32/200] | Train Loss: 0.1774, Train Acc: 0.9327 | \nEpoch [33/200] | Train Loss: 0.1956, Train Acc: 0.9091 | \nEpoch [34/200] | Train Loss: 0.1891, Train Acc: 0.9226 | \nEpoch [35/200] | Train Loss: 0.2794, Train Acc: 0.8855 | \nEpoch [36/200] | Train Loss: 0.2335, Train Acc: 0.9327 | \nEpoch [37/200] | Train Loss: 0.2291, Train Acc: 0.9192 | \nEpoch [38/200] | Train Loss: 0.1782, Train Acc: 0.9394 | \nEpoch [39/200] | Train Loss: 0.1295, Train Acc: 0.9562 | \nEpoch [40/200] | Train Loss: 0.1278, Train Acc: 0.9529 | \nEpoch [41/200] | Train Loss: 0.4728, Train Acc: 0.8114 | \nEpoch [42/200] | Train Loss: 0.3970, Train Acc: 0.8182 | \nEpoch [43/200] | Train Loss: 0.3865, Train Acc: 0.8316 | \nEpoch [44/200] | Train Loss: 0.3523, Train Acc: 0.8485 | \nEpoch [45/200] | Train Loss: 0.3286, Train Acc: 0.8721 | \nEpoch [46/200] | Train Loss: 0.3378, Train Acc: 0.8485 | \nEpoch [47/200] | Train Loss: 0.2537, Train Acc: 0.9125 | \nEpoch [48/200] | Train Loss: 0.2147, Train Acc: 0.9125 | \nEpoch [49/200] | Train Loss: 0.2144, Train Acc: 0.9226 | \nEpoch [50/200] | Train Loss: 0.2289, Train Acc: 0.9125 | \nEarly stopping triggered.\nTest Loss: 0.5234, Test Accuracy: 0.8267, Test AUC: 0.8883\n\n--- Processing: psd_fc_alpha ---\nShape: (372, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7282, Train Acc: 0.5354 | \nEpoch [2/200] | Train Loss: 0.7093, Train Acc: 0.4983 | \nEpoch [3/200] | Train Loss: 0.6964, Train Acc: 0.5387 | \nEpoch [4/200] | Train Loss: 0.6579, Train Acc: 0.6094 | \nEpoch [5/200] | Train Loss: 0.6055, Train Acc: 0.6869 | \nEpoch [6/200] | Train Loss: 0.5956, Train Acc: 0.6970 | \nEpoch [7/200] | Train Loss: 0.5297, Train Acc: 0.7340 | \nEpoch [8/200] | Train Loss: 0.5017, Train Acc: 0.7710 | \nEpoch [9/200] | Train Loss: 0.4325, Train Acc: 0.8114 | \nEpoch [10/200] | Train Loss: 0.4776, Train Acc: 0.7980 | \nEpoch [11/200] | Train Loss: 0.4202, Train Acc: 0.8114 | \nEpoch [12/200] | Train Loss: 0.4090, Train Acc: 0.8081 | \nEpoch [13/200] | Train Loss: 0.3747, Train Acc: 0.8418 | \nEpoch [14/200] | Train Loss: 0.4427, Train Acc: 0.7912 | \nEpoch [15/200] | Train Loss: 0.3836, Train Acc: 0.8283 | \nEpoch [16/200] | Train Loss: 0.4081, Train Acc: 0.8215 | \nEpoch [17/200] | Train Loss: 0.3777, Train Acc: 0.8148 | \nEpoch [18/200] | Train Loss: 0.3200, Train Acc: 0.8620 | \nEpoch [19/200] | Train Loss: 0.3094, Train Acc: 0.8788 | \nEpoch [20/200] | Train Loss: 0.3423, Train Acc: 0.8620 | \nEpoch [21/200] | Train Loss: 0.3059, Train Acc: 0.8822 | \nEpoch [22/200] | Train Loss: 0.3608, Train Acc: 0.8418 | \nEpoch [23/200] | Train Loss: 0.2748, Train Acc: 0.8822 | \nEpoch [24/200] | Train Loss: 0.2549, Train Acc: 0.8956 | \nEpoch [25/200] | Train Loss: 0.2866, Train Acc: 0.8687 | \nEpoch [26/200] | Train Loss: 0.2516, Train Acc: 0.8889 | \nEpoch [27/200] | Train Loss: 0.2043, Train Acc: 0.9226 | \nEpoch [28/200] | Train Loss: 0.2295, Train Acc: 0.9057 | \nEpoch [29/200] | Train Loss: 0.2643, Train Acc: 0.9024 | \nEpoch [30/200] | Train Loss: 0.2634, Train Acc: 0.9057 | \nEpoch [31/200] | Train Loss: 0.2977, Train Acc: 0.8822 | \nEpoch [32/200] | Train Loss: 0.2050, Train Acc: 0.9226 | \nEpoch [33/200] | Train Loss: 0.1775, Train Acc: 0.9226 | \nEpoch [34/200] | Train Loss: 0.2010, Train Acc: 0.9259 | \nEpoch [35/200] | Train Loss: 0.2466, Train Acc: 0.8990 | \nEpoch [36/200] | Train Loss: 0.2340, Train Acc: 0.8956 | \nEpoch [37/200] | Train Loss: 0.2115, Train Acc: 0.9158 | \nEpoch [38/200] | Train Loss: 0.3432, Train Acc: 0.8418 | \nEpoch [39/200] | Train Loss: 0.3208, Train Acc: 0.8620 | \nEpoch [40/200] | Train Loss: 0.2814, Train Acc: 0.8956 | \nEpoch [41/200] | Train Loss: 0.2106, Train Acc: 0.9226 | \nEpoch [42/200] | Train Loss: 0.1715, Train Acc: 0.9394 | \nEpoch [43/200] | Train Loss: 0.1800, Train Acc: 0.9327 | \nEpoch [44/200] | Train Loss: 0.1205, Train Acc: 0.9562 | \nEpoch [45/200] | Train Loss: 0.1213, Train Acc: 0.9495 | \nEpoch [46/200] | Train Loss: 0.1041, Train Acc: 0.9562 | \nEpoch [47/200] | Train Loss: 0.2242, Train Acc: 0.8889 | \nEpoch [48/200] | Train Loss: 0.3389, Train Acc: 0.8721 | \nEpoch [49/200] | Train Loss: 0.2302, Train Acc: 0.9091 | \nEpoch [50/200] | Train Loss: 0.1810, Train Acc: 0.9327 | \nEpoch [51/200] | Train Loss: 0.1408, Train Acc: 0.9428 | \nEpoch [52/200] | Train Loss: 0.1398, Train Acc: 0.9293 | \nEpoch [53/200] | Train Loss: 0.1176, Train Acc: 0.9529 | \nEpoch [54/200] | Train Loss: 0.1047, Train Acc: 0.9663 | \nEpoch [55/200] | Train Loss: 0.0922, Train Acc: 0.9630 | \nEpoch [56/200] | Train Loss: 0.0889, Train Acc: 0.9562 | \nEpoch [57/200] | Train Loss: 0.1784, Train Acc: 0.9293 | \nEpoch [58/200] | Train Loss: 0.2453, Train Acc: 0.9091 | \nEpoch [59/200] | Train Loss: 0.2306, Train Acc: 0.9125 | \nEpoch [60/200] | Train Loss: 0.1980, Train Acc: 0.9125 | \nEpoch [61/200] | Train Loss: 0.1549, Train Acc: 0.9226 | \nEpoch [62/200] | Train Loss: 0.1511, Train Acc: 0.9461 | \nEpoch [63/200] | Train Loss: 0.1424, Train Acc: 0.9461 | \nEpoch [64/200] | Train Loss: 0.1344, Train Acc: 0.9461 | \nEpoch [65/200] | Train Loss: 0.1179, Train Acc: 0.9562 | \nEpoch [66/200] | Train Loss: 0.0783, Train Acc: 0.9697 | \nEpoch [67/200] | Train Loss: 0.1636, Train Acc: 0.9259 | \nEpoch [68/200] | Train Loss: 0.1521, Train Acc: 0.9360 | \nEpoch [69/200] | Train Loss: 0.1421, Train Acc: 0.9394 | \nEpoch [70/200] | Train Loss: 0.1032, Train Acc: 0.9529 | \nEpoch [71/200] | Train Loss: 0.1021, Train Acc: 0.9562 | \nEpoch [72/200] | Train Loss: 0.0933, Train Acc: 0.9663 | \nEpoch [73/200] | Train Loss: 0.2019, Train Acc: 0.9293 | \nEpoch [74/200] | Train Loss: 0.1984, Train Acc: 0.9360 | \nEpoch [75/200] | Train Loss: 0.1595, Train Acc: 0.9428 | \nEpoch [76/200] | Train Loss: 0.1321, Train Acc: 0.9461 | \nEpoch 00077: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [77/200] | Train Loss: 0.0835, Train Acc: 0.9764 | \nEpoch [78/200] | Train Loss: 0.0719, Train Acc: 0.9798 | \nEpoch [79/200] | Train Loss: 0.0815, Train Acc: 0.9731 | \nEpoch [80/200] | Train Loss: 0.0783, Train Acc: 0.9697 | \nEpoch [81/200] | Train Loss: 0.0770, Train Acc: 0.9764 | \nEpoch [82/200] | Train Loss: 0.0574, Train Acc: 0.9832 | \nEpoch [83/200] | Train Loss: 0.0496, Train Acc: 0.9865 | \nEpoch [84/200] | Train Loss: 0.0536, Train Acc: 0.9832 | \nEpoch [85/200] | Train Loss: 0.0390, Train Acc: 0.9899 | \nEpoch [86/200] | Train Loss: 0.0452, Train Acc: 0.9865 | \nEpoch [87/200] | Train Loss: 0.0474, Train Acc: 0.9865 | \nEpoch [88/200] | Train Loss: 0.0531, Train Acc: 0.9764 | \nEpoch [89/200] | Train Loss: 0.0343, Train Acc: 0.9899 | \nEpoch [90/200] | Train Loss: 0.0379, Train Acc: 0.9798 | \nEpoch [91/200] | Train Loss: 0.0348, Train Acc: 0.9865 | \nEpoch [92/200] | Train Loss: 0.0392, Train Acc: 0.9832 | \nEpoch [93/200] | Train Loss: 0.0374, Train Acc: 0.9899 | \nEpoch [94/200] | Train Loss: 0.0344, Train Acc: 0.9865 | \nEpoch [95/200] | Train Loss: 0.0383, Train Acc: 0.9865 | \nEpoch [96/200] | Train Loss: 0.0291, Train Acc: 0.9933 | \nEpoch [97/200] | Train Loss: 0.0291, Train Acc: 0.9933 | \nEpoch [98/200] | Train Loss: 0.0297, Train Acc: 0.9899 | \nEpoch [99/200] | Train Loss: 0.0415, Train Acc: 0.9899 | \nEpoch [100/200] | Train Loss: 0.0311, Train Acc: 0.9933 | \nEpoch [101/200] | Train Loss: 0.0280, Train Acc: 0.9899 | \nEpoch [102/200] | Train Loss: 0.0273, Train Acc: 0.9933 | \nEpoch [103/200] | Train Loss: 0.0231, Train Acc: 0.9899 | \nEpoch [104/200] | Train Loss: 0.0261, Train Acc: 0.9933 | \nEpoch [105/200] | Train Loss: 0.0273, Train Acc: 0.9899 | \nEpoch [106/200] | Train Loss: 0.0228, Train Acc: 0.9933 | \nEpoch [107/200] | Train Loss: 0.0354, Train Acc: 0.9899 | \nEpoch [108/200] | Train Loss: 0.0264, Train Acc: 0.9899 | \nEpoch [109/200] | Train Loss: 0.0169, Train Acc: 0.9933 | \nEpoch [110/200] | Train Loss: 0.0244, Train Acc: 0.9899 | \nEpoch [111/200] | Train Loss: 0.0254, Train Acc: 0.9933 | \nEpoch [112/200] | Train Loss: 0.0185, Train Acc: 0.9933 | \nEpoch [113/200] | Train Loss: 0.0231, Train Acc: 0.9933 | \nEpoch [114/200] | Train Loss: 0.0186, Train Acc: 0.9933 | \nEpoch [115/200] | Train Loss: 0.0153, Train Acc: 0.9966 | \nEpoch [116/200] | Train Loss: 0.0197, Train Acc: 0.9899 | \nEpoch [117/200] | Train Loss: 0.0199, Train Acc: 0.9933 | \nEpoch [118/200] | Train Loss: 0.0213, Train Acc: 0.9933 | \nEpoch [119/200] | Train Loss: 0.0179, Train Acc: 0.9933 | \nEpoch [120/200] | Train Loss: 0.0257, Train Acc: 0.9899 | \nEpoch [121/200] | Train Loss: 0.0177, Train Acc: 0.9933 | \nEpoch [122/200] | Train Loss: 0.0175, Train Acc: 0.9933 | \nEpoch [123/200] | Train Loss: 0.0206, Train Acc: 0.9899 | \nEpoch [124/200] | Train Loss: 0.0125, Train Acc: 0.9933 | \nEpoch [125/200] | Train Loss: 0.0113, Train Acc: 0.9933 | \nEpoch [126/200] | Train Loss: 0.0141, Train Acc: 0.9966 | \nEpoch [127/200] | Train Loss: 0.0118, Train Acc: 0.9933 | \nEpoch [128/200] | Train Loss: 0.0178, Train Acc: 0.9966 | \nEpoch [129/200] | Train Loss: 0.0103, Train Acc: 1.0000 | \nEpoch [130/200] | Train Loss: 0.0134, Train Acc: 0.9966 | \nEpoch [131/200] | Train Loss: 0.0090, Train Acc: 1.0000 | \nEpoch [132/200] | Train Loss: 0.0092, Train Acc: 0.9966 | \nEpoch [133/200] | Train Loss: 0.0244, Train Acc: 0.9933 | \nEpoch [134/200] | Train Loss: 0.0098, Train Acc: 0.9966 | \nEpoch [135/200] | Train Loss: 0.0136, Train Acc: 0.9899 | \nEpoch [136/200] | Train Loss: 0.0127, Train Acc: 0.9966 | \nEpoch [137/200] | Train Loss: 0.0097, Train Acc: 0.9966 | \nEpoch [138/200] | Train Loss: 0.0210, Train Acc: 0.9933 | \nEpoch [139/200] | Train Loss: 0.0164, Train Acc: 0.9966 | \nEpoch [140/200] | Train Loss: 0.0212, Train Acc: 0.9933 | \nEpoch [141/200] | Train Loss: 0.0133, Train Acc: 0.9966 | \nEpoch 00142: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [142/200] | Train Loss: 0.0186, Train Acc: 0.9933 | \nEpoch [143/200] | Train Loss: 0.0270, Train Acc: 0.9899 | \nEpoch [144/200] | Train Loss: 0.0188, Train Acc: 0.9966 | \nEpoch [145/200] | Train Loss: 0.0226, Train Acc: 0.9899 | \nEpoch [146/200] | Train Loss: 0.0132, Train Acc: 0.9966 | \nEpoch [147/200] | Train Loss: 0.0200, Train Acc: 0.9899 | \nEpoch [148/200] | Train Loss: 0.0116, Train Acc: 0.9966 | \nEarly stopping triggered.\nTest Loss: 0.7023, Test Accuracy: 0.8800, Test AUC: 0.9374\n\n--- Processing: psd_fc_beta ---\nShape: (372, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7225, Train Acc: 0.5623 | \nEpoch [2/200] | Train Loss: 0.7116, Train Acc: 0.4983 | \nEpoch [3/200] | Train Loss: 0.7182, Train Acc: 0.5084 | \nEpoch [4/200] | Train Loss: 0.6908, Train Acc: 0.5185 | \nEpoch [5/200] | Train Loss: 0.6655, Train Acc: 0.5522 | \nEpoch [6/200] | Train Loss: 0.5821, Train Acc: 0.6936 | \nEpoch [7/200] | Train Loss: 0.5599, Train Acc: 0.7104 | \nEpoch [8/200] | Train Loss: 0.4523, Train Acc: 0.7879 | \nEpoch [9/200] | Train Loss: 0.4805, Train Acc: 0.7811 | \nEpoch [10/200] | Train Loss: 0.5119, Train Acc: 0.7407 | \nEpoch [11/200] | Train Loss: 0.4633, Train Acc: 0.7912 | \nEpoch [12/200] | Train Loss: 0.4195, Train Acc: 0.8182 | \nEpoch [13/200] | Train Loss: 0.4498, Train Acc: 0.7677 | \nEpoch [14/200] | Train Loss: 0.4004, Train Acc: 0.8283 | \nEpoch [15/200] | Train Loss: 0.3551, Train Acc: 0.8451 | \nEpoch [16/200] | Train Loss: 0.3083, Train Acc: 0.8653 | \nEpoch [17/200] | Train Loss: 0.3711, Train Acc: 0.8350 | \nEpoch [18/200] | Train Loss: 0.3514, Train Acc: 0.8350 | \nEpoch [19/200] | Train Loss: 0.2866, Train Acc: 0.8822 | \nEpoch [20/200] | Train Loss: 0.4414, Train Acc: 0.8350 | \nEpoch [21/200] | Train Loss: 0.3431, Train Acc: 0.8552 | \nEpoch [22/200] | Train Loss: 0.3033, Train Acc: 0.8788 | \nEpoch [23/200] | Train Loss: 0.2939, Train Acc: 0.8687 | \nEpoch [24/200] | Train Loss: 0.3048, Train Acc: 0.8889 | \nEpoch [25/200] | Train Loss: 0.2398, Train Acc: 0.8956 | \nEpoch [26/200] | Train Loss: 0.2371, Train Acc: 0.9091 | \nEpoch [27/200] | Train Loss: 0.2697, Train Acc: 0.8990 | \nEpoch [28/200] | Train Loss: 0.2023, Train Acc: 0.9293 | \nEpoch [29/200] | Train Loss: 0.1818, Train Acc: 0.9495 | \nEpoch [30/200] | Train Loss: 0.1794, Train Acc: 0.9293 | \nEpoch [31/200] | Train Loss: 0.2877, Train Acc: 0.8990 | \nEpoch [32/200] | Train Loss: 0.2715, Train Acc: 0.8822 | \nEpoch [33/200] | Train Loss: 0.2381, Train Acc: 0.9158 | \nEpoch [34/200] | Train Loss: 0.2546, Train Acc: 0.9024 | \nEpoch [35/200] | Train Loss: 0.2941, Train Acc: 0.8889 | \nEpoch [36/200] | Train Loss: 0.2159, Train Acc: 0.9226 | \nEpoch [37/200] | Train Loss: 0.2363, Train Acc: 0.8889 | \nEpoch [38/200] | Train Loss: 0.1377, Train Acc: 0.9495 | \nEpoch [39/200] | Train Loss: 0.1377, Train Acc: 0.9461 | \nEpoch [40/200] | Train Loss: 0.2173, Train Acc: 0.9226 | \nEpoch [41/200] | Train Loss: 0.1498, Train Acc: 0.9461 | \nEpoch [42/200] | Train Loss: 0.1323, Train Acc: 0.9495 | \nEpoch [43/200] | Train Loss: 0.1312, Train Acc: 0.9529 | \nEpoch [44/200] | Train Loss: 0.1201, Train Acc: 0.9596 | \nEpoch [45/200] | Train Loss: 0.1509, Train Acc: 0.9461 | \nEpoch [46/200] | Train Loss: 0.1376, Train Acc: 0.9630 | \nEpoch [47/200] | Train Loss: 0.1715, Train Acc: 0.9428 | \nEpoch [48/200] | Train Loss: 0.1946, Train Acc: 0.9360 | \nEpoch [49/200] | Train Loss: 0.1651, Train Acc: 0.9360 | \nEpoch [50/200] | Train Loss: 0.1473, Train Acc: 0.9428 | \nEpoch [51/200] | Train Loss: 0.1121, Train Acc: 0.9596 | \nEpoch [52/200] | Train Loss: 0.1019, Train Acc: 0.9596 | \nEpoch [53/200] | Train Loss: 0.1046, Train Acc: 0.9596 | \nEpoch [54/200] | Train Loss: 0.1304, Train Acc: 0.9562 | \nEpoch [55/200] | Train Loss: 0.0764, Train Acc: 0.9731 | \nEpoch [56/200] | Train Loss: 0.1043, Train Acc: 0.9697 | \nEpoch [57/200] | Train Loss: 0.2262, Train Acc: 0.9259 | \nEpoch [58/200] | Train Loss: 0.2966, Train Acc: 0.8788 | \nEpoch [59/200] | Train Loss: 0.2695, Train Acc: 0.9057 | \nEpoch [60/200] | Train Loss: 0.1580, Train Acc: 0.9495 | \nEpoch [61/200] | Train Loss: 0.2431, Train Acc: 0.9226 | \nEpoch [62/200] | Train Loss: 0.2174, Train Acc: 0.9091 | \nEpoch [63/200] | Train Loss: 0.1704, Train Acc: 0.9529 | \nEpoch [64/200] | Train Loss: 0.1309, Train Acc: 0.9663 | \nEpoch [65/200] | Train Loss: 0.0922, Train Acc: 0.9798 | \nEpoch 00066: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [66/200] | Train Loss: 0.0926, Train Acc: 0.9764 | \nEpoch [67/200] | Train Loss: 0.0798, Train Acc: 0.9764 | \nEpoch [68/200] | Train Loss: 0.0604, Train Acc: 0.9798 | \nEpoch [69/200] | Train Loss: 0.0543, Train Acc: 0.9865 | \nEpoch [70/200] | Train Loss: 0.0584, Train Acc: 0.9832 | \nEpoch [71/200] | Train Loss: 0.0486, Train Acc: 0.9899 | \nEpoch [72/200] | Train Loss: 0.0524, Train Acc: 0.9865 | \nEpoch [73/200] | Train Loss: 0.0572, Train Acc: 0.9832 | \nEpoch [74/200] | Train Loss: 0.0563, Train Acc: 0.9865 | \nEpoch [75/200] | Train Loss: 0.0545, Train Acc: 0.9865 | \nEpoch [76/200] | Train Loss: 0.0545, Train Acc: 0.9865 | \nEpoch [77/200] | Train Loss: 0.0506, Train Acc: 0.9899 | \nEpoch [78/200] | Train Loss: 0.0491, Train Acc: 0.9899 | \nEpoch [79/200] | Train Loss: 0.0642, Train Acc: 0.9865 | \nEpoch [80/200] | Train Loss: 0.0584, Train Acc: 0.9865 | \nEpoch [81/200] | Train Loss: 0.0543, Train Acc: 0.9899 | \nEpoch 00082: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [82/200] | Train Loss: 0.0490, Train Acc: 0.9899 | \nEpoch [83/200] | Train Loss: 0.0486, Train Acc: 0.9865 | \nEpoch [84/200] | Train Loss: 0.0488, Train Acc: 0.9899 | \nEpoch [85/200] | Train Loss: 0.0577, Train Acc: 0.9899 | \nEpoch [86/200] | Train Loss: 0.0474, Train Acc: 0.9899 | \nEpoch [87/200] | Train Loss: 0.0480, Train Acc: 0.9865 | \nEpoch [88/200] | Train Loss: 0.0494, Train Acc: 0.9865 | \nEpoch [89/200] | Train Loss: 0.0454, Train Acc: 0.9933 | \nEpoch [90/200] | Train Loss: 0.0502, Train Acc: 0.9899 | \nEpoch [91/200] | Train Loss: 0.0638, Train Acc: 0.9832 | \nEpoch [92/200] | Train Loss: 0.0537, Train Acc: 0.9899 | \nEpoch [93/200] | Train Loss: 0.0528, Train Acc: 0.9865 | \nEpoch [94/200] | Train Loss: 0.0450, Train Acc: 0.9899 | \nEpoch [95/200] | Train Loss: 0.0460, Train Acc: 0.9899 | \nEpoch [96/200] | Train Loss: 0.0471, Train Acc: 0.9899 | \nEpoch [97/200] | Train Loss: 0.0497, Train Acc: 0.9899 | \nEpoch [98/200] | Train Loss: 0.0473, Train Acc: 0.9899 | \nEpoch [99/200] | Train Loss: 0.0557, Train Acc: 0.9899 | \nEpoch [100/200] | Train Loss: 0.0545, Train Acc: 0.9865 | \nEpoch [101/200] | Train Loss: 0.0543, Train Acc: 0.9865 | \nEpoch [102/200] | Train Loss: 0.0449, Train Acc: 0.9933 | \nEpoch [103/200] | Train Loss: 0.0449, Train Acc: 0.9933 | \nEpoch [104/200] | Train Loss: 0.0531, Train Acc: 0.9899 | \nEpoch [105/200] | Train Loss: 0.0485, Train Acc: 0.9899 | \nEpoch [106/200] | Train Loss: 0.0443, Train Acc: 0.9933 | \nEpoch [107/200] | Train Loss: 0.0516, Train Acc: 0.9899 | \nEpoch [108/200] | Train Loss: 0.0586, Train Acc: 0.9798 | \nEarly stopping triggered.\nTest Loss: 0.8683, Test Accuracy: 0.7867, Test AUC: 0.8065\n\n--- Processing: psd_fc_highbeta ---\nShape: (372, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7550, Train Acc: 0.4781 | \nEpoch [2/200] | Train Loss: 0.7004, Train Acc: 0.5017 | \nEpoch [3/200] | Train Loss: 0.7058, Train Acc: 0.5051 | \nEpoch [4/200] | Train Loss: 0.7022, Train Acc: 0.4916 | \nEpoch [5/200] | Train Loss: 0.6922, Train Acc: 0.5185 | \nEpoch [6/200] | Train Loss: 0.6783, Train Acc: 0.5791 | \nEpoch [7/200] | Train Loss: 0.5927, Train Acc: 0.7104 | \nEpoch [8/200] | Train Loss: 0.5507, Train Acc: 0.7003 | \nEpoch [9/200] | Train Loss: 0.5938, Train Acc: 0.6768 | \nEpoch [10/200] | Train Loss: 0.5420, Train Acc: 0.7273 | \nEpoch [11/200] | Train Loss: 0.5196, Train Acc: 0.7340 | \nEpoch [12/200] | Train Loss: 0.4813, Train Acc: 0.7609 | \nEpoch [13/200] | Train Loss: 0.4001, Train Acc: 0.8316 | \nEpoch [14/200] | Train Loss: 0.4132, Train Acc: 0.8215 | \nEpoch [15/200] | Train Loss: 0.3697, Train Acc: 0.8283 | \nEpoch [16/200] | Train Loss: 0.4639, Train Acc: 0.7677 | \nEpoch [17/200] | Train Loss: 0.4762, Train Acc: 0.7912 | \nEpoch [18/200] | Train Loss: 0.3967, Train Acc: 0.8047 | \nEpoch [19/200] | Train Loss: 0.3889, Train Acc: 0.8316 | \nEpoch [20/200] | Train Loss: 0.3691, Train Acc: 0.8384 | \nEpoch [21/200] | Train Loss: 0.3518, Train Acc: 0.8451 | \nEpoch [22/200] | Train Loss: 0.3286, Train Acc: 0.8620 | \nEpoch [23/200] | Train Loss: 0.3359, Train Acc: 0.8552 | \nEpoch [24/200] | Train Loss: 0.3427, Train Acc: 0.8485 | \nEpoch [25/200] | Train Loss: 0.3331, Train Acc: 0.8586 | \nEpoch [26/200] | Train Loss: 0.3397, Train Acc: 0.8316 | \nEpoch [27/200] | Train Loss: 0.2933, Train Acc: 0.8788 | \nEpoch [28/200] | Train Loss: 0.2891, Train Acc: 0.8788 | \nEpoch [29/200] | Train Loss: 0.3138, Train Acc: 0.8721 | \nEpoch [30/200] | Train Loss: 0.2960, Train Acc: 0.8788 | \nEpoch [31/200] | Train Loss: 0.3557, Train Acc: 0.8384 | \nEpoch [32/200] | Train Loss: 0.2775, Train Acc: 0.8822 | \nEpoch [33/200] | Train Loss: 0.3050, Train Acc: 0.8687 | \nEpoch [34/200] | Train Loss: 0.3687, Train Acc: 0.8215 | \nEpoch [35/200] | Train Loss: 0.2832, Train Acc: 0.9057 | \nEpoch [36/200] | Train Loss: 0.2334, Train Acc: 0.9091 | \nEpoch [37/200] | Train Loss: 0.2672, Train Acc: 0.9024 | \nEpoch [38/200] | Train Loss: 0.3423, Train Acc: 0.8653 | \nEpoch [39/200] | Train Loss: 0.2314, Train Acc: 0.9057 | \nEpoch [40/200] | Train Loss: 0.2345, Train Acc: 0.9125 | \nEpoch [41/200] | Train Loss: 0.2384, Train Acc: 0.8923 | \nEpoch [42/200] | Train Loss: 0.1736, Train Acc: 0.9562 | \nEpoch [43/200] | Train Loss: 0.2190, Train Acc: 0.9360 | \nEpoch [44/200] | Train Loss: 0.1695, Train Acc: 0.9293 | \nEpoch [45/200] | Train Loss: 0.1953, Train Acc: 0.9259 | \nEpoch [46/200] | Train Loss: 0.1802, Train Acc: 0.9293 | \nEpoch [47/200] | Train Loss: 0.1438, Train Acc: 0.9529 | \nEpoch [48/200] | Train Loss: 0.1434, Train Acc: 0.9562 | \nEpoch [49/200] | Train Loss: 0.2389, Train Acc: 0.9091 | \nEpoch [50/200] | Train Loss: 0.2237, Train Acc: 0.9158 | \nEpoch [51/200] | Train Loss: 0.1707, Train Acc: 0.9360 | \nEpoch [52/200] | Train Loss: 0.1570, Train Acc: 0.9428 | \nEpoch [53/200] | Train Loss: 0.2028, Train Acc: 0.9158 | \nEpoch [54/200] | Train Loss: 0.1862, Train Acc: 0.9226 | \nEpoch [55/200] | Train Loss: 0.1748, Train Acc: 0.9125 | \nEpoch [56/200] | Train Loss: 0.1406, Train Acc: 0.9461 | \nEpoch [57/200] | Train Loss: 0.1042, Train Acc: 0.9596 | \nEpoch [58/200] | Train Loss: 0.1044, Train Acc: 0.9731 | \nEpoch [59/200] | Train Loss: 0.1213, Train Acc: 0.9562 | \nEpoch [60/200] | Train Loss: 0.1374, Train Acc: 0.9461 | \nEpoch [61/200] | Train Loss: 0.1001, Train Acc: 0.9596 | \nEpoch [62/200] | Train Loss: 0.1668, Train Acc: 0.9192 | \nEpoch [63/200] | Train Loss: 0.2111, Train Acc: 0.9125 | \nEpoch [64/200] | Train Loss: 0.2103, Train Acc: 0.9125 | \nEpoch [65/200] | Train Loss: 0.3382, Train Acc: 0.8822 | \nEpoch [66/200] | Train Loss: 0.2413, Train Acc: 0.9158 | \nEpoch [67/200] | Train Loss: 0.1960, Train Acc: 0.9192 | \nEpoch [68/200] | Train Loss: 0.1619, Train Acc: 0.9461 | \nEpoch [69/200] | Train Loss: 0.1287, Train Acc: 0.9461 | \nEpoch [70/200] | Train Loss: 0.1605, Train Acc: 0.9495 | \nEpoch [71/200] | Train Loss: 0.1835, Train Acc: 0.9192 | \nEpoch 00072: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [72/200] | Train Loss: 0.2158, Train Acc: 0.8923 | \nEpoch [73/200] | Train Loss: 0.1401, Train Acc: 0.9562 | \nEpoch [74/200] | Train Loss: 0.1239, Train Acc: 0.9630 | \nEpoch [75/200] | Train Loss: 0.1263, Train Acc: 0.9596 | \nEpoch [76/200] | Train Loss: 0.1036, Train Acc: 0.9697 | \nEpoch [77/200] | Train Loss: 0.0909, Train Acc: 0.9798 | \nEpoch [78/200] | Train Loss: 0.0962, Train Acc: 0.9697 | \nEpoch [79/200] | Train Loss: 0.0827, Train Acc: 0.9798 | \nEpoch [80/200] | Train Loss: 0.0764, Train Acc: 0.9764 | \nEpoch [81/200] | Train Loss: 0.0788, Train Acc: 0.9764 | \nEpoch [82/200] | Train Loss: 0.0766, Train Acc: 0.9764 | \nEpoch [83/200] | Train Loss: 0.0705, Train Acc: 0.9764 | \nEpoch [84/200] | Train Loss: 0.0543, Train Acc: 0.9865 | \nEpoch [85/200] | Train Loss: 0.0551, Train Acc: 0.9865 | \nEpoch [86/200] | Train Loss: 0.0531, Train Acc: 0.9832 | \nEpoch [87/200] | Train Loss: 0.0500, Train Acc: 0.9899 | \nEpoch [88/200] | Train Loss: 0.0493, Train Acc: 0.9865 | \nEpoch [89/200] | Train Loss: 0.0484, Train Acc: 0.9865 | \nEpoch [90/200] | Train Loss: 0.0563, Train Acc: 0.9731 | \nEpoch [91/200] | Train Loss: 0.0431, Train Acc: 0.9865 | \nEpoch [92/200] | Train Loss: 0.0377, Train Acc: 0.9865 | \nEpoch [93/200] | Train Loss: 0.0353, Train Acc: 0.9865 | \nEpoch [94/200] | Train Loss: 0.0440, Train Acc: 0.9865 | \nEpoch [95/200] | Train Loss: 0.0344, Train Acc: 0.9865 | \nEpoch [96/200] | Train Loss: 0.0368, Train Acc: 0.9899 | \nEpoch [97/200] | Train Loss: 0.0334, Train Acc: 0.9899 | \nEpoch [98/200] | Train Loss: 0.0468, Train Acc: 0.9865 | \nEpoch [99/200] | Train Loss: 0.0547, Train Acc: 0.9798 | \nEpoch [100/200] | Train Loss: 0.0408, Train Acc: 0.9865 | \nEpoch [101/200] | Train Loss: 0.0343, Train Acc: 0.9933 | \nEpoch [102/200] | Train Loss: 0.0399, Train Acc: 0.9899 | \nEpoch [103/200] | Train Loss: 0.0312, Train Acc: 0.9899 | \nEpoch [104/200] | Train Loss: 0.0302, Train Acc: 0.9865 | \nEpoch [105/200] | Train Loss: 0.0367, Train Acc: 0.9832 | \nEpoch [106/200] | Train Loss: 0.0241, Train Acc: 0.9865 | \nEpoch [107/200] | Train Loss: 0.0239, Train Acc: 0.9899 | \nEpoch [108/200] | Train Loss: 0.0254, Train Acc: 0.9899 | \nEpoch [109/200] | Train Loss: 0.0311, Train Acc: 0.9865 | \nEpoch [110/200] | Train Loss: 0.0263, Train Acc: 0.9899 | \nEpoch [111/200] | Train Loss: 0.0318, Train Acc: 0.9865 | \nEpoch [112/200] | Train Loss: 0.0335, Train Acc: 0.9899 | \nEpoch [113/200] | Train Loss: 0.0167, Train Acc: 0.9966 | \nEpoch [114/200] | Train Loss: 0.0207, Train Acc: 0.9899 | \nEpoch [115/200] | Train Loss: 0.0153, Train Acc: 1.0000 | \nEpoch [116/200] | Train Loss: 0.0209, Train Acc: 0.9933 | \nEpoch [117/200] | Train Loss: 0.0118, Train Acc: 1.0000 | \nEpoch [118/200] | Train Loss: 0.0204, Train Acc: 0.9899 | \nEpoch [119/200] | Train Loss: 0.0175, Train Acc: 0.9933 | \nEpoch [120/200] | Train Loss: 0.0126, Train Acc: 1.0000 | \nEpoch [121/200] | Train Loss: 0.0113, Train Acc: 0.9966 | \nEpoch [122/200] | Train Loss: 0.0149, Train Acc: 1.0000 | \nEpoch [123/200] | Train Loss: 0.0091, Train Acc: 1.0000 | \nEpoch [124/200] | Train Loss: 0.0194, Train Acc: 0.9966 | \nEpoch [125/200] | Train Loss: 0.0163, Train Acc: 0.9899 | \nEpoch [126/200] | Train Loss: 0.0071, Train Acc: 1.0000 | \nEpoch [127/200] | Train Loss: 0.0198, Train Acc: 0.9899 | \nEpoch [128/200] | Train Loss: 0.0071, Train Acc: 1.0000 | \nEpoch [129/200] | Train Loss: 0.0061, Train Acc: 1.0000 | \nEpoch [130/200] | Train Loss: 0.0100, Train Acc: 0.9966 | \nEpoch [131/200] | Train Loss: 0.0111, Train Acc: 0.9966 | \nEpoch [132/200] | Train Loss: 0.0172, Train Acc: 0.9933 | \nEpoch [133/200] | Train Loss: 0.0341, Train Acc: 0.9899 | \nEpoch [134/200] | Train Loss: 0.0136, Train Acc: 0.9966 | \nEarly stopping triggered.\nTest Loss: 0.7883, Test Accuracy: 0.8400, Test AUC: 0.9282\n\n--- Processing: psd_fc_gamma ---\nShape: (372, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7686, Train Acc: 0.4646 | \nEpoch [2/200] | Train Loss: 0.7067, Train Acc: 0.4983 | \nEpoch [3/200] | Train Loss: 0.6991, Train Acc: 0.5017 | \nEpoch [4/200] | Train Loss: 0.6938, Train Acc: 0.5118 | \nEpoch [5/200] | Train Loss: 0.6931, Train Acc: 0.5118 | \nEpoch [6/200] | Train Loss: 0.6943, Train Acc: 0.5185 | \nEpoch [7/200] | Train Loss: 0.6906, Train Acc: 0.5455 | \nEpoch [8/200] | Train Loss: 0.6822, Train Acc: 0.5724 | \nEpoch [9/200] | Train Loss: 0.6919, Train Acc: 0.5320 | \nEpoch [10/200] | Train Loss: 0.6316, Train Acc: 0.6734 | \nEpoch [11/200] | Train Loss: 0.6661, Train Acc: 0.6094 | \nEpoch [12/200] | Train Loss: 0.6155, Train Acc: 0.6734 | \nEpoch [13/200] | Train Loss: 0.5440, Train Acc: 0.7306 | \nEpoch [14/200] | Train Loss: 0.5489, Train Acc: 0.7239 | \nEpoch [15/200] | Train Loss: 0.5388, Train Acc: 0.7104 | \nEpoch [16/200] | Train Loss: 0.5001, Train Acc: 0.7677 | \nEpoch [17/200] | Train Loss: 0.4819, Train Acc: 0.7542 | \nEpoch [18/200] | Train Loss: 0.4326, Train Acc: 0.8114 | \nEpoch [19/200] | Train Loss: 0.4411, Train Acc: 0.8283 | \nEpoch [20/200] | Train Loss: 0.4304, Train Acc: 0.8047 | \nEpoch [21/200] | Train Loss: 0.4080, Train Acc: 0.8215 | \nEpoch [22/200] | Train Loss: 0.4135, Train Acc: 0.8081 | \nEpoch [23/200] | Train Loss: 0.4329, Train Acc: 0.8148 | \nEpoch [24/200] | Train Loss: 0.4536, Train Acc: 0.7912 | \nEpoch [25/200] | Train Loss: 0.3992, Train Acc: 0.8283 | \nEpoch [26/200] | Train Loss: 0.3556, Train Acc: 0.8552 | \nEpoch [27/200] | Train Loss: 0.3163, Train Acc: 0.8721 | \nEpoch [28/200] | Train Loss: 0.3356, Train Acc: 0.8653 | \nEpoch [29/200] | Train Loss: 0.3490, Train Acc: 0.8485 | \nEpoch [30/200] | Train Loss: 0.5242, Train Acc: 0.7306 | \nEpoch [31/200] | Train Loss: 0.3939, Train Acc: 0.8418 | \nEpoch [32/200] | Train Loss: 0.3419, Train Acc: 0.8552 | \nEpoch [33/200] | Train Loss: 0.3197, Train Acc: 0.8552 | \nEpoch [34/200] | Train Loss: 0.3160, Train Acc: 0.8822 | \nEpoch [35/200] | Train Loss: 0.3190, Train Acc: 0.8350 | \nEpoch [36/200] | Train Loss: 0.4393, Train Acc: 0.8182 | \nEpoch [37/200] | Train Loss: 0.3328, Train Acc: 0.8451 | \nEpoch [38/200] | Train Loss: 0.3146, Train Acc: 0.8653 | \nEpoch [39/200] | Train Loss: 0.3703, Train Acc: 0.8283 | \nEpoch [40/200] | Train Loss: 0.2837, Train Acc: 0.8889 | \nEpoch [41/200] | Train Loss: 0.3134, Train Acc: 0.8552 | \nEpoch [42/200] | Train Loss: 0.2258, Train Acc: 0.9158 | \nEpoch [43/200] | Train Loss: 0.2715, Train Acc: 0.8754 | \nEpoch [44/200] | Train Loss: 0.3005, Train Acc: 0.8687 | \nEpoch [45/200] | Train Loss: 0.4064, Train Acc: 0.8148 | \nEpoch [46/200] | Train Loss: 0.4600, Train Acc: 0.7778 | \nEpoch [47/200] | Train Loss: 0.4054, Train Acc: 0.8114 | \nEpoch [48/200] | Train Loss: 0.3691, Train Acc: 0.8249 | \nEpoch [49/200] | Train Loss: 0.2576, Train Acc: 0.8956 | \nEpoch [50/200] | Train Loss: 0.2123, Train Acc: 0.9192 | \nEpoch [51/200] | Train Loss: 0.3233, Train Acc: 0.8653 | \nEpoch [52/200] | Train Loss: 0.2923, Train Acc: 0.8822 | \nEpoch [53/200] | Train Loss: 0.2723, Train Acc: 0.8990 | \nEpoch [54/200] | Train Loss: 0.2543, Train Acc: 0.9024 | \nEpoch [55/200] | Train Loss: 0.2017, Train Acc: 0.9125 | \nEpoch [56/200] | Train Loss: 0.2188, Train Acc: 0.9293 | \nEpoch [57/200] | Train Loss: 0.2581, Train Acc: 0.8990 | \nEpoch [58/200] | Train Loss: 0.2972, Train Acc: 0.8721 | \nEpoch [59/200] | Train Loss: 0.2454, Train Acc: 0.8822 | \nEpoch [60/200] | Train Loss: 0.2036, Train Acc: 0.9293 | \nEpoch [61/200] | Train Loss: 0.1987, Train Acc: 0.9158 | \nEpoch [62/200] | Train Loss: 0.1466, Train Acc: 0.9461 | \nEpoch [63/200] | Train Loss: 0.2432, Train Acc: 0.9057 | \nEpoch [64/200] | Train Loss: 0.2477, Train Acc: 0.8855 | \nEpoch [65/200] | Train Loss: 0.2634, Train Acc: 0.8822 | \nEpoch [66/200] | Train Loss: 0.2521, Train Acc: 0.9057 | \nEpoch [67/200] | Train Loss: 0.2098, Train Acc: 0.9125 | \nEpoch [68/200] | Train Loss: 0.1986, Train Acc: 0.9293 | \nEpoch [69/200] | Train Loss: 0.1686, Train Acc: 0.9529 | \nEpoch [70/200] | Train Loss: 0.1612, Train Acc: 0.9327 | \nEpoch [71/200] | Train Loss: 0.1651, Train Acc: 0.9428 | \nEpoch [72/200] | Train Loss: 0.1418, Train Acc: 0.9461 | \nEpoch [73/200] | Train Loss: 0.1126, Train Acc: 0.9596 | \nEpoch [74/200] | Train Loss: 0.1349, Train Acc: 0.9461 | \nEpoch [75/200] | Train Loss: 0.3054, Train Acc: 0.8990 | \nEpoch [76/200] | Train Loss: 0.2858, Train Acc: 0.8687 | \nEpoch [77/200] | Train Loss: 0.2401, Train Acc: 0.8889 | \nEpoch [78/200] | Train Loss: 0.2605, Train Acc: 0.8889 | \nEpoch [79/200] | Train Loss: 0.2278, Train Acc: 0.9192 | \nEpoch [80/200] | Train Loss: 0.2361, Train Acc: 0.9024 | \nEpoch [81/200] | Train Loss: 0.1969, Train Acc: 0.9158 | \nEpoch [82/200] | Train Loss: 0.2167, Train Acc: 0.9125 | \nEpoch [83/200] | Train Loss: 0.1461, Train Acc: 0.9394 | \nEpoch 00084: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [84/200] | Train Loss: 0.1496, Train Acc: 0.9562 | \nEpoch [85/200] | Train Loss: 0.3435, Train Acc: 0.8451 | \nEpoch [86/200] | Train Loss: 0.1229, Train Acc: 0.9562 | \nEpoch [87/200] | Train Loss: 0.1156, Train Acc: 0.9630 | \nEpoch [88/200] | Train Loss: 0.0979, Train Acc: 0.9663 | \nEpoch [89/200] | Train Loss: 0.0934, Train Acc: 0.9697 | \nEpoch [90/200] | Train Loss: 0.0812, Train Acc: 0.9832 | \nEpoch [91/200] | Train Loss: 0.0693, Train Acc: 0.9832 | \nEpoch [92/200] | Train Loss: 0.0707, Train Acc: 0.9832 | \nEpoch [93/200] | Train Loss: 0.0646, Train Acc: 0.9798 | \nEpoch [94/200] | Train Loss: 0.0649, Train Acc: 0.9832 | \nEpoch [95/200] | Train Loss: 0.0612, Train Acc: 0.9832 | \nEpoch [96/200] | Train Loss: 0.0547, Train Acc: 0.9865 | \nEpoch [97/200] | Train Loss: 0.0465, Train Acc: 0.9899 | \nEpoch [98/200] | Train Loss: 0.0524, Train Acc: 0.9933 | \nEpoch [99/200] | Train Loss: 0.0501, Train Acc: 0.9865 | \nEpoch [100/200] | Train Loss: 0.0437, Train Acc: 0.9865 | \nEpoch [101/200] | Train Loss: 0.0422, Train Acc: 0.9933 | \nEpoch [102/200] | Train Loss: 0.0533, Train Acc: 0.9832 | \nEpoch [103/200] | Train Loss: 0.0449, Train Acc: 0.9832 | \nEpoch [104/200] | Train Loss: 0.0294, Train Acc: 0.9933 | \nEpoch [105/200] | Train Loss: 0.0447, Train Acc: 0.9865 | \nEpoch [106/200] | Train Loss: 0.0397, Train Acc: 0.9899 | \nEpoch [107/200] | Train Loss: 0.0321, Train Acc: 0.9899 | \nEpoch [108/200] | Train Loss: 0.0349, Train Acc: 0.9899 | \nEpoch [109/200] | Train Loss: 0.0408, Train Acc: 0.9764 | \nEpoch [110/200] | Train Loss: 0.0268, Train Acc: 0.9933 | \nEpoch [111/200] | Train Loss: 0.0315, Train Acc: 0.9832 | \nEpoch [112/200] | Train Loss: 0.0340, Train Acc: 0.9798 | \nEpoch [113/200] | Train Loss: 0.0243, Train Acc: 0.9966 | \nEpoch [114/200] | Train Loss: 0.0180, Train Acc: 1.0000 | \nEpoch [115/200] | Train Loss: 0.0363, Train Acc: 0.9832 | \nEpoch [116/200] | Train Loss: 0.0280, Train Acc: 0.9966 | \nEpoch [117/200] | Train Loss: 0.0189, Train Acc: 0.9966 | \nEpoch [118/200] | Train Loss: 0.0301, Train Acc: 0.9933 | \nEpoch [119/200] | Train Loss: 0.0159, Train Acc: 0.9966 | \nEpoch [120/200] | Train Loss: 0.0215, Train Acc: 0.9933 | \nEpoch [121/200] | Train Loss: 0.0149, Train Acc: 0.9966 | \nEpoch [122/200] | Train Loss: 0.0253, Train Acc: 0.9899 | \nEpoch [123/200] | Train Loss: 0.0176, Train Acc: 0.9966 | \nEpoch [124/200] | Train Loss: 0.0276, Train Acc: 0.9933 | \nEpoch [125/200] | Train Loss: 0.0179, Train Acc: 0.9933 | \nEpoch [126/200] | Train Loss: 0.0158, Train Acc: 0.9966 | \nEpoch [127/200] | Train Loss: 0.0128, Train Acc: 1.0000 | \nEpoch [128/200] | Train Loss: 0.0162, Train Acc: 0.9966 | \nEpoch [129/200] | Train Loss: 0.0106, Train Acc: 0.9966 | \nEpoch [130/200] | Train Loss: 0.0262, Train Acc: 0.9933 | \nEpoch [131/200] | Train Loss: 0.0145, Train Acc: 0.9966 | \nEpoch [132/200] | Train Loss: 0.0112, Train Acc: 1.0000 | \nEpoch [133/200] | Train Loss: 0.0094, Train Acc: 0.9966 | \nEarly stopping triggered.\nTest Loss: 0.8438, Test Accuracy: 0.8267, Test AUC: 0.9275\n\n=== Disorder: Anxiety disorder ===\n\n--- Processing: psd_all_bands ---\nShape: (214, 114)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7207, Train Acc: 0.4795 | \nEpoch [2/200] | Train Loss: 0.7089, Train Acc: 0.4912 | \nEpoch [3/200] | Train Loss: 0.6869, Train Acc: 0.5322 | \nEpoch [4/200] | Train Loss: 0.6662, Train Acc: 0.6023 | \nEpoch [5/200] | Train Loss: 0.6408, Train Acc: 0.5906 | \nEpoch [6/200] | Train Loss: 0.5617, Train Acc: 0.7544 | \nEpoch [7/200] | Train Loss: 0.4319, Train Acc: 0.8012 | \nEpoch [8/200] | Train Loss: 0.4047, Train Acc: 0.8070 | \nEpoch [9/200] | Train Loss: 0.3440, Train Acc: 0.8596 | \nEpoch [10/200] | Train Loss: 0.3802, Train Acc: 0.8480 | \nEpoch [11/200] | Train Loss: 0.2865, Train Acc: 0.9006 | \nEpoch [12/200] | Train Loss: 0.3049, Train Acc: 0.8772 | \nEpoch [13/200] | Train Loss: 0.2755, Train Acc: 0.9006 | \nEpoch [14/200] | Train Loss: 0.2381, Train Acc: 0.9240 | \nEpoch [15/200] | Train Loss: 0.1920, Train Acc: 0.9415 | \nEpoch [16/200] | Train Loss: 0.1693, Train Acc: 0.9474 | \nEpoch [17/200] | Train Loss: 0.1792, Train Acc: 0.9532 | \nEpoch [18/200] | Train Loss: 0.1961, Train Acc: 0.9298 | \nEpoch [19/200] | Train Loss: 0.3464, Train Acc: 0.8713 | \nEpoch [20/200] | Train Loss: 0.3302, Train Acc: 0.8713 | \nEpoch [21/200] | Train Loss: 0.2526, Train Acc: 0.9064 | \nEpoch [22/200] | Train Loss: 0.2161, Train Acc: 0.9298 | \nEpoch [23/200] | Train Loss: 0.2500, Train Acc: 0.9006 | \nEpoch [24/200] | Train Loss: 0.2264, Train Acc: 0.9123 | \nEpoch [25/200] | Train Loss: 0.1626, Train Acc: 0.9474 | \nEpoch [26/200] | Train Loss: 0.1579, Train Acc: 0.9474 | \nEpoch [27/200] | Train Loss: 0.1281, Train Acc: 0.9649 | \nEpoch [28/200] | Train Loss: 0.2539, Train Acc: 0.8772 | \nEpoch [29/200] | Train Loss: 0.3703, Train Acc: 0.8304 | \nEpoch [30/200] | Train Loss: 0.2385, Train Acc: 0.9064 | \nEpoch [31/200] | Train Loss: 0.2138, Train Acc: 0.9240 | \nEpoch [32/200] | Train Loss: 0.1667, Train Acc: 0.9649 | \nEpoch [33/200] | Train Loss: 0.1239, Train Acc: 0.9649 | \nEpoch [34/200] | Train Loss: 0.1689, Train Acc: 0.9474 | \nEpoch [35/200] | Train Loss: 0.2028, Train Acc: 0.9181 | \nEpoch [36/200] | Train Loss: 0.1616, Train Acc: 0.9474 | \nEpoch [37/200] | Train Loss: 0.2321, Train Acc: 0.9357 | \nEpoch [38/200] | Train Loss: 0.1749, Train Acc: 0.9474 | \nEpoch [39/200] | Train Loss: 0.1351, Train Acc: 0.9649 | \nEpoch [40/200] | Train Loss: 0.1328, Train Acc: 0.9591 | \nEpoch [41/200] | Train Loss: 0.1173, Train Acc: 0.9649 | \nEpoch [42/200] | Train Loss: 0.0992, Train Acc: 0.9708 | \nEpoch [43/200] | Train Loss: 0.0977, Train Acc: 0.9708 | \nEpoch [44/200] | Train Loss: 0.1020, Train Acc: 0.9766 | \nEpoch [45/200] | Train Loss: 0.1175, Train Acc: 0.9532 | \nEpoch [46/200] | Train Loss: 0.0990, Train Acc: 0.9708 | \nEpoch [47/200] | Train Loss: 0.1965, Train Acc: 0.9357 | \nEpoch [48/200] | Train Loss: 0.4117, Train Acc: 0.9006 | \nEpoch [49/200] | Train Loss: 0.2591, Train Acc: 0.9181 | \nEpoch [50/200] | Train Loss: 0.2522, Train Acc: 0.9123 | \nEpoch [51/200] | Train Loss: 0.1985, Train Acc: 0.9123 | \nEpoch [52/200] | Train Loss: 0.1674, Train Acc: 0.9357 | \nEpoch [53/200] | Train Loss: 0.1830, Train Acc: 0.9357 | \nEpoch 00054: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [54/200] | Train Loss: 0.2234, Train Acc: 0.9181 | \nEpoch [55/200] | Train Loss: 0.1676, Train Acc: 0.9415 | \nEpoch [56/200] | Train Loss: 0.1459, Train Acc: 0.9532 | \nEpoch [57/200] | Train Loss: 0.1267, Train Acc: 0.9532 | \nEpoch [58/200] | Train Loss: 0.1107, Train Acc: 0.9591 | \nEpoch [59/200] | Train Loss: 0.1040, Train Acc: 0.9649 | \nEpoch [60/200] | Train Loss: 0.1131, Train Acc: 0.9708 | \nEpoch [61/200] | Train Loss: 0.0982, Train Acc: 0.9649 | \nEpoch [62/200] | Train Loss: 0.0970, Train Acc: 0.9708 | \nEpoch [63/200] | Train Loss: 0.0937, Train Acc: 0.9766 | \nEarly stopping triggered.\nTest Loss: 0.9699, Test Accuracy: 0.7209, Test AUC: 0.7814\n\n--- Processing: fc_all_bands ---\nShape: (214, 1026)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7680, Train Acc: 0.4912 | \nEpoch [2/200] | Train Loss: 0.7109, Train Acc: 0.4620 | \nEpoch [3/200] | Train Loss: 0.6932, Train Acc: 0.5029 | \nEpoch [4/200] | Train Loss: 0.6985, Train Acc: 0.4561 | \nEpoch [5/200] | Train Loss: 0.6930, Train Acc: 0.5146 | \nEpoch [6/200] | Train Loss: 0.6994, Train Acc: 0.5029 | \nEpoch [7/200] | Train Loss: 0.6890, Train Acc: 0.5205 | \nEpoch [8/200] | Train Loss: 0.6906, Train Acc: 0.5205 | \nEpoch [9/200] | Train Loss: 0.6780, Train Acc: 0.5906 | \nEpoch [10/200] | Train Loss: 0.6461, Train Acc: 0.6491 | \nEpoch [11/200] | Train Loss: 0.6269, Train Acc: 0.6374 | \nEpoch [12/200] | Train Loss: 0.5560, Train Acc: 0.6959 | \nEpoch [13/200] | Train Loss: 0.5807, Train Acc: 0.6667 | \nEpoch [14/200] | Train Loss: 0.5509, Train Acc: 0.6901 | \nEpoch [15/200] | Train Loss: 0.5113, Train Acc: 0.7310 | \nEpoch [16/200] | Train Loss: 0.4483, Train Acc: 0.8070 | \nEpoch [17/200] | Train Loss: 0.5377, Train Acc: 0.7602 | \nEpoch [18/200] | Train Loss: 0.4289, Train Acc: 0.7953 | \nEpoch [19/200] | Train Loss: 0.3854, Train Acc: 0.8480 | \nEpoch [20/200] | Train Loss: 0.3282, Train Acc: 0.8363 | \nEpoch [21/200] | Train Loss: 0.2856, Train Acc: 0.8830 | \nEpoch [22/200] | Train Loss: 0.2752, Train Acc: 0.8889 | \nEpoch [23/200] | Train Loss: 0.3440, Train Acc: 0.8889 | \nEpoch [24/200] | Train Loss: 0.3212, Train Acc: 0.8363 | \nEpoch [25/200] | Train Loss: 0.2665, Train Acc: 0.9006 | \nEpoch [26/200] | Train Loss: 0.2666, Train Acc: 0.8947 | \nEpoch [27/200] | Train Loss: 0.2144, Train Acc: 0.8947 | \nEpoch [28/200] | Train Loss: 0.2068, Train Acc: 0.9298 | \nEpoch [29/200] | Train Loss: 0.2247, Train Acc: 0.9006 | \nEpoch [30/200] | Train Loss: 0.2258, Train Acc: 0.9240 | \nEpoch [31/200] | Train Loss: 0.1544, Train Acc: 0.9591 | \nEpoch [32/200] | Train Loss: 0.1284, Train Acc: 0.9532 | \nEpoch [33/200] | Train Loss: 0.1673, Train Acc: 0.9298 | \nEpoch [34/200] | Train Loss: 0.3237, Train Acc: 0.8947 | \nEpoch [35/200] | Train Loss: 0.2203, Train Acc: 0.9064 | \nEpoch [36/200] | Train Loss: 0.1967, Train Acc: 0.9181 | \nEpoch [37/200] | Train Loss: 0.2166, Train Acc: 0.9006 | \nEpoch [38/200] | Train Loss: 0.1541, Train Acc: 0.9298 | \nEpoch [39/200] | Train Loss: 0.1952, Train Acc: 0.8830 | \nEpoch [40/200] | Train Loss: 0.3398, Train Acc: 0.8480 | \nEpoch [41/200] | Train Loss: 0.2878, Train Acc: 0.8596 | \nEpoch [42/200] | Train Loss: 0.2144, Train Acc: 0.9240 | \nEpoch 00043: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [43/200] | Train Loss: 0.1641, Train Acc: 0.9415 | \nEpoch [44/200] | Train Loss: 0.1477, Train Acc: 0.9298 | \nEpoch [45/200] | Train Loss: 0.1488, Train Acc: 0.9474 | \nEpoch [46/200] | Train Loss: 0.1242, Train Acc: 0.9649 | \nEpoch [47/200] | Train Loss: 0.1056, Train Acc: 0.9708 | \nEpoch [48/200] | Train Loss: 0.1064, Train Acc: 0.9649 | \nEpoch [49/200] | Train Loss: 0.1054, Train Acc: 0.9708 | \nEpoch [50/200] | Train Loss: 0.0987, Train Acc: 0.9649 | \nEpoch [51/200] | Train Loss: 0.0780, Train Acc: 0.9825 | \nEpoch [52/200] | Train Loss: 0.1053, Train Acc: 0.9708 | \nEpoch [53/200] | Train Loss: 0.0831, Train Acc: 0.9708 | \nEpoch [54/200] | Train Loss: 0.0767, Train Acc: 0.9825 | \nEpoch [55/200] | Train Loss: 0.0765, Train Acc: 0.9708 | \nEpoch [56/200] | Train Loss: 0.0789, Train Acc: 0.9591 | \nEpoch [57/200] | Train Loss: 0.0863, Train Acc: 0.9708 | \nEpoch [58/200] | Train Loss: 0.0807, Train Acc: 0.9591 | \nEpoch [59/200] | Train Loss: 0.0766, Train Acc: 0.9649 | \nEpoch [60/200] | Train Loss: 0.0684, Train Acc: 0.9708 | \nEpoch [61/200] | Train Loss: 0.0690, Train Acc: 0.9825 | \nEpoch [62/200] | Train Loss: 0.0515, Train Acc: 0.9883 | \nEpoch [63/200] | Train Loss: 0.0522, Train Acc: 0.9766 | \nEpoch [64/200] | Train Loss: 0.0616, Train Acc: 0.9766 | \nEpoch [65/200] | Train Loss: 0.0592, Train Acc: 0.9825 | \nEpoch [66/200] | Train Loss: 0.0476, Train Acc: 0.9942 | \nEpoch [67/200] | Train Loss: 0.0665, Train Acc: 0.9825 | \nEpoch [68/200] | Train Loss: 0.0531, Train Acc: 0.9825 | \nEpoch [69/200] | Train Loss: 0.0388, Train Acc: 0.9942 | \nEpoch [70/200] | Train Loss: 0.0335, Train Acc: 0.9942 | \nEpoch [71/200] | Train Loss: 0.0341, Train Acc: 0.9942 | \nEpoch [72/200] | Train Loss: 0.0639, Train Acc: 0.9708 | \nEpoch [73/200] | Train Loss: 0.0365, Train Acc: 0.9883 | \nEpoch [74/200] | Train Loss: 0.0364, Train Acc: 0.9883 | \nEpoch [75/200] | Train Loss: 0.0292, Train Acc: 0.9883 | \nEpoch [76/200] | Train Loss: 0.0292, Train Acc: 0.9942 | \nEpoch [77/200] | Train Loss: 0.0298, Train Acc: 0.9942 | \nEpoch [78/200] | Train Loss: 0.0224, Train Acc: 0.9942 | \nEpoch [79/200] | Train Loss: 0.0262, Train Acc: 0.9942 | \nEpoch [80/200] | Train Loss: 0.0270, Train Acc: 0.9942 | \nEpoch [81/200] | Train Loss: 0.0174, Train Acc: 0.9942 | \nEpoch [82/200] | Train Loss: 0.0178, Train Acc: 0.9942 | \nEpoch [83/200] | Train Loss: 0.0264, Train Acc: 0.9883 | \nEpoch [84/200] | Train Loss: 0.0259, Train Acc: 0.9883 | \nEpoch [85/200] | Train Loss: 0.0241, Train Acc: 0.9883 | \nEarly stopping triggered.\nTest Loss: 1.5535, Test Accuracy: 0.6279, Test AUC: 0.6753\n\n--- Processing: psd_fc_all_bands ---\nShape: (214, 1140)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7749, Train Acc: 0.5263 | \nEpoch [2/200] | Train Loss: 0.7329, Train Acc: 0.5029 | \nEpoch [3/200] | Train Loss: 0.7014, Train Acc: 0.5029 | \nEpoch [4/200] | Train Loss: 0.7027, Train Acc: 0.5029 | \nEpoch [5/200] | Train Loss: 0.6984, Train Acc: 0.5029 | \nEpoch [6/200] | Train Loss: 0.7027, Train Acc: 0.5205 | \nEpoch [7/200] | Train Loss: 0.7018, Train Acc: 0.4971 | \nEpoch [8/200] | Train Loss: 0.6947, Train Acc: 0.5205 | \nEpoch [9/200] | Train Loss: 0.6981, Train Acc: 0.4678 | \nEpoch [10/200] | Train Loss: 0.6886, Train Acc: 0.5088 | \nEpoch [11/200] | Train Loss: 0.6785, Train Acc: 0.6550 | \nEpoch [12/200] | Train Loss: 0.6202, Train Acc: 0.7485 | \nEpoch [13/200] | Train Loss: 0.5384, Train Acc: 0.7368 | \nEpoch [14/200] | Train Loss: 0.5412, Train Acc: 0.7544 | \nEpoch [15/200] | Train Loss: 0.4604, Train Acc: 0.7836 | \nEpoch [16/200] | Train Loss: 0.4572, Train Acc: 0.7602 | \nEpoch [17/200] | Train Loss: 0.4081, Train Acc: 0.8304 | \nEpoch [18/200] | Train Loss: 0.3797, Train Acc: 0.8421 | \nEpoch [19/200] | Train Loss: 0.3806, Train Acc: 0.8187 | \nEpoch [20/200] | Train Loss: 0.2749, Train Acc: 0.9064 | \nEpoch [21/200] | Train Loss: 0.3303, Train Acc: 0.8713 | \nEpoch [22/200] | Train Loss: 0.4978, Train Acc: 0.7661 | \nEpoch [23/200] | Train Loss: 0.3345, Train Acc: 0.8713 | \nEpoch [24/200] | Train Loss: 0.6488, Train Acc: 0.7135 | \nEpoch [25/200] | Train Loss: 0.6163, Train Acc: 0.6491 | \nEpoch [26/200] | Train Loss: 0.4994, Train Acc: 0.7661 | \nEpoch [27/200] | Train Loss: 0.4692, Train Acc: 0.7778 | \nEpoch [28/200] | Train Loss: 0.4596, Train Acc: 0.8129 | \nEpoch [29/200] | Train Loss: 0.3654, Train Acc: 0.8421 | \nEpoch [30/200] | Train Loss: 0.3883, Train Acc: 0.8246 | \nEpoch 00031: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [31/200] | Train Loss: 0.3588, Train Acc: 0.8713 | \nEpoch [32/200] | Train Loss: 0.3407, Train Acc: 0.8538 | \nEpoch [33/200] | Train Loss: 0.3203, Train Acc: 0.8596 | \nEpoch [34/200] | Train Loss: 0.2901, Train Acc: 0.8889 | \nEpoch [35/200] | Train Loss: 0.2724, Train Acc: 0.8947 | \nEpoch [36/200] | Train Loss: 0.2589, Train Acc: 0.8947 | \nEpoch [37/200] | Train Loss: 0.2569, Train Acc: 0.8830 | \nEpoch [38/200] | Train Loss: 0.2715, Train Acc: 0.9181 | \nEpoch [39/200] | Train Loss: 0.2501, Train Acc: 0.9123 | \nEpoch [40/200] | Train Loss: 0.2385, Train Acc: 0.9298 | \nEpoch [41/200] | Train Loss: 0.2366, Train Acc: 0.9181 | \nEpoch [42/200] | Train Loss: 0.2239, Train Acc: 0.9181 | \nEpoch [43/200] | Train Loss: 0.2161, Train Acc: 0.9240 | \nEpoch [44/200] | Train Loss: 0.2180, Train Acc: 0.9181 | \nEpoch [45/200] | Train Loss: 0.1931, Train Acc: 0.9415 | \nEpoch [46/200] | Train Loss: 0.1829, Train Acc: 0.9415 | \nEpoch [47/200] | Train Loss: 0.1924, Train Acc: 0.9474 | \nEpoch [48/200] | Train Loss: 0.1684, Train Acc: 0.9591 | \nEpoch [49/200] | Train Loss: 0.1648, Train Acc: 0.9591 | \nEpoch [50/200] | Train Loss: 0.1476, Train Acc: 0.9766 | \nEpoch [51/200] | Train Loss: 0.1484, Train Acc: 0.9532 | \nEpoch [52/200] | Train Loss: 0.1425, Train Acc: 0.9708 | \nEpoch [53/200] | Train Loss: 0.1320, Train Acc: 0.9766 | \nEpoch [54/200] | Train Loss: 0.1369, Train Acc: 0.9649 | \nEpoch [55/200] | Train Loss: 0.1302, Train Acc: 0.9708 | \nEpoch [56/200] | Train Loss: 0.1267, Train Acc: 0.9766 | \nEpoch [57/200] | Train Loss: 0.1389, Train Acc: 0.9708 | \nEpoch [58/200] | Train Loss: 0.1405, Train Acc: 0.9649 | \nEpoch [59/200] | Train Loss: 0.1239, Train Acc: 0.9708 | \nEpoch [60/200] | Train Loss: 0.1282, Train Acc: 0.9708 | \nEpoch [61/200] | Train Loss: 0.1303, Train Acc: 0.9649 | \nEpoch [62/200] | Train Loss: 0.1262, Train Acc: 0.9708 | \nEpoch [63/200] | Train Loss: 0.1545, Train Acc: 0.9649 | \nEpoch [64/200] | Train Loss: 0.1029, Train Acc: 0.9825 | \nEpoch [65/200] | Train Loss: 0.1365, Train Acc: 0.9649 | \nEpoch [66/200] | Train Loss: 0.1340, Train Acc: 0.9649 | \nEpoch [67/200] | Train Loss: 0.1231, Train Acc: 0.9708 | \nEpoch [68/200] | Train Loss: 0.1586, Train Acc: 0.9591 | \nEpoch [69/200] | Train Loss: 0.1639, Train Acc: 0.9532 | \nEpoch [70/200] | Train Loss: 0.1672, Train Acc: 0.9532 | \nEpoch [71/200] | Train Loss: 0.1549, Train Acc: 0.9591 | \nEpoch [72/200] | Train Loss: 0.1336, Train Acc: 0.9708 | \nEpoch [73/200] | Train Loss: 0.1216, Train Acc: 0.9708 | \nEpoch [74/200] | Train Loss: 0.1177, Train Acc: 0.9766 | \nEpoch 00075: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [75/200] | Train Loss: 0.1134, Train Acc: 0.9766 | \nEpoch [76/200] | Train Loss: 0.1005, Train Acc: 0.9766 | \nEpoch [77/200] | Train Loss: 0.1074, Train Acc: 0.9766 | \nEpoch [78/200] | Train Loss: 0.1067, Train Acc: 0.9766 | \nEpoch [79/200] | Train Loss: 0.1109, Train Acc: 0.9708 | \nEpoch [80/200] | Train Loss: 0.1058, Train Acc: 0.9766 | \nEpoch [81/200] | Train Loss: 0.1032, Train Acc: 0.9766 | \nEpoch [82/200] | Train Loss: 0.1110, Train Acc: 0.9766 | \nEpoch [83/200] | Train Loss: 0.0958, Train Acc: 0.9766 | \nEarly stopping triggered.\nTest Loss: 0.7385, Test Accuracy: 0.7674, Test AUC: 0.7273\n\n--- Processing: psd_delta ---\nShape: (214, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7811, Train Acc: 0.4795 | \nEpoch [2/200] | Train Loss: 0.6739, Train Acc: 0.6140 | \nEpoch [3/200] | Train Loss: 0.6415, Train Acc: 0.6082 | \nEpoch [4/200] | Train Loss: 0.5250, Train Acc: 0.7368 | \nEpoch [5/200] | Train Loss: 0.3856, Train Acc: 0.8421 | \nEpoch [6/200] | Train Loss: 0.4110, Train Acc: 0.8480 | \nEpoch [7/200] | Train Loss: 0.3956, Train Acc: 0.8246 | \nEpoch [8/200] | Train Loss: 0.4840, Train Acc: 0.7485 | \nEpoch [9/200] | Train Loss: 0.4758, Train Acc: 0.8070 | \nEpoch [10/200] | Train Loss: 0.3964, Train Acc: 0.8304 | \nEpoch [11/200] | Train Loss: 0.3640, Train Acc: 0.8304 | \nEpoch [12/200] | Train Loss: 0.3668, Train Acc: 0.8421 | \nEpoch [13/200] | Train Loss: 0.2956, Train Acc: 0.8889 | \nEpoch [14/200] | Train Loss: 0.3671, Train Acc: 0.8538 | \nEpoch [15/200] | Train Loss: 0.3151, Train Acc: 0.8713 | \nEpoch [16/200] | Train Loss: 0.3035, Train Acc: 0.8947 | \nEpoch [17/200] | Train Loss: 0.2890, Train Acc: 0.8772 | \nEpoch [18/200] | Train Loss: 0.3040, Train Acc: 0.8713 | \nEpoch [19/200] | Train Loss: 0.3606, Train Acc: 0.8538 | \nEpoch [20/200] | Train Loss: 0.2956, Train Acc: 0.8889 | \nEpoch [21/200] | Train Loss: 0.3287, Train Acc: 0.8655 | \nEpoch [22/200] | Train Loss: 0.3390, Train Acc: 0.8538 | \nEpoch [23/200] | Train Loss: 0.3004, Train Acc: 0.8538 | \nEpoch [24/200] | Train Loss: 0.2831, Train Acc: 0.8889 | \nEpoch [25/200] | Train Loss: 0.2633, Train Acc: 0.9064 | \nEpoch [26/200] | Train Loss: 0.2848, Train Acc: 0.8772 | \nEpoch [27/200] | Train Loss: 0.2574, Train Acc: 0.9064 | \nEpoch [28/200] | Train Loss: 0.2812, Train Acc: 0.9006 | \nEpoch [29/200] | Train Loss: 0.2676, Train Acc: 0.8889 | \nEpoch [30/200] | Train Loss: 0.2600, Train Acc: 0.9123 | \nEpoch [31/200] | Train Loss: 0.2536, Train Acc: 0.9064 | \nEpoch [32/200] | Train Loss: 0.2410, Train Acc: 0.9064 | \nEpoch [33/200] | Train Loss: 0.2424, Train Acc: 0.8889 | \nEpoch [34/200] | Train Loss: 0.1924, Train Acc: 0.9064 | \nEpoch [35/200] | Train Loss: 0.2627, Train Acc: 0.9123 | \nEpoch [36/200] | Train Loss: 0.2534, Train Acc: 0.9064 | \nEpoch [37/200] | Train Loss: 0.2630, Train Acc: 0.8713 | \nEpoch [38/200] | Train Loss: 0.2318, Train Acc: 0.9123 | \nEpoch [39/200] | Train Loss: 0.1984, Train Acc: 0.9181 | \nEpoch [40/200] | Train Loss: 0.1681, Train Acc: 0.9357 | \nEpoch [41/200] | Train Loss: 0.2083, Train Acc: 0.9298 | \nEpoch [42/200] | Train Loss: 0.2380, Train Acc: 0.9006 | \nEpoch [43/200] | Train Loss: 0.2274, Train Acc: 0.9181 | \nEpoch [44/200] | Train Loss: 0.1851, Train Acc: 0.9298 | \nEpoch [45/200] | Train Loss: 0.2639, Train Acc: 0.9240 | \nEpoch [46/200] | Train Loss: 0.3528, Train Acc: 0.8304 | \nEpoch [47/200] | Train Loss: 0.3104, Train Acc: 0.8538 | \nEpoch [48/200] | Train Loss: 0.2618, Train Acc: 0.8830 | \nEpoch [49/200] | Train Loss: 0.2148, Train Acc: 0.9064 | \nEpoch [50/200] | Train Loss: 0.2228, Train Acc: 0.9064 | \nEpoch 00051: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [51/200] | Train Loss: 0.2151, Train Acc: 0.9181 | \nEpoch [52/200] | Train Loss: 0.1660, Train Acc: 0.9357 | \nEpoch [53/200] | Train Loss: 0.1822, Train Acc: 0.9357 | \nEpoch [54/200] | Train Loss: 0.1812, Train Acc: 0.9240 | \nEpoch [55/200] | Train Loss: 0.1666, Train Acc: 0.9240 | \nEpoch [56/200] | Train Loss: 0.1697, Train Acc: 0.9240 | \nEpoch [57/200] | Train Loss: 0.1648, Train Acc: 0.9298 | \nEpoch [58/200] | Train Loss: 0.1781, Train Acc: 0.9415 | \nEpoch [59/200] | Train Loss: 0.1586, Train Acc: 0.9415 | \nEpoch [60/200] | Train Loss: 0.1650, Train Acc: 0.9357 | \nEpoch [61/200] | Train Loss: 0.1629, Train Acc: 0.9357 | \nEpoch [62/200] | Train Loss: 0.1604, Train Acc: 0.9357 | \nEpoch [63/200] | Train Loss: 0.1556, Train Acc: 0.9357 | \nEpoch [64/200] | Train Loss: 0.1612, Train Acc: 0.9298 | \nEpoch [65/200] | Train Loss: 0.1441, Train Acc: 0.9532 | \nEpoch [66/200] | Train Loss: 0.1542, Train Acc: 0.9298 | \nEpoch [67/200] | Train Loss: 0.1446, Train Acc: 0.9474 | \nEpoch [68/200] | Train Loss: 0.1528, Train Acc: 0.9298 | \nEpoch [69/200] | Train Loss: 0.1490, Train Acc: 0.9415 | \nEpoch [70/200] | Train Loss: 0.1508, Train Acc: 0.9240 | \nEpoch [71/200] | Train Loss: 0.1469, Train Acc: 0.9415 | \nEpoch [72/200] | Train Loss: 0.1511, Train Acc: 0.9415 | \nEpoch [73/200] | Train Loss: 0.1397, Train Acc: 0.9474 | \nEpoch [74/200] | Train Loss: 0.1448, Train Acc: 0.9474 | \nEpoch [75/200] | Train Loss: 0.1411, Train Acc: 0.9474 | \nEpoch [76/200] | Train Loss: 0.1522, Train Acc: 0.9415 | \nEpoch [77/200] | Train Loss: 0.1369, Train Acc: 0.9474 | \nEpoch [78/200] | Train Loss: 0.1508, Train Acc: 0.9415 | \nEpoch [79/200] | Train Loss: 0.1357, Train Acc: 0.9415 | \nEpoch [80/200] | Train Loss: 0.1358, Train Acc: 0.9532 | \nEpoch [81/200] | Train Loss: 0.1288, Train Acc: 0.9532 | \nEpoch [82/200] | Train Loss: 0.1267, Train Acc: 0.9474 | \nEpoch [83/200] | Train Loss: 0.1499, Train Acc: 0.9357 | \nEpoch [84/200] | Train Loss: 0.1314, Train Acc: 0.9532 | \nEarly stopping triggered.\nTest Loss: 1.0316, Test Accuracy: 0.7674, Test AUC: 0.8593\n\n--- Processing: psd_theta ---\nShape: (214, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.8189, Train Acc: 0.4912 | \nEpoch [2/200] | Train Loss: 0.6811, Train Acc: 0.5731 | \nEpoch [3/200] | Train Loss: 0.6381, Train Acc: 0.6199 | \nEpoch [4/200] | Train Loss: 0.5631, Train Acc: 0.6901 | \nEpoch [5/200] | Train Loss: 0.4995, Train Acc: 0.7836 | \nEpoch [6/200] | Train Loss: 0.4830, Train Acc: 0.7719 | \nEpoch [7/200] | Train Loss: 0.4287, Train Acc: 0.8187 | \nEpoch [8/200] | Train Loss: 0.3766, Train Acc: 0.8538 | \nEpoch [9/200] | Train Loss: 0.3708, Train Acc: 0.8655 | \nEpoch [10/200] | Train Loss: 0.3755, Train Acc: 0.8538 | \nEpoch [11/200] | Train Loss: 0.4430, Train Acc: 0.8363 | \nEpoch [12/200] | Train Loss: 0.3574, Train Acc: 0.8596 | \nEpoch [13/200] | Train Loss: 0.3441, Train Acc: 0.8538 | \nEpoch [14/200] | Train Loss: 0.3540, Train Acc: 0.8480 | \nEpoch [15/200] | Train Loss: 0.3207, Train Acc: 0.8713 | \nEpoch [16/200] | Train Loss: 0.2954, Train Acc: 0.8772 | \nEpoch [17/200] | Train Loss: 0.2763, Train Acc: 0.9064 | \nEpoch [18/200] | Train Loss: 0.2535, Train Acc: 0.8947 | \nEpoch [19/200] | Train Loss: 0.2928, Train Acc: 0.8830 | \nEpoch [20/200] | Train Loss: 0.2776, Train Acc: 0.8889 | \nEpoch [21/200] | Train Loss: 0.2918, Train Acc: 0.8596 | \nEpoch [22/200] | Train Loss: 0.3193, Train Acc: 0.8655 | \nEpoch [23/200] | Train Loss: 0.2614, Train Acc: 0.9006 | \nEpoch [24/200] | Train Loss: 0.3088, Train Acc: 0.8713 | \nEpoch [25/200] | Train Loss: 0.2372, Train Acc: 0.9240 | \nEpoch [26/200] | Train Loss: 0.2313, Train Acc: 0.9064 | \nEpoch [27/200] | Train Loss: 0.2040, Train Acc: 0.9006 | \nEpoch [28/200] | Train Loss: 0.2248, Train Acc: 0.9240 | \nEpoch [29/200] | Train Loss: 0.1971, Train Acc: 0.9123 | \nEpoch [30/200] | Train Loss: 0.2170, Train Acc: 0.9064 | \nEpoch [31/200] | Train Loss: 0.1952, Train Acc: 0.9181 | \nEpoch [32/200] | Train Loss: 0.2387, Train Acc: 0.9064 | \nEpoch [33/200] | Train Loss: 0.2204, Train Acc: 0.9064 | \nEpoch [34/200] | Train Loss: 0.2828, Train Acc: 0.8480 | \nEpoch [35/200] | Train Loss: 0.2835, Train Acc: 0.8713 | \nEpoch [36/200] | Train Loss: 0.2019, Train Acc: 0.9006 | \nEpoch [37/200] | Train Loss: 0.2334, Train Acc: 0.9064 | \nEpoch [38/200] | Train Loss: 0.2509, Train Acc: 0.8889 | \nEpoch [39/200] | Train Loss: 0.3083, Train Acc: 0.8421 | \nEpoch [40/200] | Train Loss: 0.2744, Train Acc: 0.8713 | \nEpoch [41/200] | Train Loss: 0.2614, Train Acc: 0.8538 | \nEpoch 00042: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [42/200] | Train Loss: 0.2610, Train Acc: 0.9006 | \nEpoch [43/200] | Train Loss: 0.2898, Train Acc: 0.8596 | \nEpoch [44/200] | Train Loss: 0.1839, Train Acc: 0.9064 | \nEarly stopping triggered.\nTest Loss: 0.5249, Test Accuracy: 0.7907, Test AUC: 0.8853\n\n--- Processing: psd_alpha ---\nShape: (214, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7798, Train Acc: 0.4795 | \nEpoch [2/200] | Train Loss: 0.7260, Train Acc: 0.4503 | \nEpoch [3/200] | Train Loss: 0.6640, Train Acc: 0.6491 | \nEpoch [4/200] | Train Loss: 0.6097, Train Acc: 0.6784 | \nEpoch [5/200] | Train Loss: 0.5154, Train Acc: 0.7836 | \nEpoch [6/200] | Train Loss: 0.4157, Train Acc: 0.8363 | \nEpoch [7/200] | Train Loss: 0.4158, Train Acc: 0.8304 | \nEpoch [8/200] | Train Loss: 0.3748, Train Acc: 0.8596 | \nEpoch [9/200] | Train Loss: 0.4425, Train Acc: 0.8012 | \nEpoch [10/200] | Train Loss: 0.5354, Train Acc: 0.7778 | \nEpoch [11/200] | Train Loss: 0.4835, Train Acc: 0.8012 | \nEpoch [12/200] | Train Loss: 0.4452, Train Acc: 0.8012 | \nEpoch [13/200] | Train Loss: 0.4142, Train Acc: 0.8421 | \nEpoch [14/200] | Train Loss: 0.3906, Train Acc: 0.8538 | \nEpoch [15/200] | Train Loss: 0.3816, Train Acc: 0.8596 | \nEpoch [16/200] | Train Loss: 0.4361, Train Acc: 0.8363 | \nEpoch [17/200] | Train Loss: 0.4193, Train Acc: 0.8363 | \nEpoch [18/200] | Train Loss: 0.3911, Train Acc: 0.8187 | \nEpoch [19/200] | Train Loss: 0.3322, Train Acc: 0.8713 | \nEpoch [20/200] | Train Loss: 0.3431, Train Acc: 0.8596 | \nEpoch [21/200] | Train Loss: 0.3322, Train Acc: 0.8713 | \nEpoch [22/200] | Train Loss: 0.3239, Train Acc: 0.8538 | \nEpoch [23/200] | Train Loss: 0.3157, Train Acc: 0.8655 | \nEpoch [24/200] | Train Loss: 0.3098, Train Acc: 0.8655 | \nEpoch [25/200] | Train Loss: 0.3409, Train Acc: 0.8538 | \nEpoch [26/200] | Train Loss: 0.3394, Train Acc: 0.8655 | \nEpoch [27/200] | Train Loss: 0.2899, Train Acc: 0.8830 | \nEpoch [28/200] | Train Loss: 0.3674, Train Acc: 0.8596 | \nEpoch [29/200] | Train Loss: 0.3195, Train Acc: 0.8538 | \nEpoch [30/200] | Train Loss: 0.3118, Train Acc: 0.9006 | \nEpoch [31/200] | Train Loss: 0.2994, Train Acc: 0.8772 | \nEpoch [32/200] | Train Loss: 0.3593, Train Acc: 0.8655 | \nEpoch [33/200] | Train Loss: 0.3161, Train Acc: 0.8889 | \nEpoch [34/200] | Train Loss: 0.3188, Train Acc: 0.8655 | \nEpoch [35/200] | Train Loss: 0.3170, Train Acc: 0.8596 | \nEpoch [36/200] | Train Loss: 0.2969, Train Acc: 0.8947 | \nEpoch [37/200] | Train Loss: 0.2647, Train Acc: 0.9064 | \nEpoch [38/200] | Train Loss: 0.2872, Train Acc: 0.8655 | \nEpoch [39/200] | Train Loss: 0.2989, Train Acc: 0.8947 | \nEpoch [40/200] | Train Loss: 0.3449, Train Acc: 0.8713 | \nEpoch [41/200] | Train Loss: 0.2757, Train Acc: 0.9181 | \nEpoch [42/200] | Train Loss: 0.2814, Train Acc: 0.8713 | \nEpoch [43/200] | Train Loss: 0.2315, Train Acc: 0.9357 | \nEpoch [44/200] | Train Loss: 0.2190, Train Acc: 0.9298 | \nEpoch [45/200] | Train Loss: 0.2235, Train Acc: 0.9298 | \nEpoch [46/200] | Train Loss: 0.2362, Train Acc: 0.9240 | \nEpoch [47/200] | Train Loss: 0.2838, Train Acc: 0.9064 | \nEpoch [48/200] | Train Loss: 0.3546, Train Acc: 0.8713 | \nEpoch [49/200] | Train Loss: 0.3198, Train Acc: 0.8772 | \nEpoch [50/200] | Train Loss: 0.3649, Train Acc: 0.8596 | \nEpoch [51/200] | Train Loss: 0.2885, Train Acc: 0.8830 | \nEpoch [52/200] | Train Loss: 0.2859, Train Acc: 0.8830 | \nEpoch [53/200] | Train Loss: 0.3221, Train Acc: 0.8772 | \nEpoch [54/200] | Train Loss: 0.2531, Train Acc: 0.8830 | \nEpoch 00055: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [55/200] | Train Loss: 0.2502, Train Acc: 0.8889 | \nEpoch [56/200] | Train Loss: 0.3119, Train Acc: 0.8713 | \nEpoch [57/200] | Train Loss: 0.2559, Train Acc: 0.8889 | \nEpoch [58/200] | Train Loss: 0.2225, Train Acc: 0.9064 | \nEpoch [59/200] | Train Loss: 0.2260, Train Acc: 0.9181 | \nEpoch [60/200] | Train Loss: 0.2197, Train Acc: 0.9123 | \nEpoch [61/200] | Train Loss: 0.2231, Train Acc: 0.9064 | \nEpoch [62/200] | Train Loss: 0.2282, Train Acc: 0.9123 | \nEarly stopping triggered.\nTest Loss: 0.3611, Test Accuracy: 0.8372, Test AUC: 0.9026\n\n--- Processing: psd_beta ---\nShape: (214, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.8294, Train Acc: 0.4211 | \nEpoch [2/200] | Train Loss: 0.6877, Train Acc: 0.4854 | \nEpoch [3/200] | Train Loss: 0.6340, Train Acc: 0.6959 | \nEpoch [4/200] | Train Loss: 0.5855, Train Acc: 0.6959 | \nEpoch [5/200] | Train Loss: 0.6272, Train Acc: 0.7251 | \nEpoch [6/200] | Train Loss: 0.5559, Train Acc: 0.7251 | \nEpoch [7/200] | Train Loss: 0.5118, Train Acc: 0.7544 | \nEpoch [8/200] | Train Loss: 0.4932, Train Acc: 0.7719 | \nEpoch [9/200] | Train Loss: 0.4152, Train Acc: 0.8129 | \nEpoch [10/200] | Train Loss: 0.3945, Train Acc: 0.8363 | \nEpoch [11/200] | Train Loss: 0.3930, Train Acc: 0.8655 | \nEpoch [12/200] | Train Loss: 0.4237, Train Acc: 0.8246 | \nEpoch [13/200] | Train Loss: 0.3896, Train Acc: 0.8538 | \nEpoch [14/200] | Train Loss: 0.3558, Train Acc: 0.8596 | \nEpoch [15/200] | Train Loss: 0.3279, Train Acc: 0.8889 | \nEpoch [16/200] | Train Loss: 0.3418, Train Acc: 0.8596 | \nEpoch [17/200] | Train Loss: 0.3321, Train Acc: 0.8830 | \nEpoch [18/200] | Train Loss: 0.3665, Train Acc: 0.8363 | \nEpoch [19/200] | Train Loss: 0.4414, Train Acc: 0.8363 | \nEpoch [20/200] | Train Loss: 0.3472, Train Acc: 0.8596 | \nEpoch [21/200] | Train Loss: 0.3174, Train Acc: 0.8655 | \nEpoch [22/200] | Train Loss: 0.3147, Train Acc: 0.8830 | \nEpoch [23/200] | Train Loss: 0.3212, Train Acc: 0.8655 | \nEpoch [24/200] | Train Loss: 0.3005, Train Acc: 0.8772 | \nEpoch [25/200] | Train Loss: 0.2990, Train Acc: 0.8772 | \nEpoch [26/200] | Train Loss: 0.2951, Train Acc: 0.8655 | \nEpoch [27/200] | Train Loss: 0.3065, Train Acc: 0.8830 | \nEpoch [28/200] | Train Loss: 0.2345, Train Acc: 0.9006 | \nEpoch [29/200] | Train Loss: 0.2960, Train Acc: 0.8889 | \nEpoch [30/200] | Train Loss: 0.3453, Train Acc: 0.8480 | \nEpoch [31/200] | Train Loss: 0.3363, Train Acc: 0.8363 | \nEpoch [32/200] | Train Loss: 0.2922, Train Acc: 0.8830 | \nEpoch [33/200] | Train Loss: 0.2691, Train Acc: 0.8772 | \nEpoch [34/200] | Train Loss: 0.2612, Train Acc: 0.8830 | \nEpoch [35/200] | Train Loss: 0.2580, Train Acc: 0.9006 | \nEpoch [36/200] | Train Loss: 0.3659, Train Acc: 0.8538 | \nEpoch [37/200] | Train Loss: 0.3515, Train Acc: 0.8538 | \nEpoch [38/200] | Train Loss: 0.2865, Train Acc: 0.8830 | \nEpoch 00039: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [39/200] | Train Loss: 0.2466, Train Acc: 0.8947 | \nEpoch [40/200] | Train Loss: 0.2643, Train Acc: 0.8830 | \nEpoch [41/200] | Train Loss: 0.2381, Train Acc: 0.8947 | \nEpoch [42/200] | Train Loss: 0.2228, Train Acc: 0.8947 | \nEpoch [43/200] | Train Loss: 0.2223, Train Acc: 0.9123 | \nEpoch [44/200] | Train Loss: 0.2024, Train Acc: 0.9181 | \nEpoch [45/200] | Train Loss: 0.2118, Train Acc: 0.9123 | \nEpoch [46/200] | Train Loss: 0.2151, Train Acc: 0.9123 | \nEpoch [47/200] | Train Loss: 0.2008, Train Acc: 0.9181 | \nEpoch [48/200] | Train Loss: 0.1912, Train Acc: 0.9240 | \nEpoch [49/200] | Train Loss: 0.1889, Train Acc: 0.9123 | \nEpoch [50/200] | Train Loss: 0.1913, Train Acc: 0.9181 | \nEpoch [51/200] | Train Loss: 0.1878, Train Acc: 0.9240 | \nEpoch [52/200] | Train Loss: 0.1785, Train Acc: 0.9298 | \nEpoch [53/200] | Train Loss: 0.1815, Train Acc: 0.9181 | \nEpoch [54/200] | Train Loss: 0.1850, Train Acc: 0.9240 | \nEpoch [55/200] | Train Loss: 0.1860, Train Acc: 0.9123 | \nEpoch [56/200] | Train Loss: 0.1713, Train Acc: 0.9298 | \nEpoch [57/200] | Train Loss: 0.1726, Train Acc: 0.9240 | \nEpoch [58/200] | Train Loss: 0.1761, Train Acc: 0.9240 | \nEpoch [59/200] | Train Loss: 0.1475, Train Acc: 0.9357 | \nEpoch [60/200] | Train Loss: 0.1736, Train Acc: 0.9240 | \nEpoch [61/200] | Train Loss: 0.1550, Train Acc: 0.9298 | \nEpoch [62/200] | Train Loss: 0.1698, Train Acc: 0.9240 | \nEpoch [63/200] | Train Loss: 0.1680, Train Acc: 0.9240 | \nEpoch [64/200] | Train Loss: 0.1988, Train Acc: 0.9240 | \nEpoch [65/200] | Train Loss: 0.1645, Train Acc: 0.9298 | \nEpoch [66/200] | Train Loss: 0.1509, Train Acc: 0.9298 | \nEpoch [67/200] | Train Loss: 0.1560, Train Acc: 0.9298 | \nEpoch [68/200] | Train Loss: 0.1525, Train Acc: 0.9415 | \nEpoch [69/200] | Train Loss: 0.1434, Train Acc: 0.9474 | \nEpoch [70/200] | Train Loss: 0.1556, Train Acc: 0.9181 | \nEpoch [71/200] | Train Loss: 0.1690, Train Acc: 0.9240 | \nEpoch [72/200] | Train Loss: 0.1488, Train Acc: 0.9357 | \nEpoch [73/200] | Train Loss: 0.1413, Train Acc: 0.9357 | \nEpoch [74/200] | Train Loss: 0.1404, Train Acc: 0.9415 | \nEpoch [75/200] | Train Loss: 0.1582, Train Acc: 0.9357 | \nEpoch [76/200] | Train Loss: 0.1394, Train Acc: 0.9415 | \nEpoch [77/200] | Train Loss: 0.1578, Train Acc: 0.9357 | \nEpoch [78/200] | Train Loss: 0.1463, Train Acc: 0.9298 | \nEpoch [79/200] | Train Loss: 0.1496, Train Acc: 0.9240 | \nEpoch [80/200] | Train Loss: 0.1546, Train Acc: 0.9240 | \nEpoch [81/200] | Train Loss: 0.1391, Train Acc: 0.9298 | \nEpoch [82/200] | Train Loss: 0.1319, Train Acc: 0.9532 | \nEpoch [83/200] | Train Loss: 0.1277, Train Acc: 0.9532 | \nEpoch [84/200] | Train Loss: 0.1387, Train Acc: 0.9357 | \nEpoch [85/200] | Train Loss: 0.1596, Train Acc: 0.9181 | \nEpoch [86/200] | Train Loss: 0.1513, Train Acc: 0.9474 | \nEpoch [87/200] | Train Loss: 0.1358, Train Acc: 0.9474 | \nEpoch [88/200] | Train Loss: 0.1216, Train Acc: 0.9591 | \nEpoch [89/200] | Train Loss: 0.1665, Train Acc: 0.9240 | \nEpoch [90/200] | Train Loss: 0.1288, Train Acc: 0.9415 | \nEpoch [91/200] | Train Loss: 0.1196, Train Acc: 0.9474 | \nEpoch [92/200] | Train Loss: 0.1200, Train Acc: 0.9474 | \nEpoch [93/200] | Train Loss: 0.1238, Train Acc: 0.9474 | \nEpoch [94/200] | Train Loss: 0.1261, Train Acc: 0.9415 | \nEpoch [95/200] | Train Loss: 0.1099, Train Acc: 0.9532 | \nEpoch [96/200] | Train Loss: 0.1229, Train Acc: 0.9474 | \nEpoch [97/200] | Train Loss: 0.1171, Train Acc: 0.9532 | \nEpoch [98/200] | Train Loss: 0.1064, Train Acc: 0.9708 | \nEpoch [99/200] | Train Loss: 0.1189, Train Acc: 0.9474 | \nEpoch [100/200] | Train Loss: 0.1081, Train Acc: 0.9532 | \nEpoch [101/200] | Train Loss: 0.1132, Train Acc: 0.9532 | \nEpoch [102/200] | Train Loss: 0.1049, Train Acc: 0.9708 | \nEpoch [103/200] | Train Loss: 0.1111, Train Acc: 0.9591 | \nEpoch [104/200] | Train Loss: 0.1109, Train Acc: 0.9708 | \nEpoch [105/200] | Train Loss: 0.1260, Train Acc: 0.9415 | \nEpoch [106/200] | Train Loss: 0.1141, Train Acc: 0.9532 | \nEpoch [107/200] | Train Loss: 0.1365, Train Acc: 0.9357 | \nEpoch [108/200] | Train Loss: 0.1526, Train Acc: 0.9298 | \nEpoch [109/200] | Train Loss: 0.1487, Train Acc: 0.9415 | \nEpoch [110/200] | Train Loss: 0.1107, Train Acc: 0.9649 | \nEpoch [111/200] | Train Loss: 0.1307, Train Acc: 0.9532 | \nEpoch [112/200] | Train Loss: 0.1189, Train Acc: 0.9532 | \nEpoch 00113: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [113/200] | Train Loss: 0.1084, Train Acc: 0.9766 | \nEpoch [114/200] | Train Loss: 0.1314, Train Acc: 0.9474 | \nEpoch [115/200] | Train Loss: 0.1117, Train Acc: 0.9649 | \nEpoch [116/200] | Train Loss: 0.1121, Train Acc: 0.9591 | \nEpoch [117/200] | Train Loss: 0.1254, Train Acc: 0.9415 | \nEpoch [118/200] | Train Loss: 0.1106, Train Acc: 0.9649 | \nEpoch [119/200] | Train Loss: 0.1300, Train Acc: 0.9649 | \nEpoch [120/200] | Train Loss: 0.1094, Train Acc: 0.9649 | \nEpoch [121/200] | Train Loss: 0.1023, Train Acc: 0.9591 | \nEpoch [122/200] | Train Loss: 0.1101, Train Acc: 0.9766 | \nEpoch [123/200] | Train Loss: 0.0984, Train Acc: 0.9708 | \nEpoch [124/200] | Train Loss: 0.1136, Train Acc: 0.9708 | \nEpoch [125/200] | Train Loss: 0.1151, Train Acc: 0.9591 | \nEpoch [126/200] | Train Loss: 0.1034, Train Acc: 0.9649 | \nEpoch [127/200] | Train Loss: 0.1088, Train Acc: 0.9649 | \nEpoch [128/200] | Train Loss: 0.0912, Train Acc: 0.9825 | \nEpoch [129/200] | Train Loss: 0.1103, Train Acc: 0.9649 | \nEpoch [130/200] | Train Loss: 0.1030, Train Acc: 0.9708 | \nEpoch [131/200] | Train Loss: 0.0938, Train Acc: 0.9766 | \nEpoch [132/200] | Train Loss: 0.1025, Train Acc: 0.9766 | \nEpoch [133/200] | Train Loss: 0.1092, Train Acc: 0.9708 | \nEpoch [134/200] | Train Loss: 0.1035, Train Acc: 0.9708 | \nEpoch [135/200] | Train Loss: 0.1086, Train Acc: 0.9591 | \nEpoch [136/200] | Train Loss: 0.1009, Train Acc: 0.9591 | \nEpoch [137/200] | Train Loss: 0.1158, Train Acc: 0.9649 | \nEpoch [138/200] | Train Loss: 0.1291, Train Acc: 0.9591 | \nEpoch 00139: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [139/200] | Train Loss: 0.1101, Train Acc: 0.9591 | \nEpoch [140/200] | Train Loss: 0.1273, Train Acc: 0.9591 | \nEpoch [141/200] | Train Loss: 0.1196, Train Acc: 0.9649 | \nEpoch [142/200] | Train Loss: 0.0956, Train Acc: 0.9649 | \nEpoch [143/200] | Train Loss: 0.1164, Train Acc: 0.9708 | \nEpoch [144/200] | Train Loss: 0.0943, Train Acc: 0.9825 | \nEpoch [145/200] | Train Loss: 0.1197, Train Acc: 0.9708 | \nEpoch [146/200] | Train Loss: 0.1045, Train Acc: 0.9708 | \nEpoch [147/200] | Train Loss: 0.1065, Train Acc: 0.9649 | \nEarly stopping triggered.\nTest Loss: 0.4464, Test Accuracy: 0.8372, Test AUC: 0.9286\n\n--- Processing: psd_highbeta ---\nShape: (214, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7926, Train Acc: 0.4912 | \nEpoch [2/200] | Train Loss: 0.6907, Train Acc: 0.5439 | \nEpoch [3/200] | Train Loss: 0.6072, Train Acc: 0.6784 | \nEpoch [4/200] | Train Loss: 0.5651, Train Acc: 0.7193 | \nEpoch [5/200] | Train Loss: 0.4742, Train Acc: 0.7895 | \nEpoch [6/200] | Train Loss: 0.4359, Train Acc: 0.8246 | \nEpoch [7/200] | Train Loss: 0.4812, Train Acc: 0.7836 | \nEpoch [8/200] | Train Loss: 0.4533, Train Acc: 0.8012 | \nEpoch [9/200] | Train Loss: 0.4127, Train Acc: 0.8304 | \nEpoch [10/200] | Train Loss: 0.3799, Train Acc: 0.8421 | \nEpoch [11/200] | Train Loss: 0.4191, Train Acc: 0.8363 | \nEpoch [12/200] | Train Loss: 0.4559, Train Acc: 0.7953 | \nEpoch [13/200] | Train Loss: 0.3587, Train Acc: 0.8538 | \nEpoch [14/200] | Train Loss: 0.3668, Train Acc: 0.8304 | \nEpoch [15/200] | Train Loss: 0.3311, Train Acc: 0.8538 | \nEpoch [16/200] | Train Loss: 0.3178, Train Acc: 0.8713 | \nEpoch [17/200] | Train Loss: 0.3482, Train Acc: 0.8538 | \nEpoch [18/200] | Train Loss: 0.5452, Train Acc: 0.7953 | \nEpoch [19/200] | Train Loss: 0.4287, Train Acc: 0.8129 | \nEpoch [20/200] | Train Loss: 0.3740, Train Acc: 0.8596 | \nEpoch [21/200] | Train Loss: 0.3483, Train Acc: 0.8480 | \nEpoch [22/200] | Train Loss: 0.3278, Train Acc: 0.8772 | \nEpoch [23/200] | Train Loss: 0.3266, Train Acc: 0.8830 | \nEpoch [24/200] | Train Loss: 0.3094, Train Acc: 0.8830 | \nEpoch [25/200] | Train Loss: 0.3274, Train Acc: 0.8596 | \nEpoch [26/200] | Train Loss: 0.2989, Train Acc: 0.8830 | \nEpoch [27/200] | Train Loss: 0.2920, Train Acc: 0.8947 | \nEpoch [28/200] | Train Loss: 0.2896, Train Acc: 0.9006 | \nEpoch [29/200] | Train Loss: 0.2892, Train Acc: 0.9006 | \nEpoch [30/200] | Train Loss: 0.2826, Train Acc: 0.8889 | \nEpoch [31/200] | Train Loss: 0.2908, Train Acc: 0.8889 | \nEpoch [32/200] | Train Loss: 0.2501, Train Acc: 0.8947 | \nEpoch [33/200] | Train Loss: 0.2518, Train Acc: 0.8947 | \nEpoch [34/200] | Train Loss: 0.2846, Train Acc: 0.9006 | \nEpoch [35/200] | Train Loss: 0.3606, Train Acc: 0.8889 | \nEpoch [36/200] | Train Loss: 0.3110, Train Acc: 0.8830 | \nEpoch [37/200] | Train Loss: 0.3294, Train Acc: 0.8889 | \nEpoch [38/200] | Train Loss: 0.2919, Train Acc: 0.8830 | \nEpoch [39/200] | Train Loss: 0.2853, Train Acc: 0.8889 | \nEpoch [40/200] | Train Loss: 0.2496, Train Acc: 0.9123 | \nEpoch [41/200] | Train Loss: 0.2440, Train Acc: 0.9064 | \nEpoch [42/200] | Train Loss: 0.2740, Train Acc: 0.8889 | \nEpoch [43/200] | Train Loss: 0.2495, Train Acc: 0.9006 | \nEpoch [44/200] | Train Loss: 0.3223, Train Acc: 0.8596 | \nEpoch [45/200] | Train Loss: 0.2949, Train Acc: 0.9006 | \nEpoch [46/200] | Train Loss: 0.3496, Train Acc: 0.8596 | \nEpoch [47/200] | Train Loss: 0.3633, Train Acc: 0.8538 | \nEpoch [48/200] | Train Loss: 0.3405, Train Acc: 0.8538 | \nEpoch [49/200] | Train Loss: 0.3665, Train Acc: 0.8480 | \nEpoch [50/200] | Train Loss: 0.2794, Train Acc: 0.8947 | \nEpoch [51/200] | Train Loss: 0.2627, Train Acc: 0.8830 | \nEpoch 00052: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [52/200] | Train Loss: 0.2522, Train Acc: 0.9064 | \nEpoch [53/200] | Train Loss: 0.2930, Train Acc: 0.8947 | \nEpoch [54/200] | Train Loss: 0.2495, Train Acc: 0.9123 | \nEpoch [55/200] | Train Loss: 0.2402, Train Acc: 0.9064 | \nEpoch [56/200] | Train Loss: 0.2649, Train Acc: 0.9123 | \nEpoch [57/200] | Train Loss: 0.2408, Train Acc: 0.9240 | \nEpoch [58/200] | Train Loss: 0.2374, Train Acc: 0.9181 | \nEpoch [59/200] | Train Loss: 0.2354, Train Acc: 0.9240 | \nEpoch [60/200] | Train Loss: 0.2097, Train Acc: 0.9357 | \nEpoch [61/200] | Train Loss: 0.2104, Train Acc: 0.9357 | \nEpoch [62/200] | Train Loss: 0.2334, Train Acc: 0.9181 | \nEpoch [63/200] | Train Loss: 0.2159, Train Acc: 0.9415 | \nEpoch [64/200] | Train Loss: 0.2153, Train Acc: 0.9298 | \nEpoch [65/200] | Train Loss: 0.1981, Train Acc: 0.9298 | \nEpoch [66/200] | Train Loss: 0.2129, Train Acc: 0.9298 | \nEpoch [67/200] | Train Loss: 0.2014, Train Acc: 0.9298 | \nEpoch [68/200] | Train Loss: 0.2232, Train Acc: 0.9181 | \nEpoch [69/200] | Train Loss: 0.2380, Train Acc: 0.9298 | \nEpoch [70/200] | Train Loss: 0.1975, Train Acc: 0.9357 | \nEpoch [71/200] | Train Loss: 0.2161, Train Acc: 0.9123 | \nEpoch [72/200] | Train Loss: 0.2180, Train Acc: 0.9006 | \nEpoch [73/200] | Train Loss: 0.2005, Train Acc: 0.9298 | \nEpoch [74/200] | Train Loss: 0.1934, Train Acc: 0.9357 | \nEpoch [75/200] | Train Loss: 0.2257, Train Acc: 0.9298 | \nEpoch [76/200] | Train Loss: 0.2112, Train Acc: 0.9298 | \nEpoch [77/200] | Train Loss: 0.2171, Train Acc: 0.9298 | \nEpoch [78/200] | Train Loss: 0.2043, Train Acc: 0.9298 | \nEpoch [79/200] | Train Loss: 0.1924, Train Acc: 0.9357 | \nEpoch [80/200] | Train Loss: 0.1828, Train Acc: 0.9474 | \nEpoch [81/200] | Train Loss: 0.1924, Train Acc: 0.9357 | \nEpoch [82/200] | Train Loss: 0.1768, Train Acc: 0.9357 | \nEpoch [83/200] | Train Loss: 0.1865, Train Acc: 0.9474 | \nEpoch [84/200] | Train Loss: 0.2086, Train Acc: 0.9298 | \nEpoch [85/200] | Train Loss: 0.1707, Train Acc: 0.9532 | \nEpoch [86/200] | Train Loss: 0.1769, Train Acc: 0.9415 | \nEpoch [87/200] | Train Loss: 0.1822, Train Acc: 0.9357 | \nEpoch [88/200] | Train Loss: 0.1902, Train Acc: 0.9357 | \nEpoch [89/200] | Train Loss: 0.1739, Train Acc: 0.9415 | \nEpoch [90/200] | Train Loss: 0.1692, Train Acc: 0.9532 | \nEpoch [91/200] | Train Loss: 0.2002, Train Acc: 0.9357 | \nEpoch [92/200] | Train Loss: 0.1743, Train Acc: 0.9474 | \nEpoch [93/200] | Train Loss: 0.1496, Train Acc: 0.9591 | \nEpoch [94/200] | Train Loss: 0.1779, Train Acc: 0.9415 | \nEpoch [95/200] | Train Loss: 0.1778, Train Acc: 0.9415 | \nEpoch [96/200] | Train Loss: 0.1925, Train Acc: 0.9357 | \nEpoch [97/200] | Train Loss: 0.1684, Train Acc: 0.9474 | \nEpoch [98/200] | Train Loss: 0.1714, Train Acc: 0.9474 | \nEpoch [99/200] | Train Loss: 0.1524, Train Acc: 0.9591 | \nEpoch [100/200] | Train Loss: 0.1697, Train Acc: 0.9474 | \nEpoch [101/200] | Train Loss: 0.1736, Train Acc: 0.9415 | \nEpoch [102/200] | Train Loss: 0.1710, Train Acc: 0.9474 | \nEpoch [103/200] | Train Loss: 0.1441, Train Acc: 0.9532 | \nEpoch [104/200] | Train Loss: 0.1703, Train Acc: 0.9474 | \nEpoch [105/200] | Train Loss: 0.1517, Train Acc: 0.9591 | \nEpoch [106/200] | Train Loss: 0.1361, Train Acc: 0.9591 | \nEpoch [107/200] | Train Loss: 0.1463, Train Acc: 0.9532 | \nEpoch [108/200] | Train Loss: 0.1495, Train Acc: 0.9474 | \nEpoch [109/200] | Train Loss: 0.1601, Train Acc: 0.9474 | \nEpoch [110/200] | Train Loss: 0.1703, Train Acc: 0.9415 | \nEpoch [111/200] | Train Loss: 0.1606, Train Acc: 0.9415 | \nEpoch [112/200] | Train Loss: 0.1907, Train Acc: 0.9240 | \nEpoch [113/200] | Train Loss: 0.1226, Train Acc: 0.9649 | \nEpoch [114/200] | Train Loss: 0.1619, Train Acc: 0.9532 | \nEpoch [115/200] | Train Loss: 0.1378, Train Acc: 0.9532 | \nEpoch [116/200] | Train Loss: 0.1334, Train Acc: 0.9649 | \nEpoch [117/200] | Train Loss: 0.1240, Train Acc: 0.9708 | \nEpoch [118/200] | Train Loss: 0.1330, Train Acc: 0.9649 | \nEpoch [119/200] | Train Loss: 0.1339, Train Acc: 0.9591 | \nEpoch [120/200] | Train Loss: 0.1450, Train Acc: 0.9474 | \nEpoch [121/200] | Train Loss: 0.1294, Train Acc: 0.9649 | \nEpoch [122/200] | Train Loss: 0.1187, Train Acc: 0.9649 | \nEpoch [123/200] | Train Loss: 0.1176, Train Acc: 0.9649 | \nEpoch [124/200] | Train Loss: 0.1392, Train Acc: 0.9591 | \nEpoch [125/200] | Train Loss: 0.1088, Train Acc: 0.9649 | \nEpoch [126/200] | Train Loss: 0.1146, Train Acc: 0.9708 | \nEpoch [127/200] | Train Loss: 0.1285, Train Acc: 0.9649 | \nEpoch [128/200] | Train Loss: 0.1489, Train Acc: 0.9649 | \nEpoch [129/200] | Train Loss: 0.1489, Train Acc: 0.9532 | \nEpoch [130/200] | Train Loss: 0.1597, Train Acc: 0.9591 | \nEpoch [131/200] | Train Loss: 0.1405, Train Acc: 0.9532 | \nEpoch [132/200] | Train Loss: 0.1141, Train Acc: 0.9708 | \nEpoch [133/200] | Train Loss: 0.1367, Train Acc: 0.9649 | \nEpoch [134/200] | Train Loss: 0.1208, Train Acc: 0.9708 | \nEpoch [135/200] | Train Loss: 0.1326, Train Acc: 0.9474 | \nEpoch 00136: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [136/200] | Train Loss: 0.1407, Train Acc: 0.9474 | \nEarly stopping triggered.\nTest Loss: 0.6380, Test Accuracy: 0.8372, Test AUC: 0.8636\n\n--- Processing: psd_gamma ---\nShape: (214, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7998, Train Acc: 0.4444 | \nEpoch [2/200] | Train Loss: 0.6717, Train Acc: 0.6140 | \nEpoch [3/200] | Train Loss: 0.6687, Train Acc: 0.5556 | \nEpoch [4/200] | Train Loss: 0.6234, Train Acc: 0.7251 | \nEpoch [5/200] | Train Loss: 0.5583, Train Acc: 0.7427 | \nEpoch [6/200] | Train Loss: 0.5043, Train Acc: 0.7544 | \nEpoch [7/200] | Train Loss: 0.5313, Train Acc: 0.7836 | \nEpoch [8/200] | Train Loss: 0.4705, Train Acc: 0.7836 | \nEpoch [9/200] | Train Loss: 0.4685, Train Acc: 0.7895 | \nEpoch [10/200] | Train Loss: 0.4044, Train Acc: 0.8187 | \nEpoch [11/200] | Train Loss: 0.3891, Train Acc: 0.8246 | \nEpoch [12/200] | Train Loss: 0.4897, Train Acc: 0.7778 | \nEpoch [13/200] | Train Loss: 0.4475, Train Acc: 0.8012 | \nEpoch [14/200] | Train Loss: 0.3869, Train Acc: 0.8246 | \nEpoch [15/200] | Train Loss: 0.3978, Train Acc: 0.8363 | \nEpoch [16/200] | Train Loss: 0.3633, Train Acc: 0.8480 | \nEpoch [17/200] | Train Loss: 0.3442, Train Acc: 0.8538 | \nEpoch [18/200] | Train Loss: 0.3443, Train Acc: 0.8655 | \nEpoch [19/200] | Train Loss: 0.3425, Train Acc: 0.8713 | \nEpoch [20/200] | Train Loss: 0.3367, Train Acc: 0.8772 | \nEpoch [21/200] | Train Loss: 0.3404, Train Acc: 0.8480 | \nEpoch [22/200] | Train Loss: 0.3289, Train Acc: 0.8596 | \nEpoch [23/200] | Train Loss: 0.3014, Train Acc: 0.8889 | \nEpoch [24/200] | Train Loss: 0.3124, Train Acc: 0.8830 | \nEpoch [25/200] | Train Loss: 0.2765, Train Acc: 0.9006 | \nEpoch [26/200] | Train Loss: 0.2668, Train Acc: 0.9123 | \nEpoch [27/200] | Train Loss: 0.2426, Train Acc: 0.8947 | \nEpoch [28/200] | Train Loss: 0.2568, Train Acc: 0.9123 | \nEpoch [29/200] | Train Loss: 0.2629, Train Acc: 0.8947 | \nEpoch [30/200] | Train Loss: 0.2748, Train Acc: 0.9006 | \nEpoch [31/200] | Train Loss: 0.3071, Train Acc: 0.8596 | \nEpoch [32/200] | Train Loss: 0.2823, Train Acc: 0.8713 | \nEpoch [33/200] | Train Loss: 0.2670, Train Acc: 0.9181 | \nEpoch [34/200] | Train Loss: 0.2707, Train Acc: 0.9064 | \nEpoch [35/200] | Train Loss: 0.2447, Train Acc: 0.9181 | \nEpoch [36/200] | Train Loss: 0.2422, Train Acc: 0.9006 | \nEpoch [37/200] | Train Loss: 0.2249, Train Acc: 0.9298 | \nEpoch [38/200] | Train Loss: 0.2390, Train Acc: 0.9123 | \nEpoch [39/200] | Train Loss: 0.2214, Train Acc: 0.9181 | \nEpoch [40/200] | Train Loss: 0.2929, Train Acc: 0.9181 | \nEpoch [41/200] | Train Loss: 0.3193, Train Acc: 0.9064 | \nEpoch [42/200] | Train Loss: 0.2833, Train Acc: 0.8830 | \nEpoch [43/200] | Train Loss: 0.2584, Train Acc: 0.9181 | \nEpoch [44/200] | Train Loss: 0.2507, Train Acc: 0.9181 | \nEpoch [45/200] | Train Loss: 0.2556, Train Acc: 0.9064 | \nEpoch [46/200] | Train Loss: 0.2694, Train Acc: 0.9006 | \nEpoch [47/200] | Train Loss: 0.2378, Train Acc: 0.9181 | \nEpoch [48/200] | Train Loss: 0.2523, Train Acc: 0.9006 | \nEpoch [49/200] | Train Loss: 0.2502, Train Acc: 0.8947 | \nEpoch 00050: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [50/200] | Train Loss: 0.2544, Train Acc: 0.9064 | \nEpoch [51/200] | Train Loss: 0.2635, Train Acc: 0.8889 | \nEpoch [52/200] | Train Loss: 0.2392, Train Acc: 0.9064 | \nEpoch [53/200] | Train Loss: 0.2379, Train Acc: 0.9064 | \nEpoch [54/200] | Train Loss: 0.2436, Train Acc: 0.8947 | \nEpoch [55/200] | Train Loss: 0.2366, Train Acc: 0.9064 | \nEpoch [56/200] | Train Loss: 0.2243, Train Acc: 0.9181 | \nEarly stopping triggered.\nTest Loss: 0.6435, Test Accuracy: 0.7674, Test AUC: 0.8528\n\n--- Processing: fc_delta ---\nShape: (214, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7484, Train Acc: 0.5380 | \nEpoch [2/200] | Train Loss: 0.6985, Train Acc: 0.5205 | \nEpoch [3/200] | Train Loss: 0.6920, Train Acc: 0.5322 | \nEpoch [4/200] | Train Loss: 0.6999, Train Acc: 0.4912 | \nEpoch [5/200] | Train Loss: 0.6769, Train Acc: 0.5556 | \nEpoch [6/200] | Train Loss: 0.6541, Train Acc: 0.5965 | \nEpoch [7/200] | Train Loss: 0.4753, Train Acc: 0.7953 | \nEpoch [8/200] | Train Loss: 0.5180, Train Acc: 0.7953 | \nEpoch [9/200] | Train Loss: 0.4758, Train Acc: 0.8012 | \nEpoch [10/200] | Train Loss: 0.3951, Train Acc: 0.8538 | \nEpoch [11/200] | Train Loss: 0.4569, Train Acc: 0.7719 | \nEpoch [12/200] | Train Loss: 0.4984, Train Acc: 0.7778 | \nEpoch [13/200] | Train Loss: 0.3225, Train Acc: 0.8713 | \nEpoch [14/200] | Train Loss: 0.3055, Train Acc: 0.8655 | \nEpoch [15/200] | Train Loss: 0.2783, Train Acc: 0.8889 | \nEpoch [16/200] | Train Loss: 0.3423, Train Acc: 0.8713 | \nEpoch [17/200] | Train Loss: 0.3452, Train Acc: 0.8363 | \nEpoch [18/200] | Train Loss: 0.3597, Train Acc: 0.8480 | \nEpoch [19/200] | Train Loss: 0.2961, Train Acc: 0.9123 | \nEpoch [20/200] | Train Loss: 0.2728, Train Acc: 0.8889 | \nEpoch [21/200] | Train Loss: 0.3252, Train Acc: 0.8480 | \nEpoch [22/200] | Train Loss: 0.2822, Train Acc: 0.8889 | \nEpoch [23/200] | Train Loss: 0.2476, Train Acc: 0.8947 | \nEpoch [24/200] | Train Loss: 0.2049, Train Acc: 0.9181 | \nEpoch [25/200] | Train Loss: 0.1981, Train Acc: 0.9240 | \nEpoch [26/200] | Train Loss: 0.1929, Train Acc: 0.9181 | \nEpoch [27/200] | Train Loss: 0.3163, Train Acc: 0.8596 | \nEpoch [28/200] | Train Loss: 0.2852, Train Acc: 0.8772 | \nEpoch [29/200] | Train Loss: 0.2977, Train Acc: 0.8772 | \nEpoch [30/200] | Train Loss: 0.2131, Train Acc: 0.9123 | \nEpoch [31/200] | Train Loss: 0.2507, Train Acc: 0.9123 | \nEpoch [32/200] | Train Loss: 0.2170, Train Acc: 0.8947 | \nEpoch [33/200] | Train Loss: 0.1932, Train Acc: 0.9181 | \nEpoch [34/200] | Train Loss: 0.1944, Train Acc: 0.9006 | \nEpoch [35/200] | Train Loss: 0.2546, Train Acc: 0.8889 | \nEpoch [36/200] | Train Loss: 0.2264, Train Acc: 0.8889 | \nEpoch [37/200] | Train Loss: 0.1702, Train Acc: 0.9357 | \nEpoch [38/200] | Train Loss: 0.1443, Train Acc: 0.9415 | \nEpoch [39/200] | Train Loss: 0.1676, Train Acc: 0.9298 | \nEpoch [40/200] | Train Loss: 0.1302, Train Acc: 0.9532 | \nEpoch [41/200] | Train Loss: 0.1607, Train Acc: 0.9298 | \nEpoch [42/200] | Train Loss: 0.1735, Train Acc: 0.9357 | \nEpoch [43/200] | Train Loss: 0.1768, Train Acc: 0.9415 | \nEpoch [44/200] | Train Loss: 0.1451, Train Acc: 0.9415 | \nEpoch [45/200] | Train Loss: 0.1672, Train Acc: 0.9240 | \nEpoch [46/200] | Train Loss: 0.1486, Train Acc: 0.9474 | \nEpoch [47/200] | Train Loss: 0.1106, Train Acc: 0.9708 | \nEpoch [48/200] | Train Loss: 0.0990, Train Acc: 0.9591 | \nEpoch [49/200] | Train Loss: 0.1239, Train Acc: 0.9649 | \nEpoch [50/200] | Train Loss: 0.1470, Train Acc: 0.9532 | \nEpoch [51/200] | Train Loss: 0.1894, Train Acc: 0.9240 | \nEpoch [52/200] | Train Loss: 0.3925, Train Acc: 0.8187 | \nEpoch [53/200] | Train Loss: 0.2586, Train Acc: 0.9064 | \nEpoch [54/200] | Train Loss: 0.2133, Train Acc: 0.9006 | \nEpoch [55/200] | Train Loss: 0.1462, Train Acc: 0.9532 | \nEpoch [56/200] | Train Loss: 0.1269, Train Acc: 0.9591 | \nEpoch [57/200] | Train Loss: 0.1551, Train Acc: 0.9240 | \nEpoch [58/200] | Train Loss: 0.1154, Train Acc: 0.9532 | \nEpoch 00059: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [59/200] | Train Loss: 0.1441, Train Acc: 0.9532 | \nEpoch [60/200] | Train Loss: 0.1537, Train Acc: 0.9357 | \nEpoch [61/200] | Train Loss: 0.1171, Train Acc: 0.9708 | \nEpoch [62/200] | Train Loss: 0.0842, Train Acc: 0.9649 | \nEpoch [63/200] | Train Loss: 0.0996, Train Acc: 0.9766 | \nEpoch [64/200] | Train Loss: 0.1035, Train Acc: 0.9649 | \nEpoch [65/200] | Train Loss: 0.0967, Train Acc: 0.9708 | \nEpoch [66/200] | Train Loss: 0.0921, Train Acc: 0.9649 | \nEpoch [67/200] | Train Loss: 0.0648, Train Acc: 0.9883 | \nEpoch [68/200] | Train Loss: 0.0823, Train Acc: 0.9591 | \nEpoch [69/200] | Train Loss: 0.0597, Train Acc: 0.9708 | \nEpoch [70/200] | Train Loss: 0.0634, Train Acc: 0.9766 | \nEpoch [71/200] | Train Loss: 0.0626, Train Acc: 0.9825 | \nEpoch [72/200] | Train Loss: 0.0570, Train Acc: 0.9766 | \nEpoch [73/200] | Train Loss: 0.0461, Train Acc: 0.9825 | \nEpoch [74/200] | Train Loss: 0.0731, Train Acc: 0.9708 | \nEpoch [75/200] | Train Loss: 0.0490, Train Acc: 0.9825 | \nEpoch [76/200] | Train Loss: 0.0595, Train Acc: 0.9766 | \nEpoch [77/200] | Train Loss: 0.0639, Train Acc: 0.9766 | \nEpoch [78/200] | Train Loss: 0.0483, Train Acc: 0.9766 | \nEpoch [79/200] | Train Loss: 0.0552, Train Acc: 0.9708 | \nEpoch [80/200] | Train Loss: 0.0419, Train Acc: 0.9825 | \nEpoch [81/200] | Train Loss: 0.0491, Train Acc: 0.9766 | \nEpoch [82/200] | Train Loss: 0.0322, Train Acc: 0.9942 | \nEpoch [83/200] | Train Loss: 0.0284, Train Acc: 0.9942 | \nEpoch [84/200] | Train Loss: 0.0406, Train Acc: 0.9883 | \nEpoch [85/200] | Train Loss: 0.0304, Train Acc: 0.9942 | \nEpoch [86/200] | Train Loss: 0.0379, Train Acc: 0.9883 | \nEpoch [87/200] | Train Loss: 0.0377, Train Acc: 0.9883 | \nEpoch [88/200] | Train Loss: 0.0557, Train Acc: 0.9883 | \nEpoch [89/200] | Train Loss: 0.0297, Train Acc: 0.9942 | \nEpoch [90/200] | Train Loss: 0.0245, Train Acc: 0.9883 | \nEpoch [91/200] | Train Loss: 0.0325, Train Acc: 0.9883 | \nEpoch [92/200] | Train Loss: 0.0203, Train Acc: 0.9942 | \nEpoch [93/200] | Train Loss: 0.0260, Train Acc: 0.9883 | \nEpoch [94/200] | Train Loss: 0.0264, Train Acc: 1.0000 | \nEpoch [95/200] | Train Loss: 0.0527, Train Acc: 0.9825 | \nEpoch [96/200] | Train Loss: 0.0200, Train Acc: 0.9942 | \nEpoch [97/200] | Train Loss: 0.0236, Train Acc: 0.9942 | \nEpoch [98/200] | Train Loss: 0.0175, Train Acc: 1.0000 | \nEpoch [99/200] | Train Loss: 0.0301, Train Acc: 0.9883 | \nEpoch [100/200] | Train Loss: 0.0179, Train Acc: 1.0000 | \nEpoch [101/200] | Train Loss: 0.0120, Train Acc: 1.0000 | \nEpoch [102/200] | Train Loss: 0.0140, Train Acc: 0.9942 | \nEpoch [103/200] | Train Loss: 0.0121, Train Acc: 1.0000 | \nEpoch [104/200] | Train Loss: 0.0138, Train Acc: 1.0000 | \nEpoch [105/200] | Train Loss: 0.0157, Train Acc: 0.9942 | \nEpoch [106/200] | Train Loss: 0.0461, Train Acc: 0.9825 | \nEpoch [107/200] | Train Loss: 0.0233, Train Acc: 0.9883 | \nEpoch [108/200] | Train Loss: 0.0225, Train Acc: 0.9883 | \nEpoch [109/200] | Train Loss: 0.0108, Train Acc: 1.0000 | \nEpoch [110/200] | Train Loss: 0.0106, Train Acc: 1.0000 | \nEpoch [111/200] | Train Loss: 0.0129, Train Acc: 1.0000 | \nEpoch [112/200] | Train Loss: 0.0072, Train Acc: 1.0000 | \nEpoch [113/200] | Train Loss: 0.0076, Train Acc: 1.0000 | \nEarly stopping triggered.\nTest Loss: 1.0283, Test Accuracy: 0.7907, Test AUC: 0.8853\n\n--- Processing: fc_theta ---\nShape: (214, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7206, Train Acc: 0.5322 | \nEpoch [2/200] | Train Loss: 0.6995, Train Acc: 0.5322 | \nEpoch [3/200] | Train Loss: 0.6934, Train Acc: 0.4854 | \nEpoch [4/200] | Train Loss: 0.6760, Train Acc: 0.5789 | \nEpoch [5/200] | Train Loss: 0.6615, Train Acc: 0.5673 | \nEpoch [6/200] | Train Loss: 0.5346, Train Acc: 0.7602 | \nEpoch [7/200] | Train Loss: 0.5000, Train Acc: 0.7661 | \nEpoch [8/200] | Train Loss: 0.4510, Train Acc: 0.8246 | \nEpoch [9/200] | Train Loss: 0.4706, Train Acc: 0.7485 | \nEpoch [10/200] | Train Loss: 0.3662, Train Acc: 0.8596 | \nEpoch [11/200] | Train Loss: 0.3140, Train Acc: 0.8830 | \nEpoch [12/200] | Train Loss: 0.2913, Train Acc: 0.8772 | \nEpoch [13/200] | Train Loss: 0.2850, Train Acc: 0.8713 | \nEpoch [14/200] | Train Loss: 0.2700, Train Acc: 0.8772 | \nEpoch [15/200] | Train Loss: 0.2447, Train Acc: 0.9006 | \nEpoch [16/200] | Train Loss: 0.1990, Train Acc: 0.9415 | \nEpoch [17/200] | Train Loss: 0.2706, Train Acc: 0.8889 | \nEpoch [18/200] | Train Loss: 0.2154, Train Acc: 0.9123 | \nEpoch [19/200] | Train Loss: 0.2575, Train Acc: 0.8772 | \nEpoch [20/200] | Train Loss: 0.1809, Train Acc: 0.9357 | \nEpoch [21/200] | Train Loss: 0.1619, Train Acc: 0.9415 | \nEpoch [22/200] | Train Loss: 0.2442, Train Acc: 0.9064 | \nEpoch [23/200] | Train Loss: 0.5085, Train Acc: 0.7544 | \nEpoch [24/200] | Train Loss: 0.2310, Train Acc: 0.9123 | \nEpoch [25/200] | Train Loss: 0.2521, Train Acc: 0.9006 | \nEpoch [26/200] | Train Loss: 0.2024, Train Acc: 0.9240 | \nEpoch [27/200] | Train Loss: 0.2500, Train Acc: 0.8947 | \nEpoch [28/200] | Train Loss: 0.1875, Train Acc: 0.9298 | \nEpoch [29/200] | Train Loss: 0.1654, Train Acc: 0.9474 | \nEpoch [30/200] | Train Loss: 0.2086, Train Acc: 0.9123 | \nEpoch [31/200] | Train Loss: 0.1305, Train Acc: 0.9591 | \nEpoch [32/200] | Train Loss: 0.1154, Train Acc: 0.9591 | \nEpoch [33/200] | Train Loss: 0.1126, Train Acc: 0.9649 | \nEpoch [34/200] | Train Loss: 0.1332, Train Acc: 0.9649 | \nEpoch [35/200] | Train Loss: 0.0850, Train Acc: 0.9766 | \nEpoch [36/200] | Train Loss: 0.0849, Train Acc: 0.9708 | \nEpoch [37/200] | Train Loss: 0.0934, Train Acc: 0.9649 | \nEpoch [38/200] | Train Loss: 0.1175, Train Acc: 0.9474 | \nEpoch [39/200] | Train Loss: 0.0949, Train Acc: 0.9708 | \nEpoch [40/200] | Train Loss: 0.0728, Train Acc: 0.9825 | \nEpoch [41/200] | Train Loss: 0.0301, Train Acc: 0.9883 | \nEpoch [42/200] | Train Loss: 0.0452, Train Acc: 0.9766 | \nEpoch [43/200] | Train Loss: 0.5466, Train Acc: 0.8596 | \nEpoch [44/200] | Train Loss: 0.6102, Train Acc: 0.7485 | \nEpoch [45/200] | Train Loss: 0.3468, Train Acc: 0.8596 | \nEpoch [46/200] | Train Loss: 0.3229, Train Acc: 0.8713 | \nEpoch [47/200] | Train Loss: 0.2458, Train Acc: 0.9064 | \nEpoch [48/200] | Train Loss: 0.2696, Train Acc: 0.9006 | \nEpoch [49/200] | Train Loss: 0.3003, Train Acc: 0.8772 | \nEpoch [50/200] | Train Loss: 0.2225, Train Acc: 0.9240 | \nEpoch [51/200] | Train Loss: 0.2138, Train Acc: 0.9064 | \nEpoch 00052: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [52/200] | Train Loss: 0.1825, Train Acc: 0.9415 | \nEpoch [53/200] | Train Loss: 0.1717, Train Acc: 0.9591 | \nEpoch [54/200] | Train Loss: 0.1888, Train Acc: 0.9474 | \nEpoch [55/200] | Train Loss: 0.1545, Train Acc: 0.9474 | \nEpoch [56/200] | Train Loss: 0.1632, Train Acc: 0.9532 | \nEpoch [57/200] | Train Loss: 0.1580, Train Acc: 0.9591 | \nEpoch [58/200] | Train Loss: 0.1378, Train Acc: 0.9591 | \nEpoch [59/200] | Train Loss: 0.1461, Train Acc: 0.9649 | \nEpoch [60/200] | Train Loss: 0.1438, Train Acc: 0.9649 | \nEarly stopping triggered.\nTest Loss: 0.5797, Test Accuracy: 0.7907, Test AUC: 0.8766\n\n--- Processing: fc_alpha ---\nShape: (214, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7578, Train Acc: 0.5263 | \nEpoch [2/200] | Train Loss: 0.6904, Train Acc: 0.5029 | \nEpoch [3/200] | Train Loss: 0.6973, Train Acc: 0.5029 | \nEpoch [4/200] | Train Loss: 0.6813, Train Acc: 0.5322 | \nEpoch [5/200] | Train Loss: 0.6401, Train Acc: 0.6901 | \nEpoch [6/200] | Train Loss: 0.5370, Train Acc: 0.7076 | \nEpoch [7/200] | Train Loss: 0.5920, Train Acc: 0.6725 | \nEpoch [8/200] | Train Loss: 0.5196, Train Acc: 0.7661 | \nEpoch [9/200] | Train Loss: 0.4345, Train Acc: 0.7953 | \nEpoch [10/200] | Train Loss: 0.3637, Train Acc: 0.8480 | \nEpoch [11/200] | Train Loss: 0.3653, Train Acc: 0.8772 | \nEpoch [12/200] | Train Loss: 0.5697, Train Acc: 0.7251 | \nEpoch [13/200] | Train Loss: 0.4459, Train Acc: 0.8129 | \nEpoch [14/200] | Train Loss: 0.3885, Train Acc: 0.8246 | \nEpoch [15/200] | Train Loss: 0.3007, Train Acc: 0.8889 | \nEpoch [16/200] | Train Loss: 0.2994, Train Acc: 0.9064 | \nEpoch [17/200] | Train Loss: 0.2810, Train Acc: 0.8830 | \nEpoch [18/200] | Train Loss: 0.3161, Train Acc: 0.8830 | \nEpoch [19/200] | Train Loss: 0.3534, Train Acc: 0.8480 | \nEpoch [20/200] | Train Loss: 0.2959, Train Acc: 0.8947 | \nEpoch [21/200] | Train Loss: 0.2884, Train Acc: 0.8947 | \nEpoch [22/200] | Train Loss: 0.3298, Train Acc: 0.8596 | \nEpoch [23/200] | Train Loss: 0.3409, Train Acc: 0.8421 | \nEpoch [24/200] | Train Loss: 0.3076, Train Acc: 0.8713 | \nEpoch [25/200] | Train Loss: 0.2966, Train Acc: 0.8772 | \nEpoch [26/200] | Train Loss: 0.3308, Train Acc: 0.8538 | \nEpoch [27/200] | Train Loss: 0.2952, Train Acc: 0.8713 | \nEpoch 00028: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [28/200] | Train Loss: 0.2870, Train Acc: 0.8713 | \nEpoch [29/200] | Train Loss: 0.2464, Train Acc: 0.9064 | \nEpoch [30/200] | Train Loss: 0.2429, Train Acc: 0.9064 | \nEpoch [31/200] | Train Loss: 0.2187, Train Acc: 0.9123 | \nEpoch [32/200] | Train Loss: 0.2194, Train Acc: 0.9123 | \nEpoch [33/200] | Train Loss: 0.1894, Train Acc: 0.9240 | \nEpoch [34/200] | Train Loss: 0.1958, Train Acc: 0.9298 | \nEpoch [35/200] | Train Loss: 0.2004, Train Acc: 0.9298 | \nEpoch [36/200] | Train Loss: 0.1798, Train Acc: 0.9240 | \nEpoch [37/200] | Train Loss: 0.1784, Train Acc: 0.9240 | \nEpoch [38/200] | Train Loss: 0.1628, Train Acc: 0.9240 | \nEpoch [39/200] | Train Loss: 0.1503, Train Acc: 0.9474 | \nEpoch [40/200] | Train Loss: 0.1475, Train Acc: 0.9474 | \nEpoch [41/200] | Train Loss: 0.1497, Train Acc: 0.9240 | \nEpoch [42/200] | Train Loss: 0.1502, Train Acc: 0.9298 | \nEpoch [43/200] | Train Loss: 0.1432, Train Acc: 0.9474 | \nEpoch [44/200] | Train Loss: 0.1318, Train Acc: 0.9415 | \nEpoch [45/200] | Train Loss: 0.1367, Train Acc: 0.9415 | \nEpoch [46/200] | Train Loss: 0.1654, Train Acc: 0.9240 | \nEpoch [47/200] | Train Loss: 0.1327, Train Acc: 0.9532 | \nEpoch [48/200] | Train Loss: 0.1310, Train Acc: 0.9532 | \nEpoch [49/200] | Train Loss: 0.1108, Train Acc: 0.9591 | \nEpoch [50/200] | Train Loss: 0.1433, Train Acc: 0.9415 | \nEpoch [51/200] | Train Loss: 0.1239, Train Acc: 0.9415 | \nEpoch [52/200] | Train Loss: 0.1233, Train Acc: 0.9532 | \nEpoch [53/200] | Train Loss: 0.1071, Train Acc: 0.9591 | \nEpoch [54/200] | Train Loss: 0.1172, Train Acc: 0.9532 | \nEpoch [55/200] | Train Loss: 0.1137, Train Acc: 0.9532 | \nEpoch [56/200] | Train Loss: 0.0920, Train Acc: 0.9591 | \nEpoch [57/200] | Train Loss: 0.0872, Train Acc: 0.9649 | \nEpoch [58/200] | Train Loss: 0.1336, Train Acc: 0.9474 | \nEpoch [59/200] | Train Loss: 0.0974, Train Acc: 0.9532 | \nEpoch [60/200] | Train Loss: 0.1029, Train Acc: 0.9591 | \nEpoch [61/200] | Train Loss: 0.0834, Train Acc: 0.9649 | \nEpoch [62/200] | Train Loss: 0.0785, Train Acc: 0.9649 | \nEpoch [63/200] | Train Loss: 0.0620, Train Acc: 0.9766 | \nEpoch [64/200] | Train Loss: 0.0741, Train Acc: 0.9708 | \nEpoch [65/200] | Train Loss: 0.0807, Train Acc: 0.9708 | \nEpoch [66/200] | Train Loss: 0.0691, Train Acc: 0.9766 | \nEpoch [67/200] | Train Loss: 0.0748, Train Acc: 0.9708 | \nEpoch [68/200] | Train Loss: 0.0759, Train Acc: 0.9766 | \nEpoch [69/200] | Train Loss: 0.0694, Train Acc: 0.9825 | \nEpoch [70/200] | Train Loss: 0.0485, Train Acc: 0.9883 | \nEpoch [71/200] | Train Loss: 0.0616, Train Acc: 0.9883 | \nEpoch [72/200] | Train Loss: 0.0526, Train Acc: 0.9766 | \nEpoch [73/200] | Train Loss: 0.0516, Train Acc: 0.9825 | \nEpoch [74/200] | Train Loss: 0.0535, Train Acc: 0.9883 | \nEpoch [75/200] | Train Loss: 0.0263, Train Acc: 0.9942 | \nEpoch [76/200] | Train Loss: 0.0564, Train Acc: 0.9883 | \nEpoch [77/200] | Train Loss: 0.0614, Train Acc: 0.9883 | \nEpoch [78/200] | Train Loss: 0.1067, Train Acc: 0.9649 | \nEpoch [79/200] | Train Loss: 0.0655, Train Acc: 0.9766 | \nEpoch [80/200] | Train Loss: 0.0391, Train Acc: 0.9883 | \nEpoch [81/200] | Train Loss: 0.0513, Train Acc: 0.9883 | \nEpoch [82/200] | Train Loss: 0.0393, Train Acc: 0.9883 | \nEpoch [83/200] | Train Loss: 0.0795, Train Acc: 0.9766 | \nEpoch [84/200] | Train Loss: 0.0621, Train Acc: 0.9942 | \nEpoch [85/200] | Train Loss: 0.0556, Train Acc: 0.9825 | \nEpoch 00086: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [86/200] | Train Loss: 0.0537, Train Acc: 0.9766 | \nEpoch [87/200] | Train Loss: 0.0290, Train Acc: 0.9942 | \nEpoch [88/200] | Train Loss: 0.0308, Train Acc: 0.9883 | \nEpoch [89/200] | Train Loss: 0.0294, Train Acc: 1.0000 | \nEpoch [90/200] | Train Loss: 0.0204, Train Acc: 1.0000 | \nEpoch [91/200] | Train Loss: 0.0227, Train Acc: 0.9942 | \nEpoch [92/200] | Train Loss: 0.0238, Train Acc: 1.0000 | \nEpoch [93/200] | Train Loss: 0.0295, Train Acc: 0.9942 | \nEpoch [94/200] | Train Loss: 0.0283, Train Acc: 0.9942 | \nEpoch [95/200] | Train Loss: 0.0415, Train Acc: 0.9825 | \nEpoch [96/200] | Train Loss: 0.0274, Train Acc: 0.9942 | \nEpoch [97/200] | Train Loss: 0.0447, Train Acc: 0.9825 | \nEpoch [98/200] | Train Loss: 0.0274, Train Acc: 0.9883 | \nEpoch [99/200] | Train Loss: 0.0321, Train Acc: 0.9942 | \nEpoch [100/200] | Train Loss: 0.0326, Train Acc: 0.9825 | \nEpoch 00101: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [101/200] | Train Loss: 0.0221, Train Acc: 1.0000 | \nEpoch [102/200] | Train Loss: 0.0389, Train Acc: 0.9883 | \nEpoch [103/200] | Train Loss: 0.0424, Train Acc: 0.9942 | \nEpoch [104/200] | Train Loss: 0.0335, Train Acc: 0.9942 | \nEpoch [105/200] | Train Loss: 0.0337, Train Acc: 0.9942 | \nEpoch [106/200] | Train Loss: 0.0261, Train Acc: 0.9942 | \nEpoch [107/200] | Train Loss: 0.0204, Train Acc: 1.0000 | \nEpoch [108/200] | Train Loss: 0.0345, Train Acc: 0.9883 | \nEarly stopping triggered.\nTest Loss: 1.3701, Test Accuracy: 0.6744, Test AUC: 0.7359\n\n--- Processing: fc_beta ---\nShape: (214, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7402, Train Acc: 0.5205 | \nEpoch [2/200] | Train Loss: 0.7223, Train Acc: 0.5556 | \nEpoch [3/200] | Train Loss: 0.7985, Train Acc: 0.4971 | \nEpoch [4/200] | Train Loss: 0.7048, Train Acc: 0.4854 | \nEpoch [5/200] | Train Loss: 0.6738, Train Acc: 0.4971 | \nEpoch [6/200] | Train Loss: 0.6663, Train Acc: 0.6608 | \nEpoch [7/200] | Train Loss: 0.6113, Train Acc: 0.7368 | \nEpoch [8/200] | Train Loss: 0.5588, Train Acc: 0.7368 | \nEpoch [9/200] | Train Loss: 0.5452, Train Acc: 0.7076 | \nEpoch [10/200] | Train Loss: 0.5518, Train Acc: 0.7310 | \nEpoch [11/200] | Train Loss: 0.5935, Train Acc: 0.6608 | \nEpoch [12/200] | Train Loss: 0.5459, Train Acc: 0.7018 | \nEpoch [13/200] | Train Loss: 0.4754, Train Acc: 0.7719 | \nEpoch [14/200] | Train Loss: 0.4417, Train Acc: 0.8012 | \nEpoch [15/200] | Train Loss: 0.5558, Train Acc: 0.7310 | \nEpoch [16/200] | Train Loss: 0.4427, Train Acc: 0.8012 | \nEpoch [17/200] | Train Loss: 0.4526, Train Acc: 0.7895 | \nEpoch [18/200] | Train Loss: 0.3975, Train Acc: 0.8129 | \nEpoch [19/200] | Train Loss: 0.3324, Train Acc: 0.8830 | \nEpoch [20/200] | Train Loss: 0.3170, Train Acc: 0.8713 | \nEpoch [21/200] | Train Loss: 0.3649, Train Acc: 0.8363 | \nEpoch [22/200] | Train Loss: 0.3554, Train Acc: 0.8421 | \nEpoch [23/200] | Train Loss: 0.3596, Train Acc: 0.8187 | \nEpoch [24/200] | Train Loss: 0.4184, Train Acc: 0.8070 | \nEpoch [25/200] | Train Loss: 0.3878, Train Acc: 0.8304 | \nEpoch [26/200] | Train Loss: 0.3279, Train Acc: 0.8655 | \nEpoch [27/200] | Train Loss: 0.2766, Train Acc: 0.8947 | \nEpoch [28/200] | Train Loss: 0.3123, Train Acc: 0.8889 | \nEpoch [29/200] | Train Loss: 0.2487, Train Acc: 0.9123 | \nEpoch [30/200] | Train Loss: 0.3261, Train Acc: 0.8772 | \nEpoch [31/200] | Train Loss: 0.2657, Train Acc: 0.9006 | \nEpoch [32/200] | Train Loss: 0.2531, Train Acc: 0.8947 | \nEpoch [33/200] | Train Loss: 0.3222, Train Acc: 0.8889 | \nEpoch [34/200] | Train Loss: 0.3110, Train Acc: 0.8772 | \nEpoch [35/200] | Train Loss: 0.2927, Train Acc: 0.8655 | \nEpoch [36/200] | Train Loss: 0.2343, Train Acc: 0.9123 | \nEpoch [37/200] | Train Loss: 0.2470, Train Acc: 0.9064 | \nEpoch [38/200] | Train Loss: 0.2336, Train Acc: 0.9123 | \nEpoch [39/200] | Train Loss: 0.3080, Train Acc: 0.8655 | \nEpoch [40/200] | Train Loss: 0.2389, Train Acc: 0.9240 | \nEpoch [41/200] | Train Loss: 0.1957, Train Acc: 0.9123 | \nEpoch [42/200] | Train Loss: 0.1783, Train Acc: 0.9415 | \nEpoch [43/200] | Train Loss: 0.1474, Train Acc: 0.9474 | \nEpoch [44/200] | Train Loss: 0.1878, Train Acc: 0.9357 | \nEpoch [45/200] | Train Loss: 0.1815, Train Acc: 0.9181 | \nEpoch [46/200] | Train Loss: 0.1935, Train Acc: 0.8947 | \nEpoch [47/200] | Train Loss: 0.1770, Train Acc: 0.9240 | \nEpoch [48/200] | Train Loss: 0.1298, Train Acc: 0.9591 | \nEpoch [49/200] | Train Loss: 0.1118, Train Acc: 0.9532 | \nEpoch [50/200] | Train Loss: 0.1228, Train Acc: 0.9415 | \nEpoch [51/200] | Train Loss: 0.3298, Train Acc: 0.8947 | \nEpoch [52/200] | Train Loss: 0.2799, Train Acc: 0.8772 | \nEpoch [53/200] | Train Loss: 0.2825, Train Acc: 0.8889 | \nEpoch [54/200] | Train Loss: 0.2007, Train Acc: 0.9123 | \nEpoch [55/200] | Train Loss: 0.1561, Train Acc: 0.9474 | \nEpoch [56/200] | Train Loss: 0.1388, Train Acc: 0.9474 | \nEpoch [57/200] | Train Loss: 0.1411, Train Acc: 0.9357 | \nEpoch [58/200] | Train Loss: 0.1102, Train Acc: 0.9591 | \nEpoch [59/200] | Train Loss: 0.1190, Train Acc: 0.9649 | \nEpoch [60/200] | Train Loss: 0.0946, Train Acc: 0.9708 | \nEpoch [61/200] | Train Loss: 0.0837, Train Acc: 0.9708 | \nEpoch [62/200] | Train Loss: 0.0756, Train Acc: 0.9766 | \nEpoch [63/200] | Train Loss: 0.1995, Train Acc: 0.9240 | \nEpoch [64/200] | Train Loss: 0.2521, Train Acc: 0.8947 | \nEpoch [65/200] | Train Loss: 0.2759, Train Acc: 0.8655 | \nEpoch [66/200] | Train Loss: 0.3285, Train Acc: 0.8655 | \nEpoch [67/200] | Train Loss: 0.2086, Train Acc: 0.9006 | \nEpoch [68/200] | Train Loss: 0.1566, Train Acc: 0.9415 | \nEpoch [69/200] | Train Loss: 0.1465, Train Acc: 0.9415 | \nEpoch [70/200] | Train Loss: 0.0904, Train Acc: 0.9766 | \nEpoch [71/200] | Train Loss: 0.1144, Train Acc: 0.9649 | \nEpoch [72/200] | Train Loss: 0.1188, Train Acc: 0.9474 | \nEpoch 00073: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [73/200] | Train Loss: 0.1689, Train Acc: 0.9181 | \nEpoch [74/200] | Train Loss: 0.2135, Train Acc: 0.9240 | \nEpoch [75/200] | Train Loss: 0.1193, Train Acc: 0.9649 | \nEpoch [76/200] | Train Loss: 0.0846, Train Acc: 0.9825 | \nEpoch [77/200] | Train Loss: 0.0730, Train Acc: 0.9883 | \nEpoch [78/200] | Train Loss: 0.0826, Train Acc: 0.9708 | \nEpoch [79/200] | Train Loss: 0.0638, Train Acc: 0.9883 | \nEpoch [80/200] | Train Loss: 0.0626, Train Acc: 0.9883 | \nEpoch [81/200] | Train Loss: 0.0704, Train Acc: 0.9825 | \nEpoch [82/200] | Train Loss: 0.0645, Train Acc: 0.9883 | \nEpoch [83/200] | Train Loss: 0.0638, Train Acc: 0.9825 | \nEpoch [84/200] | Train Loss: 0.0575, Train Acc: 0.9883 | \nEpoch [85/200] | Train Loss: 0.0576, Train Acc: 0.9883 | \nEpoch [86/200] | Train Loss: 0.0513, Train Acc: 0.9883 | \nEpoch [87/200] | Train Loss: 0.0493, Train Acc: 0.9883 | \nEpoch [88/200] | Train Loss: 0.0458, Train Acc: 0.9883 | \nEpoch [89/200] | Train Loss: 0.0513, Train Acc: 0.9883 | \nEpoch [90/200] | Train Loss: 0.0513, Train Acc: 0.9883 | \nEpoch [91/200] | Train Loss: 0.0545, Train Acc: 0.9883 | \nEpoch [92/200] | Train Loss: 0.0500, Train Acc: 0.9825 | \nEpoch [93/200] | Train Loss: 0.0441, Train Acc: 0.9883 | \nEpoch [94/200] | Train Loss: 0.0504, Train Acc: 0.9825 | \nEpoch [95/200] | Train Loss: 0.0428, Train Acc: 0.9883 | \nEpoch [96/200] | Train Loss: 0.0512, Train Acc: 0.9883 | \nEpoch [97/200] | Train Loss: 0.0480, Train Acc: 0.9942 | \nEpoch [98/200] | Train Loss: 0.0398, Train Acc: 0.9883 | \nEpoch [99/200] | Train Loss: 0.0415, Train Acc: 0.9883 | \nEpoch [100/200] | Train Loss: 0.0364, Train Acc: 0.9883 | \nEpoch [101/200] | Train Loss: 0.0424, Train Acc: 0.9883 | \nEpoch [102/200] | Train Loss: 0.0389, Train Acc: 0.9883 | \nEpoch [103/200] | Train Loss: 0.0369, Train Acc: 0.9883 | \nEpoch [104/200] | Train Loss: 0.0380, Train Acc: 0.9883 | \nEpoch [105/200] | Train Loss: 0.0419, Train Acc: 0.9825 | \nEpoch [106/200] | Train Loss: 0.0389, Train Acc: 0.9883 | \nEpoch [107/200] | Train Loss: 0.0442, Train Acc: 0.9883 | \nEpoch [108/200] | Train Loss: 0.0404, Train Acc: 0.9883 | \nEpoch [109/200] | Train Loss: 0.0339, Train Acc: 0.9883 | \nEpoch [110/200] | Train Loss: 0.0419, Train Acc: 0.9825 | \nEpoch [111/200] | Train Loss: 0.0316, Train Acc: 0.9942 | \nEpoch [112/200] | Train Loss: 0.0262, Train Acc: 0.9942 | \nEpoch [113/200] | Train Loss: 0.0364, Train Acc: 0.9883 | \nEpoch [114/200] | Train Loss: 0.0423, Train Acc: 0.9883 | \nEpoch [115/200] | Train Loss: 0.0273, Train Acc: 0.9942 | \nEpoch [116/200] | Train Loss: 0.0326, Train Acc: 0.9942 | \nEarly stopping triggered.\nTest Loss: 0.7822, Test Accuracy: 0.7907, Test AUC: 0.8658\n\n--- Processing: fc_highbeta ---\nShape: (214, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7587, Train Acc: 0.5029 | \nEpoch [2/200] | Train Loss: 0.7037, Train Acc: 0.4737 | \nEpoch [3/200] | Train Loss: 0.7001, Train Acc: 0.4912 | \nEpoch [4/200] | Train Loss: 0.6998, Train Acc: 0.4971 | \nEpoch [5/200] | Train Loss: 0.6860, Train Acc: 0.5029 | \nEpoch [6/200] | Train Loss: 0.6643, Train Acc: 0.6550 | \nEpoch [7/200] | Train Loss: 0.6060, Train Acc: 0.7368 | \nEpoch [8/200] | Train Loss: 0.5450, Train Acc: 0.7018 | \nEpoch [9/200] | Train Loss: 0.5096, Train Acc: 0.7602 | \nEpoch [10/200] | Train Loss: 0.6032, Train Acc: 0.7018 | \nEpoch [11/200] | Train Loss: 0.4891, Train Acc: 0.7895 | \nEpoch [12/200] | Train Loss: 0.4870, Train Acc: 0.7895 | \nEpoch [13/200] | Train Loss: 0.4475, Train Acc: 0.8070 | \nEpoch [14/200] | Train Loss: 0.3929, Train Acc: 0.8246 | \nEpoch [15/200] | Train Loss: 0.4257, Train Acc: 0.8187 | \nEpoch [16/200] | Train Loss: 0.3985, Train Acc: 0.8363 | \nEpoch [17/200] | Train Loss: 0.3348, Train Acc: 0.8596 | \nEpoch [18/200] | Train Loss: 0.3323, Train Acc: 0.8772 | \nEpoch [19/200] | Train Loss: 0.3689, Train Acc: 0.8772 | \nEpoch [20/200] | Train Loss: 0.3132, Train Acc: 0.8830 | \nEpoch [21/200] | Train Loss: 0.3407, Train Acc: 0.8772 | \nEpoch [22/200] | Train Loss: 0.3387, Train Acc: 0.8596 | \nEpoch [23/200] | Train Loss: 0.3716, Train Acc: 0.8187 | \nEpoch [24/200] | Train Loss: 0.3020, Train Acc: 0.8713 | \nEpoch [25/200] | Train Loss: 0.2668, Train Acc: 0.8713 | \nEpoch [26/200] | Train Loss: 0.2317, Train Acc: 0.9181 | \nEpoch [27/200] | Train Loss: 0.1958, Train Acc: 0.9298 | \nEpoch [28/200] | Train Loss: 0.1619, Train Acc: 0.9474 | \nEpoch [29/200] | Train Loss: 0.2304, Train Acc: 0.9064 | \nEpoch [30/200] | Train Loss: 0.2218, Train Acc: 0.9181 | \nEpoch [31/200] | Train Loss: 0.3947, Train Acc: 0.8421 | \nEpoch [32/200] | Train Loss: 0.4420, Train Acc: 0.8363 | \nEpoch [33/200] | Train Loss: 0.4296, Train Acc: 0.8070 | \nEpoch [34/200] | Train Loss: 0.3805, Train Acc: 0.8596 | \nEpoch [35/200] | Train Loss: 0.3626, Train Acc: 0.8363 | \nEpoch [36/200] | Train Loss: 0.3358, Train Acc: 0.8655 | \nEpoch [37/200] | Train Loss: 0.2897, Train Acc: 0.8947 | \nEpoch [38/200] | Train Loss: 0.3047, Train Acc: 0.8538 | \nEpoch 00039: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [39/200] | Train Loss: 0.2855, Train Acc: 0.8713 | \nEpoch [40/200] | Train Loss: 0.2827, Train Acc: 0.8947 | \nEpoch [41/200] | Train Loss: 0.2356, Train Acc: 0.9240 | \nEpoch [42/200] | Train Loss: 0.2194, Train Acc: 0.9298 | \nEpoch [43/200] | Train Loss: 0.1929, Train Acc: 0.9181 | \nEpoch [44/200] | Train Loss: 0.1865, Train Acc: 0.9240 | \nEpoch [45/200] | Train Loss: 0.1895, Train Acc: 0.9298 | \nEpoch [46/200] | Train Loss: 0.1631, Train Acc: 0.9532 | \nEpoch [47/200] | Train Loss: 0.1655, Train Acc: 0.9474 | \nEpoch [48/200] | Train Loss: 0.1728, Train Acc: 0.9357 | \nEpoch [49/200] | Train Loss: 0.1601, Train Acc: 0.9415 | \nEpoch [50/200] | Train Loss: 0.1416, Train Acc: 0.9474 | \nEpoch [51/200] | Train Loss: 0.1226, Train Acc: 0.9532 | \nEpoch [52/200] | Train Loss: 0.1324, Train Acc: 0.9474 | \nEpoch [53/200] | Train Loss: 0.1143, Train Acc: 0.9708 | \nEpoch [54/200] | Train Loss: 0.1095, Train Acc: 0.9591 | \nEpoch [55/200] | Train Loss: 0.1034, Train Acc: 0.9766 | \nEpoch [56/200] | Train Loss: 0.1020, Train Acc: 0.9708 | \nEpoch [57/200] | Train Loss: 0.0865, Train Acc: 0.9825 | \nEpoch [58/200] | Train Loss: 0.0912, Train Acc: 0.9708 | \nEpoch [59/200] | Train Loss: 0.0750, Train Acc: 0.9825 | \nEpoch [60/200] | Train Loss: 0.0818, Train Acc: 0.9766 | \nEpoch [61/200] | Train Loss: 0.0745, Train Acc: 0.9766 | \nEpoch [62/200] | Train Loss: 0.0549, Train Acc: 0.9883 | \nEpoch [63/200] | Train Loss: 0.0610, Train Acc: 0.9825 | \nEpoch [64/200] | Train Loss: 0.0611, Train Acc: 0.9883 | \nEpoch [65/200] | Train Loss: 0.0797, Train Acc: 0.9825 | \nEpoch [66/200] | Train Loss: 0.0587, Train Acc: 0.9825 | \nEpoch [67/200] | Train Loss: 0.0422, Train Acc: 0.9883 | \nEpoch [68/200] | Train Loss: 0.0343, Train Acc: 0.9942 | \nEpoch [69/200] | Train Loss: 0.0592, Train Acc: 0.9825 | \nEpoch [70/200] | Train Loss: 0.0310, Train Acc: 0.9942 | \nEpoch [71/200] | Train Loss: 0.0264, Train Acc: 0.9942 | \nEpoch [72/200] | Train Loss: 0.0502, Train Acc: 0.9825 | \nEpoch [73/200] | Train Loss: 0.0458, Train Acc: 0.9825 | \nEpoch [74/200] | Train Loss: 0.0365, Train Acc: 0.9825 | \nEpoch [75/200] | Train Loss: 0.0375, Train Acc: 0.9883 | \nEpoch [76/200] | Train Loss: 0.0527, Train Acc: 0.9825 | \nEpoch [77/200] | Train Loss: 0.0451, Train Acc: 0.9883 | \nEpoch [78/200] | Train Loss: 0.0214, Train Acc: 1.0000 | \nEpoch [79/200] | Train Loss: 0.0278, Train Acc: 0.9883 | \nEpoch [80/200] | Train Loss: 0.0343, Train Acc: 0.9942 | \nEpoch [81/200] | Train Loss: 0.0330, Train Acc: 0.9942 | \nEpoch [82/200] | Train Loss: 0.0997, Train Acc: 0.9708 | \nEpoch [83/200] | Train Loss: 0.1668, Train Acc: 0.9474 | \nEpoch [84/200] | Train Loss: 0.0478, Train Acc: 0.9766 | \nEpoch [85/200] | Train Loss: 0.0936, Train Acc: 0.9708 | \nEpoch [86/200] | Train Loss: 0.0547, Train Acc: 0.9708 | \nEpoch [87/200] | Train Loss: 0.0295, Train Acc: 0.9883 | \nEpoch [88/200] | Train Loss: 0.0283, Train Acc: 0.9942 | \nEpoch 00089: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [89/200] | Train Loss: 0.0231, Train Acc: 1.0000 | \nEpoch [90/200] | Train Loss: 0.0300, Train Acc: 0.9883 | \nEpoch [91/200] | Train Loss: 0.0294, Train Acc: 0.9942 | \nEpoch [92/200] | Train Loss: 0.0333, Train Acc: 0.9883 | \nEpoch [93/200] | Train Loss: 0.0219, Train Acc: 0.9942 | \nEpoch [94/200] | Train Loss: 0.0256, Train Acc: 0.9942 | \nEpoch [95/200] | Train Loss: 0.0148, Train Acc: 1.0000 | \nEpoch [96/200] | Train Loss: 0.0189, Train Acc: 1.0000 | \nEpoch [97/200] | Train Loss: 0.0311, Train Acc: 0.9883 | \nEarly stopping triggered.\nTest Loss: 0.8720, Test Accuracy: 0.7907, Test AUC: 0.7857\n\n--- Processing: fc_gamma ---\nShape: (214, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.8623, Train Acc: 0.4386 | \nEpoch [2/200] | Train Loss: 0.7039, Train Acc: 0.4737 | \nEpoch [3/200] | Train Loss: 0.6972, Train Acc: 0.4678 | \nEpoch [4/200] | Train Loss: 0.6965, Train Acc: 0.4678 | \nEpoch [5/200] | Train Loss: 0.6921, Train Acc: 0.5029 | \nEpoch [6/200] | Train Loss: 0.6718, Train Acc: 0.6491 | \nEpoch [7/200] | Train Loss: 0.6285, Train Acc: 0.6784 | \nEpoch [8/200] | Train Loss: 0.5892, Train Acc: 0.6842 | \nEpoch [9/200] | Train Loss: 0.5525, Train Acc: 0.7193 | \nEpoch [10/200] | Train Loss: 0.4908, Train Acc: 0.7485 | \nEpoch [11/200] | Train Loss: 0.4970, Train Acc: 0.7661 | \nEpoch [12/200] | Train Loss: 0.5194, Train Acc: 0.7485 | \nEpoch [13/200] | Train Loss: 0.5006, Train Acc: 0.7485 | \nEpoch [14/200] | Train Loss: 0.4413, Train Acc: 0.8187 | \nEpoch [15/200] | Train Loss: 0.4766, Train Acc: 0.7836 | \nEpoch [16/200] | Train Loss: 0.3756, Train Acc: 0.8304 | \nEpoch [17/200] | Train Loss: 0.4240, Train Acc: 0.8070 | \nEpoch [18/200] | Train Loss: 0.4385, Train Acc: 0.8070 | \nEpoch [19/200] | Train Loss: 0.5297, Train Acc: 0.7895 | \nEpoch [20/200] | Train Loss: 0.4665, Train Acc: 0.7661 | \nEpoch [21/200] | Train Loss: 0.4288, Train Acc: 0.8070 | \nEpoch [22/200] | Train Loss: 0.3953, Train Acc: 0.8129 | \nEpoch [23/200] | Train Loss: 0.4202, Train Acc: 0.8129 | \nEpoch [24/200] | Train Loss: 0.4586, Train Acc: 0.7778 | \nEpoch [25/200] | Train Loss: 0.4794, Train Acc: 0.7135 | \nEpoch [26/200] | Train Loss: 0.4538, Train Acc: 0.7836 | \nEpoch 00027: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [27/200] | Train Loss: 0.3927, Train Acc: 0.8187 | \nEpoch [28/200] | Train Loss: 0.3547, Train Acc: 0.8363 | \nEpoch [29/200] | Train Loss: 0.3506, Train Acc: 0.8538 | \nEpoch [30/200] | Train Loss: 0.3319, Train Acc: 0.8655 | \nEpoch [31/200] | Train Loss: 0.3365, Train Acc: 0.8538 | \nEpoch [32/200] | Train Loss: 0.3346, Train Acc: 0.8772 | \nEpoch [33/200] | Train Loss: 0.3116, Train Acc: 0.8596 | \nEpoch [34/200] | Train Loss: 0.3102, Train Acc: 0.8655 | \nEpoch [35/200] | Train Loss: 0.3049, Train Acc: 0.8830 | \nEpoch [36/200] | Train Loss: 0.3159, Train Acc: 0.8655 | \nEpoch [37/200] | Train Loss: 0.2766, Train Acc: 0.8830 | \nEpoch [38/200] | Train Loss: 0.2867, Train Acc: 0.8772 | \nEpoch [39/200] | Train Loss: 0.2841, Train Acc: 0.8830 | \nEpoch [40/200] | Train Loss: 0.2825, Train Acc: 0.8947 | \nEpoch [41/200] | Train Loss: 0.2710, Train Acc: 0.9006 | \nEpoch [42/200] | Train Loss: 0.2713, Train Acc: 0.9006 | \nEpoch [43/200] | Train Loss: 0.2720, Train Acc: 0.8947 | \nEpoch [44/200] | Train Loss: 0.2588, Train Acc: 0.9181 | \nEpoch [45/200] | Train Loss: 0.2447, Train Acc: 0.9357 | \nEpoch [46/200] | Train Loss: 0.2468, Train Acc: 0.9123 | \nEpoch [47/200] | Train Loss: 0.2672, Train Acc: 0.9006 | \nEpoch [48/200] | Train Loss: 0.2537, Train Acc: 0.9181 | \nEpoch [49/200] | Train Loss: 0.2306, Train Acc: 0.9123 | \nEpoch [50/200] | Train Loss: 0.2218, Train Acc: 0.9298 | \nEpoch [51/200] | Train Loss: 0.2566, Train Acc: 0.8947 | \nEpoch [52/200] | Train Loss: 0.2372, Train Acc: 0.9006 | \nEpoch [53/200] | Train Loss: 0.2197, Train Acc: 0.9298 | \nEpoch [54/200] | Train Loss: 0.1969, Train Acc: 0.9415 | \nEpoch [55/200] | Train Loss: 0.2046, Train Acc: 0.9298 | \nEpoch [56/200] | Train Loss: 0.1975, Train Acc: 0.9357 | \nEpoch [57/200] | Train Loss: 0.1925, Train Acc: 0.9474 | \nEpoch [58/200] | Train Loss: 0.2069, Train Acc: 0.9357 | \nEpoch [59/200] | Train Loss: 0.1841, Train Acc: 0.9357 | \nEpoch [60/200] | Train Loss: 0.1765, Train Acc: 0.9298 | \nEpoch [61/200] | Train Loss: 0.1995, Train Acc: 0.9298 | \nEpoch [62/200] | Train Loss: 0.1912, Train Acc: 0.9357 | \nEpoch [63/200] | Train Loss: 0.1800, Train Acc: 0.9474 | \nEpoch [64/200] | Train Loss: 0.1750, Train Acc: 0.9415 | \nEpoch [65/200] | Train Loss: 0.1893, Train Acc: 0.9357 | \nEpoch [66/200] | Train Loss: 0.1831, Train Acc: 0.9415 | \nEpoch [67/200] | Train Loss: 0.1827, Train Acc: 0.9415 | \nEpoch [68/200] | Train Loss: 0.2104, Train Acc: 0.9240 | \nEpoch [69/200] | Train Loss: 0.1604, Train Acc: 0.9591 | \nEpoch [70/200] | Train Loss: 0.1789, Train Acc: 0.9298 | \nEpoch [71/200] | Train Loss: 0.1541, Train Acc: 0.9532 | \nEpoch [72/200] | Train Loss: 0.1477, Train Acc: 0.9532 | \nEpoch [73/200] | Train Loss: 0.1487, Train Acc: 0.9591 | \nEpoch [74/200] | Train Loss: 0.1454, Train Acc: 0.9591 | \nEpoch [75/200] | Train Loss: 0.1764, Train Acc: 0.9357 | \nEpoch [76/200] | Train Loss: 0.1776, Train Acc: 0.9474 | \nEpoch [77/200] | Train Loss: 0.1529, Train Acc: 0.9532 | \nEpoch [78/200] | Train Loss: 0.1621, Train Acc: 0.9415 | \nEpoch [79/200] | Train Loss: 0.1513, Train Acc: 0.9532 | \nEpoch [80/200] | Train Loss: 0.1330, Train Acc: 0.9591 | \nEpoch [81/200] | Train Loss: 0.1489, Train Acc: 0.9532 | \nEpoch [82/200] | Train Loss: 0.1383, Train Acc: 0.9532 | \nEpoch [83/200] | Train Loss: 0.1514, Train Acc: 0.9649 | \nEpoch [84/200] | Train Loss: 0.1529, Train Acc: 0.9474 | \nEpoch [85/200] | Train Loss: 0.1208, Train Acc: 0.9708 | \nEpoch [86/200] | Train Loss: 0.1433, Train Acc: 0.9649 | \nEpoch [87/200] | Train Loss: 0.1267, Train Acc: 0.9708 | \nEpoch [88/200] | Train Loss: 0.1174, Train Acc: 0.9708 | \nEpoch [89/200] | Train Loss: 0.1237, Train Acc: 0.9649 | \nEpoch [90/200] | Train Loss: 0.1391, Train Acc: 0.9649 | \nEpoch [91/200] | Train Loss: 0.1506, Train Acc: 0.9532 | \nEpoch [92/200] | Train Loss: 0.1292, Train Acc: 0.9591 | \nEpoch [93/200] | Train Loss: 0.1127, Train Acc: 0.9591 | \nEpoch [94/200] | Train Loss: 0.1278, Train Acc: 0.9532 | \nEpoch [95/200] | Train Loss: 0.1312, Train Acc: 0.9591 | \nEpoch [96/200] | Train Loss: 0.1501, Train Acc: 0.9532 | \nEpoch [97/200] | Train Loss: 0.1444, Train Acc: 0.9357 | \nEpoch [98/200] | Train Loss: 0.0961, Train Acc: 0.9766 | \nEpoch [99/200] | Train Loss: 0.1305, Train Acc: 0.9474 | \nEpoch [100/200] | Train Loss: 0.1414, Train Acc: 0.9415 | \nEpoch [101/200] | Train Loss: 0.1363, Train Acc: 0.9415 | \nEpoch [102/200] | Train Loss: 0.1093, Train Acc: 0.9708 | \nEpoch [103/200] | Train Loss: 0.1033, Train Acc: 0.9591 | \nEpoch [104/200] | Train Loss: 0.1335, Train Acc: 0.9474 | \nEpoch [105/200] | Train Loss: 0.1079, Train Acc: 0.9532 | \nEpoch [106/200] | Train Loss: 0.0827, Train Acc: 0.9708 | \nEpoch [107/200] | Train Loss: 0.0792, Train Acc: 0.9766 | \nEpoch [108/200] | Train Loss: 0.0924, Train Acc: 0.9766 | \nEpoch [109/200] | Train Loss: 0.0781, Train Acc: 0.9766 | \nEpoch [110/200] | Train Loss: 0.0934, Train Acc: 0.9649 | \nEpoch [111/200] | Train Loss: 0.0795, Train Acc: 0.9649 | \nEpoch [112/200] | Train Loss: 0.0837, Train Acc: 0.9825 | \nEpoch [113/200] | Train Loss: 0.1152, Train Acc: 0.9591 | \nEpoch [114/200] | Train Loss: 0.0869, Train Acc: 0.9708 | \nEpoch [115/200] | Train Loss: 0.0884, Train Acc: 0.9649 | \nEpoch [116/200] | Train Loss: 0.0783, Train Acc: 0.9766 | \nEpoch [117/200] | Train Loss: 0.1002, Train Acc: 0.9649 | \nEpoch [118/200] | Train Loss: 0.0728, Train Acc: 0.9766 | \nEpoch [119/200] | Train Loss: 0.0885, Train Acc: 0.9766 | \nEpoch [120/200] | Train Loss: 0.2063, Train Acc: 0.9415 | \nEpoch [121/200] | Train Loss: 0.1576, Train Acc: 0.9415 | \nEpoch [122/200] | Train Loss: 0.3031, Train Acc: 0.8830 | \nEpoch [123/200] | Train Loss: 0.1376, Train Acc: 0.9415 | \nEpoch [124/200] | Train Loss: 0.0875, Train Acc: 0.9591 | \nEpoch [125/200] | Train Loss: 0.0910, Train Acc: 0.9766 | \nEpoch [126/200] | Train Loss: 0.0989, Train Acc: 0.9591 | \nEpoch [127/200] | Train Loss: 0.0780, Train Acc: 0.9825 | \nEpoch [128/200] | Train Loss: 0.0515, Train Acc: 0.9942 | \nEpoch [129/200] | Train Loss: 0.0750, Train Acc: 0.9766 | \nEpoch [130/200] | Train Loss: 0.0550, Train Acc: 0.9883 | \nEpoch [131/200] | Train Loss: 0.0466, Train Acc: 0.9883 | \nEpoch [132/200] | Train Loss: 0.0846, Train Acc: 0.9766 | \nEpoch [133/200] | Train Loss: 0.0475, Train Acc: 0.9883 | \nEpoch [134/200] | Train Loss: 0.0984, Train Acc: 0.9649 | \nEpoch [135/200] | Train Loss: 0.0548, Train Acc: 0.9766 | \nEpoch [136/200] | Train Loss: 0.0395, Train Acc: 0.9942 | \nEpoch [137/200] | Train Loss: 0.0499, Train Acc: 0.9766 | \nEpoch [138/200] | Train Loss: 0.0488, Train Acc: 0.9883 | \nEpoch [139/200] | Train Loss: 0.0608, Train Acc: 0.9883 | \nEpoch [140/200] | Train Loss: 0.0761, Train Acc: 0.9708 | \nEpoch [141/200] | Train Loss: 0.0267, Train Acc: 1.0000 | \nEpoch [142/200] | Train Loss: 0.0449, Train Acc: 0.9883 | \nEpoch [143/200] | Train Loss: 0.0619, Train Acc: 0.9825 | \nEpoch [144/200] | Train Loss: 0.0384, Train Acc: 0.9942 | \nEpoch [145/200] | Train Loss: 0.0664, Train Acc: 0.9766 | \nEpoch [146/200] | Train Loss: 0.0427, Train Acc: 0.9825 | \nEpoch [147/200] | Train Loss: 0.0478, Train Acc: 0.9883 | \nEpoch [148/200] | Train Loss: 0.0368, Train Acc: 0.9942 | \nEpoch [149/200] | Train Loss: 0.0423, Train Acc: 0.9883 | \nEpoch [150/200] | Train Loss: 0.0423, Train Acc: 0.9942 | \nEpoch [151/200] | Train Loss: 0.0366, Train Acc: 0.9942 | \nEpoch 00152: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [152/200] | Train Loss: 0.0341, Train Acc: 0.9942 | \nEpoch [153/200] | Train Loss: 0.0329, Train Acc: 0.9883 | \nEpoch [154/200] | Train Loss: 0.0323, Train Acc: 0.9883 | \nEpoch [155/200] | Train Loss: 0.0586, Train Acc: 0.9825 | \nEpoch [156/200] | Train Loss: 0.0316, Train Acc: 0.9942 | \nEpoch [157/200] | Train Loss: 0.0292, Train Acc: 0.9942 | \nEpoch [158/200] | Train Loss: 0.0285, Train Acc: 0.9942 | \nEpoch [159/200] | Train Loss: 0.0332, Train Acc: 0.9942 | \nEpoch [160/200] | Train Loss: 0.0379, Train Acc: 0.9942 | \nEarly stopping triggered.\nTest Loss: 1.0940, Test Accuracy: 0.7674, Test AUC: 0.7900\n\n--- Processing: psd_fc_delta ---\nShape: (214, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7353, Train Acc: 0.5497 | \nEpoch [2/200] | Train Loss: 0.7162, Train Acc: 0.4678 | \nEpoch [3/200] | Train Loss: 0.6862, Train Acc: 0.5439 | \nEpoch [4/200] | Train Loss: 0.6866, Train Acc: 0.4912 | \nEpoch [5/200] | Train Loss: 0.6898, Train Acc: 0.4971 | \nEpoch [6/200] | Train Loss: 0.6502, Train Acc: 0.6199 | \nEpoch [7/200] | Train Loss: 0.5944, Train Acc: 0.7018 | \nEpoch [8/200] | Train Loss: 0.4789, Train Acc: 0.8012 | \nEpoch [9/200] | Train Loss: 0.4136, Train Acc: 0.8187 | \nEpoch [10/200] | Train Loss: 0.3857, Train Acc: 0.8129 | \nEpoch [11/200] | Train Loss: 0.3746, Train Acc: 0.8363 | \nEpoch [12/200] | Train Loss: 0.3224, Train Acc: 0.8596 | \nEpoch [13/200] | Train Loss: 0.3111, Train Acc: 0.8596 | \nEpoch [14/200] | Train Loss: 0.2887, Train Acc: 0.8713 | \nEpoch [15/200] | Train Loss: 0.2984, Train Acc: 0.8538 | \nEpoch [16/200] | Train Loss: 0.2738, Train Acc: 0.8830 | \nEpoch [17/200] | Train Loss: 0.2865, Train Acc: 0.8889 | \nEpoch [18/200] | Train Loss: 0.2671, Train Acc: 0.9006 | \nEpoch [19/200] | Train Loss: 0.2443, Train Acc: 0.9064 | \nEpoch [20/200] | Train Loss: 0.2039, Train Acc: 0.9064 | \nEpoch [21/200] | Train Loss: 0.2899, Train Acc: 0.9006 | \nEpoch [22/200] | Train Loss: 0.2806, Train Acc: 0.8772 | \nEpoch [23/200] | Train Loss: 0.1796, Train Acc: 0.9298 | \nEpoch [24/200] | Train Loss: 0.1574, Train Acc: 0.9532 | \nEpoch [25/200] | Train Loss: 0.1361, Train Acc: 0.9298 | \nEpoch [26/200] | Train Loss: 0.1715, Train Acc: 0.9474 | \nEpoch [27/200] | Train Loss: 0.1729, Train Acc: 0.9240 | \nEpoch [28/200] | Train Loss: 0.2192, Train Acc: 0.9006 | \nEpoch [29/200] | Train Loss: 0.1293, Train Acc: 0.9532 | \nEpoch [30/200] | Train Loss: 0.1304, Train Acc: 0.9474 | \nEpoch [31/200] | Train Loss: 0.1436, Train Acc: 0.9532 | \nEpoch [32/200] | Train Loss: 0.1754, Train Acc: 0.9240 | \nEpoch [33/200] | Train Loss: 0.2692, Train Acc: 0.8947 | \nEpoch [34/200] | Train Loss: 0.1909, Train Acc: 0.9298 | \nEpoch [35/200] | Train Loss: 0.1688, Train Acc: 0.9474 | \nEpoch [36/200] | Train Loss: 0.1538, Train Acc: 0.9415 | \nEpoch [37/200] | Train Loss: 0.0973, Train Acc: 0.9766 | \nEpoch [38/200] | Train Loss: 0.1114, Train Acc: 0.9532 | \nEpoch [39/200] | Train Loss: 0.0603, Train Acc: 0.9708 | \nEpoch [40/200] | Train Loss: 0.1506, Train Acc: 0.9474 | \nEpoch [41/200] | Train Loss: 0.1354, Train Acc: 0.9532 | \nEpoch [42/200] | Train Loss: 0.2594, Train Acc: 0.9064 | \nEpoch [43/200] | Train Loss: 0.2029, Train Acc: 0.9357 | \nEpoch [44/200] | Train Loss: 0.1214, Train Acc: 0.9591 | \nEpoch [45/200] | Train Loss: 0.1303, Train Acc: 0.9415 | \nEpoch [46/200] | Train Loss: 0.1430, Train Acc: 0.9474 | \nEpoch [47/200] | Train Loss: 0.1063, Train Acc: 0.9532 | \nEpoch [48/200] | Train Loss: 0.1152, Train Acc: 0.9649 | \nEpoch [49/200] | Train Loss: 0.1056, Train Acc: 0.9532 | \nEpoch 00050: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [50/200] | Train Loss: 0.1968, Train Acc: 0.9474 | \nEpoch [51/200] | Train Loss: 0.1092, Train Acc: 0.9708 | \nEpoch [52/200] | Train Loss: 0.1136, Train Acc: 0.9591 | \nEpoch [53/200] | Train Loss: 0.0802, Train Acc: 0.9766 | \nEpoch [54/200] | Train Loss: 0.0952, Train Acc: 0.9649 | \nEpoch [55/200] | Train Loss: 0.0845, Train Acc: 0.9708 | \nEpoch [56/200] | Train Loss: 0.0714, Train Acc: 0.9766 | \nEarly stopping triggered.\nTest Loss: 0.8824, Test Accuracy: 0.7442, Test AUC: 0.8095\n\n--- Processing: psd_fc_theta ---\nShape: (214, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7718, Train Acc: 0.4737 | \nEpoch [2/200] | Train Loss: 0.6895, Train Acc: 0.5614 | \nEpoch [3/200] | Train Loss: 0.6978, Train Acc: 0.5029 | \nEpoch [4/200] | Train Loss: 0.6917, Train Acc: 0.4971 | \nEpoch [5/200] | Train Loss: 0.6975, Train Acc: 0.4620 | \nEpoch [6/200] | Train Loss: 0.6837, Train Acc: 0.5848 | \nEpoch [7/200] | Train Loss: 0.6683, Train Acc: 0.6550 | \nEpoch [8/200] | Train Loss: 0.6002, Train Acc: 0.7368 | \nEpoch [9/200] | Train Loss: 0.5106, Train Acc: 0.7719 | \nEpoch [10/200] | Train Loss: 0.4929, Train Acc: 0.7953 | \nEpoch [11/200] | Train Loss: 0.4222, Train Acc: 0.8363 | \nEpoch [12/200] | Train Loss: 0.3887, Train Acc: 0.8655 | \nEpoch [13/200] | Train Loss: 0.3322, Train Acc: 0.8772 | \nEpoch [14/200] | Train Loss: 0.4107, Train Acc: 0.8655 | \nEpoch [15/200] | Train Loss: 0.3035, Train Acc: 0.8830 | \nEpoch [16/200] | Train Loss: 0.3212, Train Acc: 0.8772 | \nEpoch [17/200] | Train Loss: 0.2872, Train Acc: 0.8947 | \nEpoch [18/200] | Train Loss: 0.2605, Train Acc: 0.9064 | \nEpoch [19/200] | Train Loss: 0.2425, Train Acc: 0.9123 | \nEpoch [20/200] | Train Loss: 0.3383, Train Acc: 0.8596 | \nEpoch [21/200] | Train Loss: 0.3304, Train Acc: 0.8830 | \nEpoch [22/200] | Train Loss: 0.2513, Train Acc: 0.9240 | \nEpoch [23/200] | Train Loss: 0.2259, Train Acc: 0.9357 | \nEpoch [24/200] | Train Loss: 0.2341, Train Acc: 0.9006 | \nEpoch [25/200] | Train Loss: 0.2481, Train Acc: 0.9123 | \nEpoch [26/200] | Train Loss: 0.2519, Train Acc: 0.9181 | \nEpoch [27/200] | Train Loss: 0.2007, Train Acc: 0.9415 | \nEpoch [28/200] | Train Loss: 0.1966, Train Acc: 0.9298 | \nEpoch [29/200] | Train Loss: 0.1652, Train Acc: 0.9298 | \nEpoch [30/200] | Train Loss: 0.1728, Train Acc: 0.9240 | \nEpoch [31/200] | Train Loss: 0.1598, Train Acc: 0.9415 | \nEpoch [32/200] | Train Loss: 0.1483, Train Acc: 0.9532 | \nEpoch [33/200] | Train Loss: 0.1739, Train Acc: 0.9357 | \nEpoch [34/200] | Train Loss: 0.2039, Train Acc: 0.9474 | \nEpoch [35/200] | Train Loss: 0.2115, Train Acc: 0.9123 | \nEpoch [36/200] | Train Loss: 0.2970, Train Acc: 0.8772 | \nEpoch [37/200] | Train Loss: 0.2441, Train Acc: 0.9123 | \nEpoch [38/200] | Train Loss: 0.1840, Train Acc: 0.9357 | \nEpoch [39/200] | Train Loss: 0.1703, Train Acc: 0.9474 | \nEpoch [40/200] | Train Loss: 0.2158, Train Acc: 0.9123 | \nEpoch [41/200] | Train Loss: 0.1770, Train Acc: 0.9357 | \nEpoch [42/200] | Train Loss: 0.1775, Train Acc: 0.9181 | \nEpoch 00043: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [43/200] | Train Loss: 0.1798, Train Acc: 0.9298 | \nEpoch [44/200] | Train Loss: 0.1617, Train Acc: 0.9474 | \nEpoch [45/200] | Train Loss: 0.1477, Train Acc: 0.9532 | \nEpoch [46/200] | Train Loss: 0.1443, Train Acc: 0.9415 | \nEpoch [47/200] | Train Loss: 0.1369, Train Acc: 0.9532 | \nEpoch [48/200] | Train Loss: 0.1197, Train Acc: 0.9591 | \nEpoch [49/200] | Train Loss: 0.1123, Train Acc: 0.9649 | \nEpoch [50/200] | Train Loss: 0.1138, Train Acc: 0.9532 | \nEpoch [51/200] | Train Loss: 0.0967, Train Acc: 0.9708 | \nEpoch [52/200] | Train Loss: 0.1096, Train Acc: 0.9708 | \nEpoch [53/200] | Train Loss: 0.1169, Train Acc: 0.9532 | \nEpoch [54/200] | Train Loss: 0.1101, Train Acc: 0.9532 | \nEpoch [55/200] | Train Loss: 0.1115, Train Acc: 0.9591 | \nEpoch [56/200] | Train Loss: 0.0953, Train Acc: 0.9708 | \nEpoch [57/200] | Train Loss: 0.0968, Train Acc: 0.9708 | \nEpoch [58/200] | Train Loss: 0.0936, Train Acc: 0.9649 | \nEpoch [59/200] | Train Loss: 0.0992, Train Acc: 0.9649 | \nEpoch [60/200] | Train Loss: 0.0809, Train Acc: 0.9825 | \nEpoch [61/200] | Train Loss: 0.0963, Train Acc: 0.9649 | \nEpoch [62/200] | Train Loss: 0.0795, Train Acc: 0.9766 | \nEpoch [63/200] | Train Loss: 0.0791, Train Acc: 0.9825 | \nEpoch [64/200] | Train Loss: 0.0827, Train Acc: 0.9649 | \nEpoch [65/200] | Train Loss: 0.0779, Train Acc: 0.9766 | \nEpoch [66/200] | Train Loss: 0.0786, Train Acc: 0.9825 | \nEpoch [67/200] | Train Loss: 0.0711, Train Acc: 0.9883 | \nEpoch [68/200] | Train Loss: 0.0826, Train Acc: 0.9766 | \nEpoch [69/200] | Train Loss: 0.0746, Train Acc: 0.9766 | \nEpoch [70/200] | Train Loss: 0.0751, Train Acc: 0.9766 | \nEpoch [71/200] | Train Loss: 0.0755, Train Acc: 0.9766 | \nEpoch [72/200] | Train Loss: 0.0795, Train Acc: 0.9708 | \nEpoch [73/200] | Train Loss: 0.0783, Train Acc: 0.9766 | \nEpoch [74/200] | Train Loss: 0.0876, Train Acc: 0.9708 | \nEpoch [75/200] | Train Loss: 0.0796, Train Acc: 0.9766 | \nEpoch [76/200] | Train Loss: 0.0667, Train Acc: 0.9883 | \nEpoch [77/200] | Train Loss: 0.0743, Train Acc: 0.9708 | \nEpoch [78/200] | Train Loss: 0.0662, Train Acc: 0.9825 | \nEpoch [79/200] | Train Loss: 0.1028, Train Acc: 0.9766 | \nEpoch [80/200] | Train Loss: 0.0713, Train Acc: 0.9766 | \nEpoch [81/200] | Train Loss: 0.0655, Train Acc: 0.9825 | \nEpoch [82/200] | Train Loss: 0.0725, Train Acc: 0.9825 | \nEpoch [83/200] | Train Loss: 0.0739, Train Acc: 0.9825 | \nEpoch [84/200] | Train Loss: 0.0747, Train Acc: 0.9766 | \nEpoch [85/200] | Train Loss: 0.0678, Train Acc: 0.9766 | \nEpoch [86/200] | Train Loss: 0.0669, Train Acc: 0.9825 | \nEarly stopping triggered.\nTest Loss: 0.7937, Test Accuracy: 0.8372, Test AUC: 0.8874\n\n--- Processing: psd_fc_alpha ---\nShape: (214, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7068, Train Acc: 0.5029 | \nEpoch [2/200] | Train Loss: 0.7136, Train Acc: 0.5146 | \nEpoch [3/200] | Train Loss: 0.7092, Train Acc: 0.4737 | \nEpoch [4/200] | Train Loss: 0.7054, Train Acc: 0.4620 | \nEpoch [5/200] | Train Loss: 0.6664, Train Acc: 0.5789 | \nEpoch [6/200] | Train Loss: 0.6351, Train Acc: 0.6140 | \nEpoch [7/200] | Train Loss: 0.5449, Train Acc: 0.7310 | \nEpoch [8/200] | Train Loss: 0.6033, Train Acc: 0.7018 | \nEpoch [9/200] | Train Loss: 0.5002, Train Acc: 0.7602 | \nEpoch [10/200] | Train Loss: 0.4836, Train Acc: 0.7661 | \nEpoch [11/200] | Train Loss: 0.4204, Train Acc: 0.8421 | \nEpoch [12/200] | Train Loss: 0.4528, Train Acc: 0.7953 | \nEpoch [13/200] | Train Loss: 0.3818, Train Acc: 0.8129 | \nEpoch [14/200] | Train Loss: 0.3627, Train Acc: 0.8480 | \nEpoch [15/200] | Train Loss: 0.3698, Train Acc: 0.8655 | \nEpoch [16/200] | Train Loss: 0.3376, Train Acc: 0.8772 | \nEpoch [17/200] | Train Loss: 0.3582, Train Acc: 0.8538 | \nEpoch [18/200] | Train Loss: 0.2697, Train Acc: 0.9006 | \nEpoch [19/200] | Train Loss: 0.3306, Train Acc: 0.8538 | \nEpoch [20/200] | Train Loss: 0.3099, Train Acc: 0.8655 | \nEpoch [21/200] | Train Loss: 0.3028, Train Acc: 0.8830 | \nEpoch [22/200] | Train Loss: 0.2307, Train Acc: 0.9123 | \nEpoch [23/200] | Train Loss: 0.2630, Train Acc: 0.9006 | \nEpoch [24/200] | Train Loss: 0.2139, Train Acc: 0.9357 | \nEpoch [25/200] | Train Loss: 0.1962, Train Acc: 0.9181 | \nEpoch [26/200] | Train Loss: 0.1892, Train Acc: 0.9357 | \nEpoch [27/200] | Train Loss: 0.2016, Train Acc: 0.9181 | \nEpoch [28/200] | Train Loss: 0.2842, Train Acc: 0.8713 | \nEpoch [29/200] | Train Loss: 0.2032, Train Acc: 0.9357 | \nEpoch [30/200] | Train Loss: 0.2164, Train Acc: 0.9240 | \nEpoch [31/200] | Train Loss: 0.1346, Train Acc: 0.9474 | \nEpoch [32/200] | Train Loss: 0.1844, Train Acc: 0.9298 | \nEpoch [33/200] | Train Loss: 0.1998, Train Acc: 0.9298 | \nEpoch [34/200] | Train Loss: 0.1323, Train Acc: 0.9591 | \nEpoch [35/200] | Train Loss: 0.1163, Train Acc: 0.9649 | \nEpoch [36/200] | Train Loss: 0.1285, Train Acc: 0.9474 | \nEpoch [37/200] | Train Loss: 0.1552, Train Acc: 0.9474 | \nEpoch [38/200] | Train Loss: 0.2316, Train Acc: 0.9181 | \nEpoch [39/200] | Train Loss: 0.2133, Train Acc: 0.9181 | \nEpoch [40/200] | Train Loss: 0.2326, Train Acc: 0.9006 | \nEpoch [41/200] | Train Loss: 0.2161, Train Acc: 0.9123 | \nEpoch [42/200] | Train Loss: 0.2312, Train Acc: 0.9181 | \nEpoch [43/200] | Train Loss: 0.2272, Train Acc: 0.9123 | \nEpoch [44/200] | Train Loss: 0.1999, Train Acc: 0.9123 | \nEpoch [45/200] | Train Loss: 0.2060, Train Acc: 0.9064 | \nEpoch 00046: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [46/200] | Train Loss: 0.1912, Train Acc: 0.9298 | \nEpoch [47/200] | Train Loss: 0.1538, Train Acc: 0.9474 | \nEpoch [48/200] | Train Loss: 0.1469, Train Acc: 0.9474 | \nEpoch [49/200] | Train Loss: 0.1198, Train Acc: 0.9591 | \nEpoch [50/200] | Train Loss: 0.1069, Train Acc: 0.9532 | \nEpoch [51/200] | Train Loss: 0.1079, Train Acc: 0.9591 | \nEpoch [52/200] | Train Loss: 0.1155, Train Acc: 0.9532 | \nEpoch [53/200] | Train Loss: 0.0849, Train Acc: 0.9708 | \nEpoch [54/200] | Train Loss: 0.0867, Train Acc: 0.9649 | \nEpoch [55/200] | Train Loss: 0.0876, Train Acc: 0.9649 | \nEpoch [56/200] | Train Loss: 0.0800, Train Acc: 0.9649 | \nEpoch [57/200] | Train Loss: 0.0653, Train Acc: 0.9825 | \nEpoch [58/200] | Train Loss: 0.0669, Train Acc: 0.9825 | \nEpoch [59/200] | Train Loss: 0.0709, Train Acc: 0.9766 | \nEpoch [60/200] | Train Loss: 0.0607, Train Acc: 0.9766 | \nEpoch [61/200] | Train Loss: 0.0480, Train Acc: 0.9708 | \nEpoch [62/200] | Train Loss: 0.0532, Train Acc: 0.9825 | \nEpoch [63/200] | Train Loss: 0.0490, Train Acc: 0.9825 | \nEpoch [64/200] | Train Loss: 0.0559, Train Acc: 0.9825 | \nEpoch [65/200] | Train Loss: 0.0455, Train Acc: 0.9883 | \nEpoch [66/200] | Train Loss: 0.0446, Train Acc: 0.9825 | \nEpoch [67/200] | Train Loss: 0.0333, Train Acc: 0.9942 | \nEpoch [68/200] | Train Loss: 0.0279, Train Acc: 0.9942 | \nEpoch [69/200] | Train Loss: 0.0623, Train Acc: 0.9766 | \nEpoch [70/200] | Train Loss: 0.0291, Train Acc: 0.9942 | \nEpoch [71/200] | Train Loss: 0.0289, Train Acc: 0.9942 | \nEpoch [72/200] | Train Loss: 0.0262, Train Acc: 0.9883 | \nEpoch [73/200] | Train Loss: 0.0190, Train Acc: 1.0000 | \nEpoch [74/200] | Train Loss: 0.0175, Train Acc: 1.0000 | \nEpoch [75/200] | Train Loss: 0.0303, Train Acc: 0.9883 | \nEpoch [76/200] | Train Loss: 0.0183, Train Acc: 0.9942 | \nEpoch [77/200] | Train Loss: 0.0214, Train Acc: 0.9942 | \nEpoch [78/200] | Train Loss: 0.0203, Train Acc: 1.0000 | \nEpoch [79/200] | Train Loss: 0.0226, Train Acc: 0.9883 | \nEpoch [80/200] | Train Loss: 0.0140, Train Acc: 1.0000 | \nEpoch [81/200] | Train Loss: 0.0135, Train Acc: 1.0000 | \nEpoch [82/200] | Train Loss: 0.0128, Train Acc: 1.0000 | \nEpoch [83/200] | Train Loss: 0.0115, Train Acc: 1.0000 | \nEpoch [84/200] | Train Loss: 0.0158, Train Acc: 0.9942 | \nEpoch [85/200] | Train Loss: 0.0338, Train Acc: 0.9942 | \nEpoch [86/200] | Train Loss: 0.0142, Train Acc: 1.0000 | \nEpoch [87/200] | Train Loss: 0.0250, Train Acc: 0.9942 | \nEpoch [88/200] | Train Loss: 0.0153, Train Acc: 0.9942 | \nEpoch [89/200] | Train Loss: 0.0080, Train Acc: 1.0000 | \nEpoch [90/200] | Train Loss: 0.0264, Train Acc: 0.9942 | \nEpoch [91/200] | Train Loss: 0.0174, Train Acc: 0.9942 | \nEpoch [92/200] | Train Loss: 0.0094, Train Acc: 1.0000 | \nEarly stopping triggered.\nTest Loss: 1.2394, Test Accuracy: 0.7209, Test AUC: 0.8442\n\n--- Processing: psd_fc_beta ---\nShape: (214, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7882, Train Acc: 0.5029 | \nEpoch [2/200] | Train Loss: 0.7509, Train Acc: 0.4152 | \nEpoch [3/200] | Train Loss: 0.7112, Train Acc: 0.5029 | \nEpoch [4/200] | Train Loss: 0.6899, Train Acc: 0.5146 | \nEpoch [5/200] | Train Loss: 0.7145, Train Acc: 0.4971 | \nEpoch [6/200] | Train Loss: 0.6924, Train Acc: 0.5146 | \nEpoch [7/200] | Train Loss: 0.6893, Train Acc: 0.5029 | \nEpoch [8/200] | Train Loss: 0.6702, Train Acc: 0.6199 | \nEpoch [9/200] | Train Loss: 0.6208, Train Acc: 0.7076 | \nEpoch [10/200] | Train Loss: 0.5266, Train Acc: 0.7310 | \nEpoch [11/200] | Train Loss: 0.5808, Train Acc: 0.7193 | \nEpoch [12/200] | Train Loss: 0.4680, Train Acc: 0.7836 | \nEpoch [13/200] | Train Loss: 0.3900, Train Acc: 0.8421 | \nEpoch [14/200] | Train Loss: 0.3760, Train Acc: 0.8538 | \nEpoch [15/200] | Train Loss: 0.3734, Train Acc: 0.8480 | \nEpoch [16/200] | Train Loss: 0.3722, Train Acc: 0.8363 | \nEpoch [17/200] | Train Loss: 0.4078, Train Acc: 0.7895 | \nEpoch [18/200] | Train Loss: 0.4041, Train Acc: 0.8246 | \nEpoch [19/200] | Train Loss: 0.5302, Train Acc: 0.7427 | \nEpoch [20/200] | Train Loss: 0.3846, Train Acc: 0.8363 | \nEpoch [21/200] | Train Loss: 0.3005, Train Acc: 0.8889 | \nEpoch [22/200] | Train Loss: 0.3029, Train Acc: 0.8889 | \nEpoch [23/200] | Train Loss: 0.2716, Train Acc: 0.9064 | \nEpoch [24/200] | Train Loss: 0.2584, Train Acc: 0.9006 | \nEpoch [25/200] | Train Loss: 0.2222, Train Acc: 0.9240 | \nEpoch [26/200] | Train Loss: 0.2557, Train Acc: 0.9006 | \nEpoch [27/200] | Train Loss: 0.2907, Train Acc: 0.8830 | \nEpoch [28/200] | Train Loss: 0.2675, Train Acc: 0.9006 | \nEpoch [29/200] | Train Loss: 0.3399, Train Acc: 0.8772 | \nEpoch [30/200] | Train Loss: 0.2466, Train Acc: 0.9006 | \nEpoch [31/200] | Train Loss: 0.2231, Train Acc: 0.9123 | \nEpoch [32/200] | Train Loss: 0.1910, Train Acc: 0.9240 | \nEpoch [33/200] | Train Loss: 0.1794, Train Acc: 0.9181 | \nEpoch [34/200] | Train Loss: 0.1750, Train Acc: 0.9298 | \nEpoch [35/200] | Train Loss: 0.1264, Train Acc: 0.9415 | \nEpoch [36/200] | Train Loss: 0.1308, Train Acc: 0.9357 | \nEpoch [37/200] | Train Loss: 0.1904, Train Acc: 0.9415 | \nEpoch [38/200] | Train Loss: 0.1625, Train Acc: 0.9474 | \nEpoch [39/200] | Train Loss: 0.1472, Train Acc: 0.9415 | \nEpoch [40/200] | Train Loss: 0.1172, Train Acc: 0.9591 | \nEpoch [41/200] | Train Loss: 0.1194, Train Acc: 0.9591 | \nEpoch [42/200] | Train Loss: 0.1262, Train Acc: 0.9532 | \nEpoch [43/200] | Train Loss: 0.2393, Train Acc: 0.9006 | \nEpoch [44/200] | Train Loss: 0.1709, Train Acc: 0.9357 | \nEpoch [45/200] | Train Loss: 0.2741, Train Acc: 0.8538 | \nEpoch [46/200] | Train Loss: 0.2832, Train Acc: 0.8421 | \nEpoch [47/200] | Train Loss: 0.2448, Train Acc: 0.9006 | \nEpoch [48/200] | Train Loss: 0.1377, Train Acc: 0.9415 | \nEpoch [49/200] | Train Loss: 0.1136, Train Acc: 0.9474 | \nEpoch [50/200] | Train Loss: 0.0887, Train Acc: 0.9649 | \nEpoch [51/200] | Train Loss: 0.0684, Train Acc: 0.9766 | \nEpoch [52/200] | Train Loss: 0.0670, Train Acc: 0.9708 | \nEpoch [53/200] | Train Loss: 0.1159, Train Acc: 0.9532 | \nEpoch [54/200] | Train Loss: 0.0877, Train Acc: 0.9532 | \nEpoch [55/200] | Train Loss: 0.1333, Train Acc: 0.9532 | \nEpoch [56/200] | Train Loss: 0.1929, Train Acc: 0.9474 | \nEpoch [57/200] | Train Loss: 0.3543, Train Acc: 0.8713 | \nEpoch [58/200] | Train Loss: 0.3805, Train Acc: 0.8246 | \nEpoch [59/200] | Train Loss: 0.2588, Train Acc: 0.9064 | \nEpoch [60/200] | Train Loss: 0.1733, Train Acc: 0.9357 | \nEpoch [61/200] | Train Loss: 0.1358, Train Acc: 0.9591 | \nEpoch [62/200] | Train Loss: 0.1282, Train Acc: 0.9532 | \nEpoch 00063: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [63/200] | Train Loss: 0.1313, Train Acc: 0.9649 | \nEpoch [64/200] | Train Loss: 0.1477, Train Acc: 0.9298 | \nEpoch [65/200] | Train Loss: 0.1125, Train Acc: 0.9532 | \nEpoch [66/200] | Train Loss: 0.1093, Train Acc: 0.9649 | \nEpoch [67/200] | Train Loss: 0.0906, Train Acc: 0.9766 | \nEpoch [68/200] | Train Loss: 0.0704, Train Acc: 0.9825 | \nEpoch [69/200] | Train Loss: 0.0718, Train Acc: 0.9825 | \nEpoch [70/200] | Train Loss: 0.0773, Train Acc: 0.9649 | \nEpoch [71/200] | Train Loss: 0.0822, Train Acc: 0.9708 | \nEpoch [72/200] | Train Loss: 0.0710, Train Acc: 0.9825 | \nEpoch [73/200] | Train Loss: 0.0558, Train Acc: 0.9766 | \nEpoch [74/200] | Train Loss: 0.0610, Train Acc: 0.9883 | \nEpoch [75/200] | Train Loss: 0.0543, Train Acc: 0.9825 | \nEpoch [76/200] | Train Loss: 0.0703, Train Acc: 0.9708 | \nEpoch [77/200] | Train Loss: 0.0574, Train Acc: 0.9825 | \nEpoch [78/200] | Train Loss: 0.0569, Train Acc: 0.9766 | \nEpoch [79/200] | Train Loss: 0.0497, Train Acc: 0.9883 | \nEpoch [80/200] | Train Loss: 0.0587, Train Acc: 0.9825 | \nEpoch [81/200] | Train Loss: 0.0469, Train Acc: 0.9942 | \nEpoch [82/200] | Train Loss: 0.0399, Train Acc: 0.9883 | \nEpoch [83/200] | Train Loss: 0.0627, Train Acc: 0.9766 | \nEpoch [84/200] | Train Loss: 0.0533, Train Acc: 0.9766 | \nEpoch [85/200] | Train Loss: 0.0443, Train Acc: 0.9883 | \nEpoch [86/200] | Train Loss: 0.0405, Train Acc: 0.9883 | \nEpoch [87/200] | Train Loss: 0.0336, Train Acc: 0.9942 | \nEpoch [88/200] | Train Loss: 0.0387, Train Acc: 0.9942 | \nEpoch [89/200] | Train Loss: 0.0464, Train Acc: 0.9883 | \nEpoch [90/200] | Train Loss: 0.0459, Train Acc: 0.9825 | \nEpoch [91/200] | Train Loss: 0.0324, Train Acc: 0.9883 | \nEpoch [92/200] | Train Loss: 0.0396, Train Acc: 0.9883 | \nEpoch [93/200] | Train Loss: 0.0393, Train Acc: 0.9942 | \nEpoch [94/200] | Train Loss: 0.0392, Train Acc: 0.9942 | \nEpoch [95/200] | Train Loss: 0.0429, Train Acc: 0.9883 | \nEpoch [96/200] | Train Loss: 0.0372, Train Acc: 0.9825 | \nEpoch [97/200] | Train Loss: 0.0360, Train Acc: 0.9942 | \nEpoch [98/200] | Train Loss: 0.0422, Train Acc: 0.9825 | \nEpoch [99/200] | Train Loss: 0.0347, Train Acc: 0.9883 | \nEpoch [100/200] | Train Loss: 0.0457, Train Acc: 0.9883 | \nEarly stopping triggered.\nTest Loss: 1.1070, Test Accuracy: 0.7674, Test AUC: 0.8398\n\n--- Processing: psd_fc_highbeta ---\nShape: (214, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7475, Train Acc: 0.5029 | \nEpoch [2/200] | Train Loss: 0.6947, Train Acc: 0.5029 | \nEpoch [3/200] | Train Loss: 0.6898, Train Acc: 0.5497 | \nEpoch [4/200] | Train Loss: 0.6871, Train Acc: 0.4912 | \nEpoch [5/200] | Train Loss: 0.6659, Train Acc: 0.6023 | \nEpoch [6/200] | Train Loss: 0.5600, Train Acc: 0.7310 | \nEpoch [7/200] | Train Loss: 0.5293, Train Acc: 0.7602 | \nEpoch [8/200] | Train Loss: 0.6170, Train Acc: 0.6901 | \nEpoch [9/200] | Train Loss: 0.5092, Train Acc: 0.7602 | \nEpoch [10/200] | Train Loss: 0.5079, Train Acc: 0.7836 | \nEpoch [11/200] | Train Loss: 0.4451, Train Acc: 0.8070 | \nEpoch [12/200] | Train Loss: 0.3937, Train Acc: 0.7953 | \nEpoch [13/200] | Train Loss: 0.3670, Train Acc: 0.8304 | \nEpoch [14/200] | Train Loss: 0.3739, Train Acc: 0.8246 | \nEpoch [15/200] | Train Loss: 0.3158, Train Acc: 0.8655 | \nEpoch [16/200] | Train Loss: 0.3137, Train Acc: 0.8655 | \nEpoch [17/200] | Train Loss: 0.4174, Train Acc: 0.8421 | \nEpoch [18/200] | Train Loss: 0.4275, Train Acc: 0.7953 | \nEpoch [19/200] | Train Loss: 0.3280, Train Acc: 0.8830 | \nEpoch [20/200] | Train Loss: 0.2756, Train Acc: 0.9006 | \nEpoch [21/200] | Train Loss: 0.2209, Train Acc: 0.9064 | \nEpoch [22/200] | Train Loss: 0.2154, Train Acc: 0.9298 | \nEpoch [23/200] | Train Loss: 0.2838, Train Acc: 0.8655 | \nEpoch [24/200] | Train Loss: 0.2219, Train Acc: 0.9123 | \nEpoch [25/200] | Train Loss: 0.1676, Train Acc: 0.9357 | \nEpoch [26/200] | Train Loss: 0.2652, Train Acc: 0.8947 | \nEpoch [27/200] | Train Loss: 0.2117, Train Acc: 0.9064 | \nEpoch [28/200] | Train Loss: 0.1792, Train Acc: 0.9474 | \nEpoch [29/200] | Train Loss: 0.1806, Train Acc: 0.9181 | \nEpoch [30/200] | Train Loss: 0.2004, Train Acc: 0.9415 | \nEpoch [31/200] | Train Loss: 0.3470, Train Acc: 0.8421 | \nEpoch [32/200] | Train Loss: 0.2086, Train Acc: 0.9181 | \nEpoch [33/200] | Train Loss: 0.1908, Train Acc: 0.9357 | \nEpoch [34/200] | Train Loss: 0.1412, Train Acc: 0.9415 | \nEpoch [35/200] | Train Loss: 0.1258, Train Acc: 0.9591 | \nEpoch [36/200] | Train Loss: 0.1380, Train Acc: 0.9532 | \nEpoch [37/200] | Train Loss: 0.1170, Train Acc: 0.9591 | \nEpoch [38/200] | Train Loss: 0.2305, Train Acc: 0.9064 | \nEpoch [39/200] | Train Loss: 0.2884, Train Acc: 0.9123 | \nEpoch [40/200] | Train Loss: 0.2469, Train Acc: 0.9240 | \nEpoch [41/200] | Train Loss: 0.2518, Train Acc: 0.9006 | \nEpoch [42/200] | Train Loss: 0.2815, Train Acc: 0.8889 | \nEpoch [43/200] | Train Loss: 0.2936, Train Acc: 0.8889 | \nEpoch [44/200] | Train Loss: 0.2217, Train Acc: 0.9123 | \nEpoch [45/200] | Train Loss: 0.1407, Train Acc: 0.9708 | \nEpoch [46/200] | Train Loss: 0.1151, Train Acc: 0.9591 | \nEpoch [47/200] | Train Loss: 0.0814, Train Acc: 0.9766 | \nEpoch [48/200] | Train Loss: 0.1917, Train Acc: 0.9357 | \nEpoch [49/200] | Train Loss: 0.0947, Train Acc: 0.9649 | \nEpoch [50/200] | Train Loss: 0.1617, Train Acc: 0.9532 | \nEpoch [51/200] | Train Loss: 0.1605, Train Acc: 0.9415 | \nEpoch [52/200] | Train Loss: 0.1030, Train Acc: 0.9591 | \nEpoch [53/200] | Train Loss: 0.0658, Train Acc: 0.9708 | \nEpoch [54/200] | Train Loss: 0.1139, Train Acc: 0.9649 | \nEpoch [55/200] | Train Loss: 0.1195, Train Acc: 0.9708 | \nEpoch [56/200] | Train Loss: 0.2125, Train Acc: 0.9123 | \nEpoch [57/200] | Train Loss: 0.1227, Train Acc: 0.9708 | \nEpoch [58/200] | Train Loss: 0.1027, Train Acc: 0.9649 | \nEpoch [59/200] | Train Loss: 0.0744, Train Acc: 0.9591 | \nEpoch [60/200] | Train Loss: 0.0231, Train Acc: 0.9942 | \nEpoch [61/200] | Train Loss: 0.0471, Train Acc: 0.9825 | \nEpoch [62/200] | Train Loss: 0.1468, Train Acc: 0.9591 | \nEpoch [63/200] | Train Loss: 0.1625, Train Acc: 0.9474 | \nEpoch [64/200] | Train Loss: 0.1371, Train Acc: 0.9532 | \nEpoch [65/200] | Train Loss: 0.1305, Train Acc: 0.9415 | \nEpoch [66/200] | Train Loss: 0.1068, Train Acc: 0.9415 | \nEpoch [67/200] | Train Loss: 0.0830, Train Acc: 0.9649 | \nEpoch [68/200] | Train Loss: 0.0777, Train Acc: 0.9708 | \nEpoch [69/200] | Train Loss: 0.0413, Train Acc: 0.9883 | \nEpoch [70/200] | Train Loss: 0.1307, Train Acc: 0.9415 | \nEpoch 00071: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [71/200] | Train Loss: 0.1196, Train Acc: 0.9474 | \nEpoch [72/200] | Train Loss: 0.1502, Train Acc: 0.9357 | \nEpoch [73/200] | Train Loss: 0.0815, Train Acc: 0.9708 | \nEpoch [74/200] | Train Loss: 0.0618, Train Acc: 0.9766 | \nEpoch [75/200] | Train Loss: 0.0468, Train Acc: 0.9825 | \nEpoch [76/200] | Train Loss: 0.0472, Train Acc: 0.9883 | \nEpoch [77/200] | Train Loss: 0.0467, Train Acc: 0.9766 | \nEpoch [78/200] | Train Loss: 0.0324, Train Acc: 1.0000 | \nEpoch [79/200] | Train Loss: 0.0328, Train Acc: 0.9883 | \nEpoch [80/200] | Train Loss: 0.0331, Train Acc: 0.9883 | \nEpoch [81/200] | Train Loss: 0.0259, Train Acc: 0.9942 | \nEpoch 00082: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [82/200] | Train Loss: 0.0264, Train Acc: 0.9942 | \nEpoch [83/200] | Train Loss: 0.0159, Train Acc: 1.0000 | \nEpoch [84/200] | Train Loss: 0.0243, Train Acc: 0.9883 | \nEpoch [85/200] | Train Loss: 0.0351, Train Acc: 0.9883 | \nEpoch [86/200] | Train Loss: 0.0176, Train Acc: 1.0000 | \nEpoch [87/200] | Train Loss: 0.0157, Train Acc: 1.0000 | \nEpoch [88/200] | Train Loss: 0.0214, Train Acc: 0.9942 | \nEpoch [89/200] | Train Loss: 0.0313, Train Acc: 0.9942 | \nEpoch [90/200] | Train Loss: 0.0205, Train Acc: 0.9942 | \nEpoch [91/200] | Train Loss: 0.0167, Train Acc: 1.0000 | \nEpoch [92/200] | Train Loss: 0.0357, Train Acc: 0.9825 | \nEpoch [93/200] | Train Loss: 0.0212, Train Acc: 0.9883 | \nEpoch [94/200] | Train Loss: 0.0209, Train Acc: 1.0000 | \nEpoch [95/200] | Train Loss: 0.0226, Train Acc: 0.9942 | \nEpoch [96/200] | Train Loss: 0.0179, Train Acc: 1.0000 | \nEpoch [97/200] | Train Loss: 0.0287, Train Acc: 0.9942 | \nEarly stopping triggered.\nTest Loss: 0.9248, Test Accuracy: 0.7674, Test AUC: 0.8463\n\n--- Processing: psd_fc_gamma ---\nShape: (214, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.8556, Train Acc: 0.4444 | \nEpoch [2/200] | Train Loss: 0.7111, Train Acc: 0.5029 | \nEpoch [3/200] | Train Loss: 0.7037, Train Acc: 0.4971 | \nEpoch [4/200] | Train Loss: 0.7044, Train Acc: 0.5029 | \nEpoch [5/200] | Train Loss: 0.6934, Train Acc: 0.5205 | \nEpoch [6/200] | Train Loss: 0.6943, Train Acc: 0.4971 | \nEpoch [7/200] | Train Loss: 0.6839, Train Acc: 0.5263 | \nEpoch [8/200] | Train Loss: 0.6415, Train Acc: 0.6901 | \nEpoch [9/200] | Train Loss: 0.5911, Train Acc: 0.7193 | \nEpoch [10/200] | Train Loss: 0.4958, Train Acc: 0.7778 | \nEpoch [11/200] | Train Loss: 0.5633, Train Acc: 0.7076 | \nEpoch [12/200] | Train Loss: 0.5894, Train Acc: 0.7193 | \nEpoch [13/200] | Train Loss: 0.5859, Train Acc: 0.6842 | \nEpoch [14/200] | Train Loss: 0.5273, Train Acc: 0.7310 | \nEpoch [15/200] | Train Loss: 0.5460, Train Acc: 0.7018 | \nEpoch [16/200] | Train Loss: 0.4661, Train Acc: 0.7719 | \nEpoch [17/200] | Train Loss: 0.4255, Train Acc: 0.8187 | \nEpoch [18/200] | Train Loss: 0.6343, Train Acc: 0.7193 | \nEpoch [19/200] | Train Loss: 0.5550, Train Acc: 0.7076 | \nEpoch [20/200] | Train Loss: 0.4523, Train Acc: 0.7953 | \nEpoch [21/200] | Train Loss: 0.4408, Train Acc: 0.8012 | \nEpoch [22/200] | Train Loss: 0.4162, Train Acc: 0.8246 | \nEpoch [23/200] | Train Loss: 0.3845, Train Acc: 0.8421 | \nEpoch [24/200] | Train Loss: 0.3561, Train Acc: 0.8363 | \nEpoch [25/200] | Train Loss: 0.3530, Train Acc: 0.8129 | \nEpoch [26/200] | Train Loss: 0.3609, Train Acc: 0.8246 | \nEpoch [27/200] | Train Loss: 0.3467, Train Acc: 0.8538 | \nEpoch [28/200] | Train Loss: 0.2525, Train Acc: 0.8830 | \nEpoch [29/200] | Train Loss: 0.3704, Train Acc: 0.8538 | \nEpoch [30/200] | Train Loss: 0.3786, Train Acc: 0.8421 | \nEpoch [31/200] | Train Loss: 0.3193, Train Acc: 0.8655 | \nEpoch [32/200] | Train Loss: 0.3089, Train Acc: 0.8655 | \nEpoch [33/200] | Train Loss: 0.3297, Train Acc: 0.8713 | \nEpoch [34/200] | Train Loss: 0.3246, Train Acc: 0.8480 | \nEpoch [35/200] | Train Loss: 0.3149, Train Acc: 0.8538 | \nEpoch [36/200] | Train Loss: 0.3399, Train Acc: 0.8304 | \nEpoch [37/200] | Train Loss: 0.2982, Train Acc: 0.8830 | \nEpoch [38/200] | Train Loss: 0.2178, Train Acc: 0.9240 | \nEpoch [39/200] | Train Loss: 0.2487, Train Acc: 0.8947 | \nEpoch [40/200] | Train Loss: 0.2101, Train Acc: 0.9123 | \nEpoch [41/200] | Train Loss: 0.2651, Train Acc: 0.9006 | \nEpoch [42/200] | Train Loss: 0.2232, Train Acc: 0.9181 | \nEpoch [43/200] | Train Loss: 0.1950, Train Acc: 0.9064 | \nEpoch [44/200] | Train Loss: 0.1977, Train Acc: 0.9415 | \nEpoch [45/200] | Train Loss: 0.2026, Train Acc: 0.9123 | \nEpoch [46/200] | Train Loss: 0.3056, Train Acc: 0.9064 | \nEpoch [47/200] | Train Loss: 0.2001, Train Acc: 0.9181 | \nEpoch [48/200] | Train Loss: 0.1942, Train Acc: 0.9298 | \nEpoch [49/200] | Train Loss: 0.1863, Train Acc: 0.9123 | \nEpoch [50/200] | Train Loss: 0.2391, Train Acc: 0.8947 | \nEpoch [51/200] | Train Loss: 0.2003, Train Acc: 0.9415 | \nEpoch [52/200] | Train Loss: 0.2073, Train Acc: 0.9006 | \nEpoch [53/200] | Train Loss: 0.1809, Train Acc: 0.9298 | \nEpoch [54/200] | Train Loss: 0.1495, Train Acc: 0.9474 | \nEpoch [55/200] | Train Loss: 0.1245, Train Acc: 0.9298 | \nEpoch [56/200] | Train Loss: 0.3392, Train Acc: 0.8947 | \nEpoch [57/200] | Train Loss: 0.2550, Train Acc: 0.9064 | \nEpoch [58/200] | Train Loss: 0.1726, Train Acc: 0.9298 | \nEpoch [59/200] | Train Loss: 0.1279, Train Acc: 0.9532 | \nEpoch [60/200] | Train Loss: 0.1307, Train Acc: 0.9474 | \nEpoch [61/200] | Train Loss: 0.1221, Train Acc: 0.9474 | \nEpoch [62/200] | Train Loss: 0.1407, Train Acc: 0.9415 | \nEpoch [63/200] | Train Loss: 0.1253, Train Acc: 0.9591 | \nEpoch [64/200] | Train Loss: 0.1225, Train Acc: 0.9532 | \nEpoch [65/200] | Train Loss: 0.1143, Train Acc: 0.9708 | \nEpoch [66/200] | Train Loss: 0.1358, Train Acc: 0.9298 | \nEpoch [67/200] | Train Loss: 0.0861, Train Acc: 0.9708 | \nEpoch [68/200] | Train Loss: 0.0820, Train Acc: 0.9649 | \nEpoch [69/200] | Train Loss: 0.1958, Train Acc: 0.9064 | \nEpoch [70/200] | Train Loss: 0.1858, Train Acc: 0.9064 | \nEpoch [71/200] | Train Loss: 0.2245, Train Acc: 0.9064 | \nEpoch [72/200] | Train Loss: 0.1825, Train Acc: 0.9240 | \nEpoch [73/200] | Train Loss: 0.1126, Train Acc: 0.9649 | \nEpoch [74/200] | Train Loss: 0.1591, Train Acc: 0.9357 | \nEpoch [75/200] | Train Loss: 0.1328, Train Acc: 0.9357 | \nEpoch [76/200] | Train Loss: 0.1365, Train Acc: 0.9474 | \nEpoch [77/200] | Train Loss: 0.1438, Train Acc: 0.9649 | \nEpoch [78/200] | Train Loss: 0.1552, Train Acc: 0.9415 | \nEpoch 00079: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [79/200] | Train Loss: 0.2633, Train Acc: 0.9181 | \nEpoch [80/200] | Train Loss: 0.3752, Train Acc: 0.8655 | \nEpoch [81/200] | Train Loss: 0.2283, Train Acc: 0.9064 | \nEpoch [82/200] | Train Loss: 0.1897, Train Acc: 0.9415 | \nEpoch [83/200] | Train Loss: 0.1681, Train Acc: 0.9240 | \nEpoch [84/200] | Train Loss: 0.1409, Train Acc: 0.9357 | \nEarly stopping triggered.\nTest Loss: 0.5604, Test Accuracy: 0.7907, Test AUC: 0.8463\n\n=== Disorder: Mood disorder ===\n\n--- Processing: psd_all_bands ---\nShape: (532, 114)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7896, Train Acc: 0.4659 | \nEpoch [2/200] | Train Loss: 0.6943, Train Acc: 0.5435 | \nEpoch [3/200] | Train Loss: 0.6913, Train Acc: 0.5224 | \nEpoch [4/200] | Train Loss: 0.5906, Train Acc: 0.7224 | \nEpoch [5/200] | Train Loss: 0.5915, Train Acc: 0.7341 | \nEpoch [6/200] | Train Loss: 0.5721, Train Acc: 0.7506 | \nEpoch [7/200] | Train Loss: 0.4891, Train Acc: 0.7976 | \nEpoch [8/200] | Train Loss: 0.5504, Train Acc: 0.7365 | \nEpoch [9/200] | Train Loss: 0.4721, Train Acc: 0.7976 | \nEpoch [10/200] | Train Loss: 0.4764, Train Acc: 0.8024 | \nEpoch [11/200] | Train Loss: 0.4562, Train Acc: 0.8024 | \nEpoch [12/200] | Train Loss: 0.4271, Train Acc: 0.8000 | \nEpoch [13/200] | Train Loss: 0.4345, Train Acc: 0.7929 | \nEpoch [14/200] | Train Loss: 0.4012, Train Acc: 0.8353 | \nEpoch [15/200] | Train Loss: 0.3851, Train Acc: 0.8212 | \nEpoch [16/200] | Train Loss: 0.3496, Train Acc: 0.8565 | \nEpoch [17/200] | Train Loss: 0.3643, Train Acc: 0.8400 | \nEpoch [18/200] | Train Loss: 0.3399, Train Acc: 0.8518 | \nEpoch [19/200] | Train Loss: 0.3239, Train Acc: 0.8635 | \nEpoch [20/200] | Train Loss: 0.3119, Train Acc: 0.8659 | \nEpoch [21/200] | Train Loss: 0.3247, Train Acc: 0.8635 | \nEpoch [22/200] | Train Loss: 0.3293, Train Acc: 0.8635 | \nEpoch [23/200] | Train Loss: 0.3010, Train Acc: 0.8729 | \nEpoch [24/200] | Train Loss: 0.2958, Train Acc: 0.8753 | \nEpoch [25/200] | Train Loss: 0.3773, Train Acc: 0.8518 | \nEpoch [26/200] | Train Loss: 0.3196, Train Acc: 0.8753 | \nEpoch [27/200] | Train Loss: 0.2752, Train Acc: 0.8918 | \nEpoch [28/200] | Train Loss: 0.3086, Train Acc: 0.8871 | \nEpoch [29/200] | Train Loss: 0.2798, Train Acc: 0.8871 | \nEpoch [30/200] | Train Loss: 0.2884, Train Acc: 0.8776 | \nEpoch [31/200] | Train Loss: 0.3125, Train Acc: 0.8729 | \nEpoch [32/200] | Train Loss: 0.2951, Train Acc: 0.8871 | \nEpoch [33/200] | Train Loss: 0.2616, Train Acc: 0.9059 | \nEpoch [34/200] | Train Loss: 0.2748, Train Acc: 0.8965 | \nEpoch [35/200] | Train Loss: 0.2746, Train Acc: 0.9059 | \nEpoch [36/200] | Train Loss: 0.2860, Train Acc: 0.8824 | \nEpoch [37/200] | Train Loss: 0.2535, Train Acc: 0.9059 | \nEpoch [38/200] | Train Loss: 0.2345, Train Acc: 0.9176 | \nEpoch [39/200] | Train Loss: 0.2616, Train Acc: 0.8988 | \nEpoch [40/200] | Train Loss: 0.2707, Train Acc: 0.8894 | \nEpoch [41/200] | Train Loss: 0.2538, Train Acc: 0.9059 | \nEpoch [42/200] | Train Loss: 0.2328, Train Acc: 0.9176 | \nEpoch [43/200] | Train Loss: 0.2142, Train Acc: 0.9271 | \nEpoch [44/200] | Train Loss: 0.2106, Train Acc: 0.9247 | \nEpoch [45/200] | Train Loss: 0.2249, Train Acc: 0.9224 | \nEpoch [46/200] | Train Loss: 0.3661, Train Acc: 0.8471 | \nEpoch [47/200] | Train Loss: 0.4762, Train Acc: 0.7671 | \nEpoch [48/200] | Train Loss: 0.2999, Train Acc: 0.8894 | \nEpoch [49/200] | Train Loss: 0.2983, Train Acc: 0.8753 | \nEpoch [50/200] | Train Loss: 0.2553, Train Acc: 0.9082 | \nEpoch [51/200] | Train Loss: 0.2864, Train Acc: 0.8871 | \nEpoch [52/200] | Train Loss: 0.3330, Train Acc: 0.8659 | \nEpoch [53/200] | Train Loss: 0.2592, Train Acc: 0.9059 | \nEpoch [54/200] | Train Loss: 0.2752, Train Acc: 0.8941 | \nEpoch 00055: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [55/200] | Train Loss: 0.2241, Train Acc: 0.9153 | \nEpoch [56/200] | Train Loss: 0.3047, Train Acc: 0.8941 | \nEpoch [57/200] | Train Loss: 0.2406, Train Acc: 0.9129 | \nEpoch [58/200] | Train Loss: 0.2260, Train Acc: 0.9176 | \nEpoch [59/200] | Train Loss: 0.2137, Train Acc: 0.9247 | \nEpoch [60/200] | Train Loss: 0.2091, Train Acc: 0.9294 | \nEpoch [61/200] | Train Loss: 0.2005, Train Acc: 0.9341 | \nEpoch [62/200] | Train Loss: 0.1995, Train Acc: 0.9318 | \nEpoch [63/200] | Train Loss: 0.2026, Train Acc: 0.9271 | \nEpoch [64/200] | Train Loss: 0.2021, Train Acc: 0.9294 | \nEpoch [65/200] | Train Loss: 0.1982, Train Acc: 0.9318 | \nEpoch [66/200] | Train Loss: 0.2029, Train Acc: 0.9247 | \nEpoch [67/200] | Train Loss: 0.2059, Train Acc: 0.9318 | \nEpoch [68/200] | Train Loss: 0.2095, Train Acc: 0.9224 | \nEpoch [69/200] | Train Loss: 0.1902, Train Acc: 0.9318 | \nEpoch [70/200] | Train Loss: 0.1928, Train Acc: 0.9412 | \nEpoch [71/200] | Train Loss: 0.1957, Train Acc: 0.9341 | \nEpoch [72/200] | Train Loss: 0.1908, Train Acc: 0.9365 | \nEpoch [73/200] | Train Loss: 0.1803, Train Acc: 0.9412 | \nEpoch [74/200] | Train Loss: 0.1914, Train Acc: 0.9341 | \nEpoch [75/200] | Train Loss: 0.1923, Train Acc: 0.9318 | \nEpoch [76/200] | Train Loss: 0.1886, Train Acc: 0.9435 | \nEpoch [77/200] | Train Loss: 0.1821, Train Acc: 0.9388 | \nEpoch [78/200] | Train Loss: 0.1791, Train Acc: 0.9341 | \nEpoch [79/200] | Train Loss: 0.1805, Train Acc: 0.9388 | \nEpoch [80/200] | Train Loss: 0.1728, Train Acc: 0.9388 | \nEpoch [81/200] | Train Loss: 0.1786, Train Acc: 0.9365 | \nEpoch [82/200] | Train Loss: 0.1792, Train Acc: 0.9435 | \nEpoch [83/200] | Train Loss: 0.1778, Train Acc: 0.9459 | \nEpoch [84/200] | Train Loss: 0.1894, Train Acc: 0.9247 | \nEpoch [85/200] | Train Loss: 0.1811, Train Acc: 0.9388 | \nEpoch [86/200] | Train Loss: 0.1784, Train Acc: 0.9388 | \nEpoch [87/200] | Train Loss: 0.1942, Train Acc: 0.9318 | \nEpoch [88/200] | Train Loss: 0.1749, Train Acc: 0.9341 | \nEpoch [89/200] | Train Loss: 0.1769, Train Acc: 0.9412 | \nEpoch [90/200] | Train Loss: 0.1687, Train Acc: 0.9482 | \nEpoch [91/200] | Train Loss: 0.1696, Train Acc: 0.9412 | \nEpoch [92/200] | Train Loss: 0.1763, Train Acc: 0.9412 | \nEpoch [93/200] | Train Loss: 0.1698, Train Acc: 0.9388 | \nEpoch [94/200] | Train Loss: 0.1649, Train Acc: 0.9482 | \nEpoch [95/200] | Train Loss: 0.2173, Train Acc: 0.9271 | \nEpoch [96/200] | Train Loss: 0.1982, Train Acc: 0.9271 | \nEpoch [97/200] | Train Loss: 0.1894, Train Acc: 0.9318 | \nEpoch [98/200] | Train Loss: 0.1762, Train Acc: 0.9412 | \nEpoch [99/200] | Train Loss: 0.1713, Train Acc: 0.9459 | \nEpoch [100/200] | Train Loss: 0.1709, Train Acc: 0.9365 | \nEpoch [101/200] | Train Loss: 0.1724, Train Acc: 0.9412 | \nEpoch [102/200] | Train Loss: 0.1687, Train Acc: 0.9459 | \nEpoch [103/200] | Train Loss: 0.1794, Train Acc: 0.9341 | \nEpoch [104/200] | Train Loss: 0.1684, Train Acc: 0.9388 | \nEpoch 00105: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [105/200] | Train Loss: 0.1657, Train Acc: 0.9482 | \nEpoch [106/200] | Train Loss: 0.1659, Train Acc: 0.9459 | \nEpoch [107/200] | Train Loss: 0.1659, Train Acc: 0.9388 | \nEpoch [108/200] | Train Loss: 0.1732, Train Acc: 0.9412 | \nEpoch [109/200] | Train Loss: 0.1643, Train Acc: 0.9435 | \nEarly stopping triggered.\nTest Loss: 0.3194, Test Accuracy: 0.9159, Test AUC: 0.9336\n\n--- Processing: fc_all_bands ---\nShape: (532, 1026)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7803, Train Acc: 0.4847 | \nEpoch [2/200] | Train Loss: 0.7050, Train Acc: 0.5176 | \nEpoch [3/200] | Train Loss: 0.6938, Train Acc: 0.5341 | \nEpoch [4/200] | Train Loss: 0.7042, Train Acc: 0.4706 | \nEpoch [5/200] | Train Loss: 0.6989, Train Acc: 0.4706 | \nEpoch [6/200] | Train Loss: 0.6926, Train Acc: 0.5200 | \nEpoch [7/200] | Train Loss: 0.6906, Train Acc: 0.5082 | \nEpoch [8/200] | Train Loss: 0.6320, Train Acc: 0.6965 | \nEpoch [9/200] | Train Loss: 0.6955, Train Acc: 0.5976 | \nEpoch [10/200] | Train Loss: 0.6937, Train Acc: 0.4824 | \nEpoch [11/200] | Train Loss: 0.6866, Train Acc: 0.5647 | \nEpoch [12/200] | Train Loss: 0.5971, Train Acc: 0.7106 | \nEpoch [13/200] | Train Loss: 0.5900, Train Acc: 0.7153 | \nEpoch [14/200] | Train Loss: 0.5131, Train Acc: 0.7694 | \nEpoch [15/200] | Train Loss: 0.5430, Train Acc: 0.7365 | \nEpoch [16/200] | Train Loss: 0.5524, Train Acc: 0.7224 | \nEpoch [17/200] | Train Loss: 0.4829, Train Acc: 0.7835 | \nEpoch [18/200] | Train Loss: 0.5561, Train Acc: 0.7576 | \nEpoch [19/200] | Train Loss: 0.4740, Train Acc: 0.7953 | \nEpoch [20/200] | Train Loss: 0.4302, Train Acc: 0.8235 | \nEpoch [21/200] | Train Loss: 0.4207, Train Acc: 0.8141 | \nEpoch [22/200] | Train Loss: 0.3823, Train Acc: 0.8518 | \nEpoch [23/200] | Train Loss: 0.3837, Train Acc: 0.8518 | \nEpoch [24/200] | Train Loss: 0.3787, Train Acc: 0.8259 | \nEpoch [25/200] | Train Loss: 0.3219, Train Acc: 0.8776 | \nEpoch [26/200] | Train Loss: 0.3211, Train Acc: 0.8776 | \nEpoch [27/200] | Train Loss: 0.3239, Train Acc: 0.8800 | \nEpoch [28/200] | Train Loss: 0.2825, Train Acc: 0.9035 | \nEpoch [29/200] | Train Loss: 0.2599, Train Acc: 0.8941 | \nEpoch [30/200] | Train Loss: 0.2528, Train Acc: 0.9082 | \nEpoch [31/200] | Train Loss: 0.2834, Train Acc: 0.8988 | \nEpoch [32/200] | Train Loss: 0.2571, Train Acc: 0.9059 | \nEpoch [33/200] | Train Loss: 0.2286, Train Acc: 0.9271 | \nEpoch [34/200] | Train Loss: 0.2652, Train Acc: 0.8894 | \nEpoch [35/200] | Train Loss: 0.2284, Train Acc: 0.9176 | \nEpoch [36/200] | Train Loss: 0.1918, Train Acc: 0.9341 | \nEpoch [37/200] | Train Loss: 0.2308, Train Acc: 0.9200 | \nEpoch [38/200] | Train Loss: 0.2890, Train Acc: 0.8847 | \nEpoch [39/200] | Train Loss: 0.2256, Train Acc: 0.9176 | \nEpoch [40/200] | Train Loss: 0.2138, Train Acc: 0.9224 | \nEpoch [41/200] | Train Loss: 0.2309, Train Acc: 0.9129 | \nEpoch [42/200] | Train Loss: 0.2544, Train Acc: 0.9059 | \nEpoch [43/200] | Train Loss: 0.1918, Train Acc: 0.9435 | \nEpoch [44/200] | Train Loss: 0.2013, Train Acc: 0.9271 | \nEpoch [45/200] | Train Loss: 0.1719, Train Acc: 0.9506 | \nEpoch [46/200] | Train Loss: 0.1539, Train Acc: 0.9529 | \nEpoch [47/200] | Train Loss: 0.1476, Train Acc: 0.9576 | \nEpoch [48/200] | Train Loss: 0.1253, Train Acc: 0.9647 | \nEpoch [49/200] | Train Loss: 0.1376, Train Acc: 0.9600 | \nEpoch [50/200] | Train Loss: 0.1317, Train Acc: 0.9624 | \nEpoch [51/200] | Train Loss: 0.2101, Train Acc: 0.9224 | \nEpoch [52/200] | Train Loss: 0.4149, Train Acc: 0.8165 | \nEpoch [53/200] | Train Loss: 0.2811, Train Acc: 0.8682 | \nEpoch [54/200] | Train Loss: 0.1470, Train Acc: 0.9553 | \nEpoch [55/200] | Train Loss: 0.2049, Train Acc: 0.9271 | \nEpoch [56/200] | Train Loss: 0.2331, Train Acc: 0.9200 | \nEpoch [57/200] | Train Loss: 0.1985, Train Acc: 0.9341 | \nEpoch [58/200] | Train Loss: 0.1567, Train Acc: 0.9529 | \nEpoch 00059: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [59/200] | Train Loss: 0.1480, Train Acc: 0.9506 | \nEpoch [60/200] | Train Loss: 0.0979, Train Acc: 0.9694 | \nEpoch [61/200] | Train Loss: 0.0993, Train Acc: 0.9647 | \nEpoch [62/200] | Train Loss: 0.0941, Train Acc: 0.9741 | \nEpoch [63/200] | Train Loss: 0.0897, Train Acc: 0.9741 | \nEpoch [64/200] | Train Loss: 0.0937, Train Acc: 0.9741 | \nEpoch [65/200] | Train Loss: 0.0812, Train Acc: 0.9788 | \nEpoch [66/200] | Train Loss: 0.0822, Train Acc: 0.9812 | \nEpoch [67/200] | Train Loss: 0.0821, Train Acc: 0.9812 | \nEpoch [68/200] | Train Loss: 0.0904, Train Acc: 0.9765 | \nEpoch [69/200] | Train Loss: 0.0860, Train Acc: 0.9788 | \nEpoch [70/200] | Train Loss: 0.0826, Train Acc: 0.9765 | \nEpoch [71/200] | Train Loss: 0.0734, Train Acc: 0.9859 | \nEpoch [72/200] | Train Loss: 0.0712, Train Acc: 0.9859 | \nEpoch [73/200] | Train Loss: 0.0709, Train Acc: 0.9859 | \nEpoch [74/200] | Train Loss: 0.0778, Train Acc: 0.9765 | \nEpoch [75/200] | Train Loss: 0.0720, Train Acc: 0.9788 | \nEpoch [76/200] | Train Loss: 0.0759, Train Acc: 0.9788 | \nEpoch [77/200] | Train Loss: 0.0720, Train Acc: 0.9835 | \nEpoch [78/200] | Train Loss: 0.0779, Train Acc: 0.9812 | \nEpoch [79/200] | Train Loss: 0.0760, Train Acc: 0.9788 | \nEpoch [80/200] | Train Loss: 0.0687, Train Acc: 0.9835 | \nEpoch [81/200] | Train Loss: 0.0715, Train Acc: 0.9835 | \nEpoch [82/200] | Train Loss: 0.0679, Train Acc: 0.9835 | \nEpoch [83/200] | Train Loss: 0.0689, Train Acc: 0.9859 | \nEpoch [84/200] | Train Loss: 0.0657, Train Acc: 0.9859 | \nEpoch [85/200] | Train Loss: 0.0686, Train Acc: 0.9835 | \nEpoch [86/200] | Train Loss: 0.0737, Train Acc: 0.9835 | \nEpoch [87/200] | Train Loss: 0.0708, Train Acc: 0.9812 | \nEpoch [88/200] | Train Loss: 0.0732, Train Acc: 0.9812 | \nEpoch [89/200] | Train Loss: 0.0687, Train Acc: 0.9835 | \nEpoch [90/200] | Train Loss: 0.0661, Train Acc: 0.9859 | \nEarly stopping triggered.\nTest Loss: 0.4006, Test Accuracy: 0.8785, Test AUC: 0.9476\n\n--- Processing: psd_fc_all_bands ---\nShape: (532, 1140)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7455, Train Acc: 0.5129 | \nEpoch [2/200] | Train Loss: 0.6938, Train Acc: 0.4776 | \nEpoch [3/200] | Train Loss: 0.7013, Train Acc: 0.5082 | \nEpoch [4/200] | Train Loss: 0.6991, Train Acc: 0.5035 | \nEpoch [5/200] | Train Loss: 0.6668, Train Acc: 0.6141 | \nEpoch [6/200] | Train Loss: 0.6072, Train Acc: 0.6776 | \nEpoch [7/200] | Train Loss: 0.5640, Train Acc: 0.7082 | \nEpoch [8/200] | Train Loss: 0.3982, Train Acc: 0.8165 | \nEpoch [9/200] | Train Loss: 0.3481, Train Acc: 0.8588 | \nEpoch [10/200] | Train Loss: 0.3443, Train Acc: 0.8659 | \nEpoch [11/200] | Train Loss: 0.2999, Train Acc: 0.8800 | \nEpoch [12/200] | Train Loss: 0.2650, Train Acc: 0.8941 | \nEpoch [13/200] | Train Loss: 0.2446, Train Acc: 0.8965 | \nEpoch [14/200] | Train Loss: 0.3124, Train Acc: 0.8776 | \nEpoch [15/200] | Train Loss: 0.2759, Train Acc: 0.8800 | \nEpoch [16/200] | Train Loss: 0.1939, Train Acc: 0.9341 | \nEpoch [17/200] | Train Loss: 0.1663, Train Acc: 0.9506 | \nEpoch [18/200] | Train Loss: 0.2284, Train Acc: 0.9106 | \nEpoch [19/200] | Train Loss: 0.2096, Train Acc: 0.9200 | \nEpoch [20/200] | Train Loss: 0.1607, Train Acc: 0.9506 | \nEpoch [21/200] | Train Loss: 0.3171, Train Acc: 0.8894 | \nEpoch [22/200] | Train Loss: 0.2197, Train Acc: 0.9059 | \nEpoch [23/200] | Train Loss: 0.1515, Train Acc: 0.9459 | \nEpoch [24/200] | Train Loss: 0.1617, Train Acc: 0.9459 | \nEpoch [25/200] | Train Loss: 0.1316, Train Acc: 0.9529 | \nEpoch [26/200] | Train Loss: 0.1881, Train Acc: 0.9318 | \nEpoch [27/200] | Train Loss: 0.2235, Train Acc: 0.9106 | \nEpoch [28/200] | Train Loss: 0.1687, Train Acc: 0.9459 | \nEpoch [29/200] | Train Loss: 0.1134, Train Acc: 0.9647 | \nEpoch [30/200] | Train Loss: 0.2068, Train Acc: 0.9200 | \nEpoch [31/200] | Train Loss: 0.2367, Train Acc: 0.9176 | \nEpoch [32/200] | Train Loss: 0.1859, Train Acc: 0.9482 | \nEpoch [33/200] | Train Loss: 0.1061, Train Acc: 0.9647 | \nEpoch [34/200] | Train Loss: 0.1607, Train Acc: 0.9553 | \nEpoch [35/200] | Train Loss: 0.2300, Train Acc: 0.9224 | \nEpoch [36/200] | Train Loss: 0.1587, Train Acc: 0.9388 | \nEpoch [37/200] | Train Loss: 0.1473, Train Acc: 0.9435 | \nEpoch [38/200] | Train Loss: 0.0712, Train Acc: 0.9741 | \nEpoch [39/200] | Train Loss: 0.0775, Train Acc: 0.9765 | \nEpoch [40/200] | Train Loss: 0.2465, Train Acc: 0.9153 | \nEpoch [41/200] | Train Loss: 0.2003, Train Acc: 0.9318 | \nEpoch [42/200] | Train Loss: 0.1374, Train Acc: 0.9600 | \nEpoch [43/200] | Train Loss: 0.1088, Train Acc: 0.9694 | \nEpoch [44/200] | Train Loss: 0.0878, Train Acc: 0.9812 | \nEpoch [45/200] | Train Loss: 0.0569, Train Acc: 0.9859 | \nEpoch [46/200] | Train Loss: 0.1130, Train Acc: 0.9647 | \nEpoch [47/200] | Train Loss: 0.0910, Train Acc: 0.9718 | \nEpoch [48/200] | Train Loss: 0.1340, Train Acc: 0.9506 | \nEpoch [49/200] | Train Loss: 0.1138, Train Acc: 0.9576 | \nEpoch [50/200] | Train Loss: 0.0727, Train Acc: 0.9835 | \nEpoch [51/200] | Train Loss: 0.0865, Train Acc: 0.9765 | \nEpoch [52/200] | Train Loss: 0.0620, Train Acc: 0.9812 | \nEpoch [53/200] | Train Loss: 0.0724, Train Acc: 0.9765 | \nEpoch [54/200] | Train Loss: 0.1209, Train Acc: 0.9553 | \nEpoch [55/200] | Train Loss: 0.0604, Train Acc: 0.9835 | \nEpoch [56/200] | Train Loss: 0.0525, Train Acc: 0.9882 | \nEpoch [57/200] | Train Loss: 0.0381, Train Acc: 0.9929 | \nEpoch [58/200] | Train Loss: 0.0392, Train Acc: 0.9929 | \nEpoch [59/200] | Train Loss: 0.0741, Train Acc: 0.9835 | \nEpoch [60/200] | Train Loss: 0.1064, Train Acc: 0.9718 | \nEpoch [61/200] | Train Loss: 0.0716, Train Acc: 0.9718 | \nEpoch [62/200] | Train Loss: 0.0634, Train Acc: 0.9859 | \nEpoch [63/200] | Train Loss: 0.0569, Train Acc: 0.9812 | \nEpoch [64/200] | Train Loss: 0.0811, Train Acc: 0.9694 | \nEpoch [65/200] | Train Loss: 0.2171, Train Acc: 0.9318 | \nEpoch [66/200] | Train Loss: 0.0950, Train Acc: 0.9624 | \nEpoch [67/200] | Train Loss: 0.0553, Train Acc: 0.9882 | \nEpoch 00068: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [68/200] | Train Loss: 0.0386, Train Acc: 0.9882 | \nEpoch [69/200] | Train Loss: 0.0333, Train Acc: 0.9929 | \nEpoch [70/200] | Train Loss: 0.0356, Train Acc: 0.9906 | \nEpoch [71/200] | Train Loss: 0.0334, Train Acc: 0.9929 | \nEpoch [72/200] | Train Loss: 0.0295, Train Acc: 0.9953 | \nEpoch [73/200] | Train Loss: 0.0302, Train Acc: 0.9953 | \nEpoch [74/200] | Train Loss: 0.0293, Train Acc: 0.9953 | \nEpoch [75/200] | Train Loss: 0.0293, Train Acc: 0.9953 | \nEpoch [76/200] | Train Loss: 0.0285, Train Acc: 0.9953 | \nEpoch [77/200] | Train Loss: 0.0318, Train Acc: 0.9929 | \nEpoch [78/200] | Train Loss: 0.0293, Train Acc: 0.9953 | \nEpoch [79/200] | Train Loss: 0.0293, Train Acc: 0.9953 | \nEpoch [80/200] | Train Loss: 0.0289, Train Acc: 0.9953 | \nEpoch [81/200] | Train Loss: 0.0285, Train Acc: 0.9953 | \nEpoch [82/200] | Train Loss: 0.0288, Train Acc: 0.9953 | \nEpoch [83/200] | Train Loss: 0.0289, Train Acc: 0.9953 | \nEpoch [84/200] | Train Loss: 0.0295, Train Acc: 0.9953 | \nEpoch [85/200] | Train Loss: 0.0282, Train Acc: 0.9953 | \nEpoch [86/200] | Train Loss: 0.0285, Train Acc: 0.9953 | \nEpoch [87/200] | Train Loss: 0.0281, Train Acc: 0.9953 | \nEpoch [88/200] | Train Loss: 0.0289, Train Acc: 0.9953 | \nEpoch [89/200] | Train Loss: 0.0284, Train Acc: 0.9953 | \nEpoch [90/200] | Train Loss: 0.0281, Train Acc: 0.9953 | \nEpoch [91/200] | Train Loss: 0.0285, Train Acc: 0.9953 | \nEarly stopping triggered.\nTest Loss: 0.4283, Test Accuracy: 0.8879, Test AUC: 0.9406\n\n--- Processing: psd_delta ---\nShape: (532, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7143, Train Acc: 0.5035 | \nEpoch [2/200] | Train Loss: 0.5981, Train Acc: 0.6871 | \nEpoch [3/200] | Train Loss: 0.4938, Train Acc: 0.7765 | \nEpoch [4/200] | Train Loss: 0.4991, Train Acc: 0.7741 | \nEpoch [5/200] | Train Loss: 0.4413, Train Acc: 0.8000 | \nEpoch [6/200] | Train Loss: 0.4783, Train Acc: 0.7765 | \nEpoch [7/200] | Train Loss: 0.3992, Train Acc: 0.8141 | \nEpoch [8/200] | Train Loss: 0.3951, Train Acc: 0.8188 | \nEpoch [9/200] | Train Loss: 0.4159, Train Acc: 0.7929 | \nEpoch [10/200] | Train Loss: 0.4229, Train Acc: 0.8071 | \nEpoch [11/200] | Train Loss: 0.4212, Train Acc: 0.8094 | \nEpoch [12/200] | Train Loss: 0.3836, Train Acc: 0.8188 | \nEpoch [13/200] | Train Loss: 0.3828, Train Acc: 0.8165 | \nEpoch [14/200] | Train Loss: 0.3647, Train Acc: 0.8353 | \nEpoch [15/200] | Train Loss: 0.3523, Train Acc: 0.8565 | \nEpoch [16/200] | Train Loss: 0.3632, Train Acc: 0.8259 | \nEpoch [17/200] | Train Loss: 0.3441, Train Acc: 0.8494 | \nEpoch [18/200] | Train Loss: 0.4046, Train Acc: 0.8071 | \nEpoch [19/200] | Train Loss: 0.3896, Train Acc: 0.8282 | \nEpoch [20/200] | Train Loss: 0.4183, Train Acc: 0.8165 | \nEpoch [21/200] | Train Loss: 0.3723, Train Acc: 0.8400 | \nEpoch [22/200] | Train Loss: 0.3492, Train Acc: 0.8447 | \nEpoch [23/200] | Train Loss: 0.3484, Train Acc: 0.8541 | \nEpoch [24/200] | Train Loss: 0.3762, Train Acc: 0.8400 | \nEpoch [25/200] | Train Loss: 0.3470, Train Acc: 0.8471 | \nEpoch [26/200] | Train Loss: 0.3243, Train Acc: 0.8518 | \nEpoch [27/200] | Train Loss: 0.3350, Train Acc: 0.8612 | \nEpoch [28/200] | Train Loss: 0.3617, Train Acc: 0.8447 | \nEpoch [29/200] | Train Loss: 0.4050, Train Acc: 0.8141 | \nEpoch [30/200] | Train Loss: 0.3509, Train Acc: 0.8588 | \nEpoch [31/200] | Train Loss: 0.3269, Train Acc: 0.8635 | \nEpoch [32/200] | Train Loss: 0.3172, Train Acc: 0.8753 | \nEpoch [33/200] | Train Loss: 0.3571, Train Acc: 0.8471 | \nEpoch [34/200] | Train Loss: 0.3353, Train Acc: 0.8565 | \nEpoch [35/200] | Train Loss: 0.3556, Train Acc: 0.8471 | \nEpoch [36/200] | Train Loss: 0.3379, Train Acc: 0.8518 | \nEpoch [37/200] | Train Loss: 0.3284, Train Acc: 0.8682 | \nEpoch [38/200] | Train Loss: 0.3405, Train Acc: 0.8494 | \nEpoch [39/200] | Train Loss: 0.3756, Train Acc: 0.8118 | \nEpoch [40/200] | Train Loss: 0.3199, Train Acc: 0.8588 | \nEpoch [41/200] | Train Loss: 0.3119, Train Acc: 0.8776 | \nEpoch [42/200] | Train Loss: 0.4457, Train Acc: 0.8259 | \nEpoch [43/200] | Train Loss: 0.4226, Train Acc: 0.8118 | \nEpoch [44/200] | Train Loss: 0.3800, Train Acc: 0.8235 | \nEpoch [45/200] | Train Loss: 0.3776, Train Acc: 0.8306 | \nEpoch [46/200] | Train Loss: 0.3556, Train Acc: 0.8400 | \nEpoch [47/200] | Train Loss: 0.3505, Train Acc: 0.8353 | \nEpoch [48/200] | Train Loss: 0.3520, Train Acc: 0.8400 | \nEpoch [49/200] | Train Loss: 0.3544, Train Acc: 0.8494 | \nEpoch [50/200] | Train Loss: 0.3522, Train Acc: 0.8376 | \nEpoch [51/200] | Train Loss: 0.3255, Train Acc: 0.8541 | \nEpoch 00052: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [52/200] | Train Loss: 0.3288, Train Acc: 0.8612 | \nEpoch [53/200] | Train Loss: 0.3177, Train Acc: 0.8682 | \nEpoch [54/200] | Train Loss: 0.3070, Train Acc: 0.8682 | \nEpoch [55/200] | Train Loss: 0.2964, Train Acc: 0.8824 | \nEpoch [56/200] | Train Loss: 0.2957, Train Acc: 0.8871 | \nEpoch [57/200] | Train Loss: 0.2901, Train Acc: 0.8800 | \nEpoch [58/200] | Train Loss: 0.2910, Train Acc: 0.8753 | \nEpoch [59/200] | Train Loss: 0.2918, Train Acc: 0.8729 | \nEpoch [60/200] | Train Loss: 0.2945, Train Acc: 0.8871 | \nEpoch [61/200] | Train Loss: 0.2753, Train Acc: 0.8847 | \nEpoch [62/200] | Train Loss: 0.2742, Train Acc: 0.8965 | \nEpoch [63/200] | Train Loss: 0.2752, Train Acc: 0.8776 | \nEpoch [64/200] | Train Loss: 0.2875, Train Acc: 0.8871 | \nEpoch [65/200] | Train Loss: 0.2754, Train Acc: 0.8824 | \nEpoch [66/200] | Train Loss: 0.2677, Train Acc: 0.8965 | \nEpoch [67/200] | Train Loss: 0.2687, Train Acc: 0.8918 | \nEpoch [68/200] | Train Loss: 0.2645, Train Acc: 0.8965 | \nEpoch [69/200] | Train Loss: 0.2594, Train Acc: 0.9035 | \nEpoch [70/200] | Train Loss: 0.2628, Train Acc: 0.8988 | \nEpoch [71/200] | Train Loss: 0.2620, Train Acc: 0.8918 | \nEpoch [72/200] | Train Loss: 0.2685, Train Acc: 0.8965 | \nEpoch [73/200] | Train Loss: 0.2665, Train Acc: 0.8871 | \nEpoch [74/200] | Train Loss: 0.2692, Train Acc: 0.8941 | \nEpoch [75/200] | Train Loss: 0.2724, Train Acc: 0.8941 | \nEpoch [76/200] | Train Loss: 0.2610, Train Acc: 0.9012 | \nEpoch [77/200] | Train Loss: 0.2659, Train Acc: 0.8988 | \nEpoch [78/200] | Train Loss: 0.2655, Train Acc: 0.8965 | \nEpoch [79/200] | Train Loss: 0.2611, Train Acc: 0.9012 | \nEpoch [80/200] | Train Loss: 0.2503, Train Acc: 0.8941 | \nEpoch [81/200] | Train Loss: 0.2490, Train Acc: 0.9059 | \nEpoch [82/200] | Train Loss: 0.2676, Train Acc: 0.8894 | \nEpoch [83/200] | Train Loss: 0.2483, Train Acc: 0.9035 | \nEpoch [84/200] | Train Loss: 0.2644, Train Acc: 0.8965 | \nEpoch [85/200] | Train Loss: 0.2626, Train Acc: 0.8871 | \nEpoch [86/200] | Train Loss: 0.2664, Train Acc: 0.8941 | \nEpoch [87/200] | Train Loss: 0.2579, Train Acc: 0.8941 | \nEpoch [88/200] | Train Loss: 0.2614, Train Acc: 0.9012 | \nEpoch [89/200] | Train Loss: 0.2386, Train Acc: 0.9082 | \nEpoch [90/200] | Train Loss: 0.2421, Train Acc: 0.9082 | \nEpoch [91/200] | Train Loss: 0.2459, Train Acc: 0.8965 | \nEpoch [92/200] | Train Loss: 0.2376, Train Acc: 0.9035 | \nEpoch [93/200] | Train Loss: 0.2476, Train Acc: 0.8988 | \nEpoch [94/200] | Train Loss: 0.2342, Train Acc: 0.9059 | \nEpoch [95/200] | Train Loss: 0.2397, Train Acc: 0.9129 | \nEpoch [96/200] | Train Loss: 0.2611, Train Acc: 0.9035 | \nEpoch [97/200] | Train Loss: 0.2381, Train Acc: 0.9129 | \nEpoch [98/200] | Train Loss: 0.2418, Train Acc: 0.9153 | \nEpoch [99/200] | Train Loss: 0.2354, Train Acc: 0.9082 | \nEpoch [100/200] | Train Loss: 0.2438, Train Acc: 0.9035 | \nEpoch [101/200] | Train Loss: 0.2370, Train Acc: 0.9129 | \nEpoch [102/200] | Train Loss: 0.2383, Train Acc: 0.9082 | \nEpoch [103/200] | Train Loss: 0.2402, Train Acc: 0.9059 | \nEpoch [104/200] | Train Loss: 0.2421, Train Acc: 0.9035 | \nEpoch 00105: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [105/200] | Train Loss: 0.2386, Train Acc: 0.9059 | \nEpoch [106/200] | Train Loss: 0.2472, Train Acc: 0.9035 | \nEpoch [107/200] | Train Loss: 0.2421, Train Acc: 0.8988 | \nEpoch [108/200] | Train Loss: 0.2340, Train Acc: 0.9129 | \nEpoch [109/200] | Train Loss: 0.2221, Train Acc: 0.9224 | \nEpoch [110/200] | Train Loss: 0.2328, Train Acc: 0.9059 | \nEpoch [111/200] | Train Loss: 0.2332, Train Acc: 0.9129 | \nEpoch [112/200] | Train Loss: 0.2248, Train Acc: 0.9129 | \nEpoch [113/200] | Train Loss: 0.2366, Train Acc: 0.9035 | \nEpoch [114/200] | Train Loss: 0.2401, Train Acc: 0.9059 | \nEpoch [115/200] | Train Loss: 0.2259, Train Acc: 0.9176 | \nEpoch [116/200] | Train Loss: 0.2340, Train Acc: 0.9153 | \nEpoch [117/200] | Train Loss: 0.2291, Train Acc: 0.9106 | \nEpoch [118/200] | Train Loss: 0.2291, Train Acc: 0.9153 | \nEpoch [119/200] | Train Loss: 0.2248, Train Acc: 0.9106 | \nEpoch 00120: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [120/200] | Train Loss: 0.2238, Train Acc: 0.9153 | \nEpoch [121/200] | Train Loss: 0.2321, Train Acc: 0.9176 | \nEpoch [122/200] | Train Loss: 0.2270, Train Acc: 0.9176 | \nEpoch [123/200] | Train Loss: 0.2330, Train Acc: 0.9153 | \nEpoch [124/200] | Train Loss: 0.2211, Train Acc: 0.9200 | \nEpoch [125/200] | Train Loss: 0.2226, Train Acc: 0.9200 | \nEpoch [126/200] | Train Loss: 0.2319, Train Acc: 0.9200 | \nEpoch [127/200] | Train Loss: 0.2362, Train Acc: 0.9059 | \nEpoch [128/200] | Train Loss: 0.2333, Train Acc: 0.9176 | \nEpoch [129/200] | Train Loss: 0.2233, Train Acc: 0.9247 | \nEpoch [130/200] | Train Loss: 0.2146, Train Acc: 0.9153 | \nEpoch [131/200] | Train Loss: 0.2274, Train Acc: 0.9200 | \nEpoch [132/200] | Train Loss: 0.2261, Train Acc: 0.9106 | \nEpoch [133/200] | Train Loss: 0.2190, Train Acc: 0.9129 | \nEpoch [134/200] | Train Loss: 0.2256, Train Acc: 0.9176 | \nEpoch [135/200] | Train Loss: 0.2201, Train Acc: 0.9153 | \nEpoch [136/200] | Train Loss: 0.2127, Train Acc: 0.9153 | \nEpoch [137/200] | Train Loss: 0.2250, Train Acc: 0.9200 | \nEpoch [138/200] | Train Loss: 0.2213, Train Acc: 0.9176 | \nEpoch [139/200] | Train Loss: 0.2184, Train Acc: 0.9129 | \nEpoch [140/200] | Train Loss: 0.2182, Train Acc: 0.9200 | \nEpoch [141/200] | Train Loss: 0.2246, Train Acc: 0.9153 | \nEpoch [142/200] | Train Loss: 0.2358, Train Acc: 0.9224 | \nEpoch [143/200] | Train Loss: 0.2240, Train Acc: 0.9200 | \nEpoch [144/200] | Train Loss: 0.2324, Train Acc: 0.9059 | \nEpoch [145/200] | Train Loss: 0.2176, Train Acc: 0.9176 | \nEpoch [146/200] | Train Loss: 0.2132, Train Acc: 0.9271 | \nEpoch 00147: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [147/200] | Train Loss: 0.2286, Train Acc: 0.9129 | \nEpoch [148/200] | Train Loss: 0.2153, Train Acc: 0.9176 | \nEpoch [149/200] | Train Loss: 0.2160, Train Acc: 0.9224 | \nEpoch [150/200] | Train Loss: 0.2243, Train Acc: 0.9224 | \nEpoch [151/200] | Train Loss: 0.2318, Train Acc: 0.9129 | \nEpoch [152/200] | Train Loss: 0.2313, Train Acc: 0.9176 | \nEpoch [153/200] | Train Loss: 0.2229, Train Acc: 0.9153 | \nEpoch [154/200] | Train Loss: 0.2270, Train Acc: 0.9200 | \nEpoch [155/200] | Train Loss: 0.2134, Train Acc: 0.9247 | \nEpoch [156/200] | Train Loss: 0.2229, Train Acc: 0.9200 | \nEpoch [157/200] | Train Loss: 0.2278, Train Acc: 0.9082 | \nEpoch 00158: reducing learning rate of group 0 to 1.0000e-08.\nEpoch [158/200] | Train Loss: 0.2322, Train Acc: 0.9176 | \nEpoch [159/200] | Train Loss: 0.2105, Train Acc: 0.9294 | \nEpoch [160/200] | Train Loss: 0.2272, Train Acc: 0.9059 | \nEpoch [161/200] | Train Loss: 0.2161, Train Acc: 0.9224 | \nEpoch [162/200] | Train Loss: 0.2280, Train Acc: 0.9106 | \nEpoch [163/200] | Train Loss: 0.2262, Train Acc: 0.9129 | \nEpoch [164/200] | Train Loss: 0.2146, Train Acc: 0.9224 | \nEpoch [165/200] | Train Loss: 0.2244, Train Acc: 0.9129 | \nEpoch [166/200] | Train Loss: 0.2187, Train Acc: 0.9176 | \nEpoch [167/200] | Train Loss: 0.2270, Train Acc: 0.9106 | \nEpoch [168/200] | Train Loss: 0.2198, Train Acc: 0.9153 | \nEpoch [169/200] | Train Loss: 0.2432, Train Acc: 0.8941 | \nEpoch [170/200] | Train Loss: 0.2186, Train Acc: 0.9200 | \nEpoch [171/200] | Train Loss: 0.2248, Train Acc: 0.9129 | \nEpoch [172/200] | Train Loss: 0.2105, Train Acc: 0.9271 | \nEpoch [173/200] | Train Loss: 0.2305, Train Acc: 0.9082 | \nEpoch [174/200] | Train Loss: 0.2315, Train Acc: 0.9106 | \nEpoch [175/200] | Train Loss: 0.2314, Train Acc: 0.9129 | \nEpoch [176/200] | Train Loss: 0.2267, Train Acc: 0.9224 | \nEpoch [177/200] | Train Loss: 0.2160, Train Acc: 0.9224 | \nEpoch [178/200] | Train Loss: 0.2272, Train Acc: 0.9059 | \nEarly stopping triggered.\nTest Loss: 0.3202, Test Accuracy: 0.8972, Test AUC: 0.9308\n\n--- Processing: psd_theta ---\nShape: (532, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7265, Train Acc: 0.4988 | \nEpoch [2/200] | Train Loss: 0.5739, Train Acc: 0.7129 | \nEpoch [3/200] | Train Loss: 0.5451, Train Acc: 0.7247 | \nEpoch [4/200] | Train Loss: 0.4861, Train Acc: 0.7553 | \nEpoch [5/200] | Train Loss: 0.4963, Train Acc: 0.7882 | \nEpoch [6/200] | Train Loss: 0.4758, Train Acc: 0.7812 | \nEpoch [7/200] | Train Loss: 0.5046, Train Acc: 0.7529 | \nEpoch [8/200] | Train Loss: 0.4819, Train Acc: 0.7553 | \nEpoch [9/200] | Train Loss: 0.4689, Train Acc: 0.7835 | \nEpoch [10/200] | Train Loss: 0.4570, Train Acc: 0.7835 | \nEpoch [11/200] | Train Loss: 0.4997, Train Acc: 0.7765 | \nEpoch [12/200] | Train Loss: 0.4372, Train Acc: 0.7953 | \nEpoch [13/200] | Train Loss: 0.4019, Train Acc: 0.8094 | \nEpoch [14/200] | Train Loss: 0.4854, Train Acc: 0.7576 | \nEpoch [15/200] | Train Loss: 0.4656, Train Acc: 0.7929 | \nEpoch [16/200] | Train Loss: 0.4516, Train Acc: 0.7741 | \nEpoch [17/200] | Train Loss: 0.4153, Train Acc: 0.8047 | \nEpoch [18/200] | Train Loss: 0.3974, Train Acc: 0.8282 | \nEpoch [19/200] | Train Loss: 0.4308, Train Acc: 0.7929 | \nEpoch [20/200] | Train Loss: 0.4173, Train Acc: 0.8047 | \nEpoch [21/200] | Train Loss: 0.3974, Train Acc: 0.8212 | \nEpoch [22/200] | Train Loss: 0.4126, Train Acc: 0.8071 | \nEpoch [23/200] | Train Loss: 0.4421, Train Acc: 0.7906 | \nEpoch [24/200] | Train Loss: 0.4078, Train Acc: 0.7929 | \nEpoch [25/200] | Train Loss: 0.3862, Train Acc: 0.8188 | \nEpoch [26/200] | Train Loss: 0.4135, Train Acc: 0.7976 | \nEpoch [27/200] | Train Loss: 0.3712, Train Acc: 0.8329 | \nEpoch [28/200] | Train Loss: 0.3720, Train Acc: 0.8400 | \nEpoch [29/200] | Train Loss: 0.4267, Train Acc: 0.7976 | \nEpoch [30/200] | Train Loss: 0.4562, Train Acc: 0.7976 | \nEpoch [31/200] | Train Loss: 0.3863, Train Acc: 0.8329 | \nEpoch [32/200] | Train Loss: 0.3682, Train Acc: 0.8400 | \nEpoch [33/200] | Train Loss: 0.3975, Train Acc: 0.8235 | \nEpoch [34/200] | Train Loss: 0.4005, Train Acc: 0.8188 | \nEpoch [35/200] | Train Loss: 0.3712, Train Acc: 0.8353 | \nEpoch [36/200] | Train Loss: 0.3674, Train Acc: 0.8424 | \nEpoch [37/200] | Train Loss: 0.3737, Train Acc: 0.8424 | \nEpoch [38/200] | Train Loss: 0.3933, Train Acc: 0.8329 | \nEpoch [39/200] | Train Loss: 0.3898, Train Acc: 0.8376 | \nEpoch [40/200] | Train Loss: 0.3764, Train Acc: 0.8329 | \nEpoch [41/200] | Train Loss: 0.3637, Train Acc: 0.8282 | \nEpoch [42/200] | Train Loss: 0.3671, Train Acc: 0.8282 | \nEpoch [43/200] | Train Loss: 0.3997, Train Acc: 0.8235 | \nEpoch [44/200] | Train Loss: 0.3754, Train Acc: 0.8212 | \nEpoch [45/200] | Train Loss: 0.3912, Train Acc: 0.8212 | \nEpoch [46/200] | Train Loss: 0.4240, Train Acc: 0.8071 | \nEpoch [47/200] | Train Loss: 0.3653, Train Acc: 0.8471 | \nEpoch [48/200] | Train Loss: 0.3645, Train Acc: 0.8400 | \nEpoch [49/200] | Train Loss: 0.3571, Train Acc: 0.8306 | \nEpoch [50/200] | Train Loss: 0.3436, Train Acc: 0.8494 | \nEpoch [51/200] | Train Loss: 0.3814, Train Acc: 0.8424 | \nEpoch [52/200] | Train Loss: 0.3550, Train Acc: 0.8329 | \nEpoch [53/200] | Train Loss: 0.3559, Train Acc: 0.8424 | \nEpoch [54/200] | Train Loss: 0.3826, Train Acc: 0.8212 | \nEpoch [55/200] | Train Loss: 0.3375, Train Acc: 0.8565 | \nEpoch [56/200] | Train Loss: 0.3197, Train Acc: 0.8518 | \nEpoch [57/200] | Train Loss: 0.3081, Train Acc: 0.8776 | \nEpoch [58/200] | Train Loss: 0.3262, Train Acc: 0.8588 | \nEpoch [59/200] | Train Loss: 0.3435, Train Acc: 0.8541 | \nEpoch [60/200] | Train Loss: 0.3790, Train Acc: 0.8282 | \nEpoch [61/200] | Train Loss: 0.3398, Train Acc: 0.8541 | \nEpoch [62/200] | Train Loss: 0.4108, Train Acc: 0.8118 | \nEpoch [63/200] | Train Loss: 0.3433, Train Acc: 0.8541 | \nEpoch [64/200] | Train Loss: 0.3119, Train Acc: 0.8635 | \nEpoch [65/200] | Train Loss: 0.3161, Train Acc: 0.8682 | \nEpoch [66/200] | Train Loss: 0.3280, Train Acc: 0.8682 | \nEpoch [67/200] | Train Loss: 0.3555, Train Acc: 0.8306 | \nEpoch 00068: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [68/200] | Train Loss: 0.3294, Train Acc: 0.8612 | \nEpoch [69/200] | Train Loss: 0.2907, Train Acc: 0.8659 | \nEpoch [70/200] | Train Loss: 0.2859, Train Acc: 0.8753 | \nEpoch [71/200] | Train Loss: 0.2737, Train Acc: 0.8941 | \nEpoch [72/200] | Train Loss: 0.2685, Train Acc: 0.8800 | \nEpoch [73/200] | Train Loss: 0.2627, Train Acc: 0.9012 | \nEpoch [74/200] | Train Loss: 0.2565, Train Acc: 0.9035 | \nEpoch [75/200] | Train Loss: 0.2528, Train Acc: 0.9059 | \nEpoch [76/200] | Train Loss: 0.2560, Train Acc: 0.9082 | \nEpoch [77/200] | Train Loss: 0.2504, Train Acc: 0.8965 | \nEpoch [78/200] | Train Loss: 0.2497, Train Acc: 0.8965 | \nEpoch [79/200] | Train Loss: 0.2415, Train Acc: 0.9035 | \nEpoch [80/200] | Train Loss: 0.2449, Train Acc: 0.9035 | \nEpoch [81/200] | Train Loss: 0.2470, Train Acc: 0.9059 | \nEpoch [82/200] | Train Loss: 0.2375, Train Acc: 0.9129 | \nEpoch [83/200] | Train Loss: 0.2340, Train Acc: 0.9200 | \nEpoch [84/200] | Train Loss: 0.2379, Train Acc: 0.9153 | \nEpoch [85/200] | Train Loss: 0.2570, Train Acc: 0.8941 | \nEpoch [86/200] | Train Loss: 0.2515, Train Acc: 0.9129 | \nEpoch [87/200] | Train Loss: 0.2521, Train Acc: 0.9059 | \nEpoch [88/200] | Train Loss: 0.2328, Train Acc: 0.9200 | \nEpoch [89/200] | Train Loss: 0.2241, Train Acc: 0.9200 | \nEpoch [90/200] | Train Loss: 0.2384, Train Acc: 0.9012 | \nEpoch [91/200] | Train Loss: 0.2045, Train Acc: 0.9176 | \nEpoch [92/200] | Train Loss: 0.2344, Train Acc: 0.8965 | \nEpoch [93/200] | Train Loss: 0.2114, Train Acc: 0.9176 | \nEpoch [94/200] | Train Loss: 0.2181, Train Acc: 0.9153 | \nEpoch [95/200] | Train Loss: 0.2425, Train Acc: 0.9012 | \nEpoch [96/200] | Train Loss: 0.2267, Train Acc: 0.9082 | \nEpoch [97/200] | Train Loss: 0.2503, Train Acc: 0.8918 | \nEpoch [98/200] | Train Loss: 0.2391, Train Acc: 0.9082 | \nEpoch [99/200] | Train Loss: 0.2516, Train Acc: 0.9082 | \nEpoch [100/200] | Train Loss: 0.2267, Train Acc: 0.9106 | \nEpoch [101/200] | Train Loss: 0.2179, Train Acc: 0.9200 | \nEpoch 00102: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [102/200] | Train Loss: 0.2281, Train Acc: 0.9224 | \nEpoch [103/200] | Train Loss: 0.1981, Train Acc: 0.9271 | \nEpoch [104/200] | Train Loss: 0.2173, Train Acc: 0.9200 | \nEpoch [105/200] | Train Loss: 0.2188, Train Acc: 0.9035 | \nEpoch [106/200] | Train Loss: 0.2183, Train Acc: 0.9200 | \nEpoch [107/200] | Train Loss: 0.2191, Train Acc: 0.9176 | \nEpoch [108/200] | Train Loss: 0.2103, Train Acc: 0.9153 | \nEpoch [109/200] | Train Loss: 0.2230, Train Acc: 0.9035 | \nEpoch [110/200] | Train Loss: 0.2242, Train Acc: 0.9059 | \nEpoch [111/200] | Train Loss: 0.2113, Train Acc: 0.9200 | \nEpoch [112/200] | Train Loss: 0.2149, Train Acc: 0.9200 | \nEpoch [113/200] | Train Loss: 0.2362, Train Acc: 0.9059 | \nEpoch 00114: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [114/200] | Train Loss: 0.2147, Train Acc: 0.9129 | \nEpoch [115/200] | Train Loss: 0.2111, Train Acc: 0.9271 | \nEpoch [116/200] | Train Loss: 0.2140, Train Acc: 0.9153 | \nEpoch [117/200] | Train Loss: 0.2112, Train Acc: 0.9224 | \nEpoch [118/200] | Train Loss: 0.2227, Train Acc: 0.9129 | \nEpoch [119/200] | Train Loss: 0.2101, Train Acc: 0.9224 | \nEpoch [120/200] | Train Loss: 0.2111, Train Acc: 0.9200 | \nEpoch [121/200] | Train Loss: 0.2193, Train Acc: 0.9176 | \nEpoch [122/200] | Train Loss: 0.2105, Train Acc: 0.9247 | \nEarly stopping triggered.\nTest Loss: 0.3951, Test Accuracy: 0.8131, Test AUC: 0.8732\n\n--- Processing: psd_alpha ---\nShape: (532, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7102, Train Acc: 0.5435 | \nEpoch [2/200] | Train Loss: 0.6063, Train Acc: 0.6871 | \nEpoch [3/200] | Train Loss: 0.5231, Train Acc: 0.7294 | \nEpoch [4/200] | Train Loss: 0.4962, Train Acc: 0.7647 | \nEpoch [5/200] | Train Loss: 0.4787, Train Acc: 0.7694 | \nEpoch [6/200] | Train Loss: 0.5148, Train Acc: 0.7576 | \nEpoch [7/200] | Train Loss: 0.5735, Train Acc: 0.7082 | \nEpoch [8/200] | Train Loss: 0.5209, Train Acc: 0.7647 | \nEpoch [9/200] | Train Loss: 0.5656, Train Acc: 0.7294 | \nEpoch [10/200] | Train Loss: 0.4863, Train Acc: 0.7882 | \nEpoch [11/200] | Train Loss: 0.5041, Train Acc: 0.7412 | \nEpoch [12/200] | Train Loss: 0.5139, Train Acc: 0.7412 | \nEpoch [13/200] | Train Loss: 0.5188, Train Acc: 0.7294 | \nEpoch [14/200] | Train Loss: 0.4692, Train Acc: 0.7671 | \nEpoch [15/200] | Train Loss: 0.4570, Train Acc: 0.7859 | \nEpoch [16/200] | Train Loss: 0.4748, Train Acc: 0.7553 | \nEpoch [17/200] | Train Loss: 0.4369, Train Acc: 0.7859 | \nEpoch [18/200] | Train Loss: 0.4319, Train Acc: 0.7906 | \nEpoch [19/200] | Train Loss: 0.4164, Train Acc: 0.8047 | \nEpoch [20/200] | Train Loss: 0.4282, Train Acc: 0.7929 | \nEpoch [21/200] | Train Loss: 0.4390, Train Acc: 0.7647 | \nEpoch [22/200] | Train Loss: 0.4284, Train Acc: 0.8000 | \nEpoch [23/200] | Train Loss: 0.4716, Train Acc: 0.7529 | \nEpoch [24/200] | Train Loss: 0.4446, Train Acc: 0.7859 | \nEpoch [25/200] | Train Loss: 0.4296, Train Acc: 0.7906 | \nEpoch [26/200] | Train Loss: 0.4203, Train Acc: 0.8094 | \nEpoch [27/200] | Train Loss: 0.4193, Train Acc: 0.8212 | \nEpoch [28/200] | Train Loss: 0.4155, Train Acc: 0.8141 | \nEpoch [29/200] | Train Loss: 0.3946, Train Acc: 0.8118 | \nEpoch [30/200] | Train Loss: 0.4074, Train Acc: 0.8141 | \nEpoch [31/200] | Train Loss: 0.3989, Train Acc: 0.8282 | \nEpoch [32/200] | Train Loss: 0.4395, Train Acc: 0.8141 | \nEpoch [33/200] | Train Loss: 0.4321, Train Acc: 0.7929 | \nEpoch [34/200] | Train Loss: 0.4335, Train Acc: 0.7976 | \nEpoch [35/200] | Train Loss: 0.4234, Train Acc: 0.7976 | \nEpoch [36/200] | Train Loss: 0.4205, Train Acc: 0.8118 | \nEpoch [37/200] | Train Loss: 0.4095, Train Acc: 0.8212 | \nEpoch [38/200] | Train Loss: 0.4633, Train Acc: 0.7788 | \nEpoch [39/200] | Train Loss: 0.4452, Train Acc: 0.7906 | \nEpoch [40/200] | Train Loss: 0.3847, Train Acc: 0.8165 | \nEpoch [41/200] | Train Loss: 0.3912, Train Acc: 0.8141 | \nEpoch [42/200] | Train Loss: 0.4117, Train Acc: 0.8165 | \nEpoch [43/200] | Train Loss: 0.4138, Train Acc: 0.8047 | \nEpoch [44/200] | Train Loss: 0.3821, Train Acc: 0.8259 | \nEpoch [45/200] | Train Loss: 0.3829, Train Acc: 0.8282 | \nEpoch [46/200] | Train Loss: 0.4126, Train Acc: 0.7953 | \nEpoch [47/200] | Train Loss: 0.4233, Train Acc: 0.7882 | \nEpoch [48/200] | Train Loss: 0.3767, Train Acc: 0.8282 | \nEpoch [49/200] | Train Loss: 0.3664, Train Acc: 0.8259 | \nEpoch [50/200] | Train Loss: 0.3748, Train Acc: 0.8259 | \nEpoch [51/200] | Train Loss: 0.3710, Train Acc: 0.8306 | \nEpoch [52/200] | Train Loss: 0.3532, Train Acc: 0.8376 | \nEpoch [53/200] | Train Loss: 0.3526, Train Acc: 0.8424 | \nEpoch [54/200] | Train Loss: 0.3580, Train Acc: 0.8353 | \nEpoch [55/200] | Train Loss: 0.3631, Train Acc: 0.8400 | \nEpoch [56/200] | Train Loss: 0.3875, Train Acc: 0.8118 | \nEpoch [57/200] | Train Loss: 0.3698, Train Acc: 0.8306 | \nEpoch [58/200] | Train Loss: 0.3409, Train Acc: 0.8424 | \nEpoch [59/200] | Train Loss: 0.4900, Train Acc: 0.7718 | \nEpoch [60/200] | Train Loss: 0.5233, Train Acc: 0.7788 | \nEpoch [61/200] | Train Loss: 0.4707, Train Acc: 0.7929 | \nEpoch [62/200] | Train Loss: 0.4258, Train Acc: 0.7976 | \nEpoch [63/200] | Train Loss: 0.4022, Train Acc: 0.8118 | \nEpoch [64/200] | Train Loss: 0.4336, Train Acc: 0.8024 | \nEpoch [65/200] | Train Loss: 0.4574, Train Acc: 0.8000 | \nEpoch [66/200] | Train Loss: 0.4296, Train Acc: 0.8000 | \nEpoch [67/200] | Train Loss: 0.3970, Train Acc: 0.8282 | \nEpoch [68/200] | Train Loss: 0.4661, Train Acc: 0.7835 | \nEpoch 00069: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [69/200] | Train Loss: 0.4549, Train Acc: 0.7882 | \nEpoch [70/200] | Train Loss: 0.3906, Train Acc: 0.8212 | \nEpoch [71/200] | Train Loss: 0.3967, Train Acc: 0.8259 | \nEpoch [72/200] | Train Loss: 0.3669, Train Acc: 0.8447 | \nEpoch [73/200] | Train Loss: 0.3761, Train Acc: 0.8471 | \nEpoch [74/200] | Train Loss: 0.3720, Train Acc: 0.8424 | \nEpoch [75/200] | Train Loss: 0.3770, Train Acc: 0.8329 | \nEpoch [76/200] | Train Loss: 0.3680, Train Acc: 0.8424 | \nEpoch [77/200] | Train Loss: 0.3594, Train Acc: 0.8447 | \nEpoch [78/200] | Train Loss: 0.3495, Train Acc: 0.8588 | \nEpoch [79/200] | Train Loss: 0.3602, Train Acc: 0.8541 | \nEpoch 00080: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [80/200] | Train Loss: 0.3547, Train Acc: 0.8541 | \nEpoch [81/200] | Train Loss: 0.3566, Train Acc: 0.8494 | \nEpoch [82/200] | Train Loss: 0.3512, Train Acc: 0.8494 | \nEpoch [83/200] | Train Loss: 0.3658, Train Acc: 0.8494 | \nEpoch [84/200] | Train Loss: 0.3633, Train Acc: 0.8447 | \nEpoch [85/200] | Train Loss: 0.3677, Train Acc: 0.8353 | \nEpoch [86/200] | Train Loss: 0.3610, Train Acc: 0.8424 | \nEpoch [87/200] | Train Loss: 0.3588, Train Acc: 0.8400 | \nEpoch [88/200] | Train Loss: 0.3576, Train Acc: 0.8376 | \nEpoch [89/200] | Train Loss: 0.3623, Train Acc: 0.8400 | \nEpoch [90/200] | Train Loss: 0.3735, Train Acc: 0.8353 | \nEpoch 00091: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [91/200] | Train Loss: 0.3600, Train Acc: 0.8353 | \nEpoch [92/200] | Train Loss: 0.3511, Train Acc: 0.8541 | \nEpoch [93/200] | Train Loss: 0.3690, Train Acc: 0.8424 | \nEpoch [94/200] | Train Loss: 0.3604, Train Acc: 0.8471 | \nEpoch [95/200] | Train Loss: 0.3666, Train Acc: 0.8471 | \nEpoch [96/200] | Train Loss: 0.3431, Train Acc: 0.8612 | \nEpoch [97/200] | Train Loss: 0.3674, Train Acc: 0.8376 | \nEpoch [98/200] | Train Loss: 0.3588, Train Acc: 0.8471 | \nEpoch [99/200] | Train Loss: 0.3572, Train Acc: 0.8471 | \nEpoch [100/200] | Train Loss: 0.3557, Train Acc: 0.8494 | \nEpoch [101/200] | Train Loss: 0.3549, Train Acc: 0.8424 | \nEpoch 00102: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [102/200] | Train Loss: 0.3627, Train Acc: 0.8376 | \nEpoch [103/200] | Train Loss: 0.3611, Train Acc: 0.8424 | \nEpoch [104/200] | Train Loss: 0.3495, Train Acc: 0.8635 | \nEpoch [105/200] | Train Loss: 0.3456, Train Acc: 0.8565 | \nEpoch [106/200] | Train Loss: 0.3517, Train Acc: 0.8541 | \nEpoch [107/200] | Train Loss: 0.3583, Train Acc: 0.8376 | \nEpoch [108/200] | Train Loss: 0.3654, Train Acc: 0.8400 | \nEpoch [109/200] | Train Loss: 0.3664, Train Acc: 0.8494 | \nEpoch [110/200] | Train Loss: 0.3673, Train Acc: 0.8376 | \nEpoch [111/200] | Train Loss: 0.3564, Train Acc: 0.8471 | \nEpoch [112/200] | Train Loss: 0.3502, Train Acc: 0.8518 | \nEpoch 00113: reducing learning rate of group 0 to 1.0000e-08.\nEpoch [113/200] | Train Loss: 0.3651, Train Acc: 0.8376 | \nEpoch [114/200] | Train Loss: 0.3563, Train Acc: 0.8541 | \nEpoch [115/200] | Train Loss: 0.3585, Train Acc: 0.8518 | \nEpoch [116/200] | Train Loss: 0.3515, Train Acc: 0.8518 | \nEpoch [117/200] | Train Loss: 0.3572, Train Acc: 0.8424 | \nEpoch [118/200] | Train Loss: 0.3533, Train Acc: 0.8424 | \nEpoch [119/200] | Train Loss: 0.3554, Train Acc: 0.8494 | \nEpoch [120/200] | Train Loss: 0.3605, Train Acc: 0.8494 | \nEpoch [121/200] | Train Loss: 0.3515, Train Acc: 0.8471 | \nEpoch [122/200] | Train Loss: 0.3689, Train Acc: 0.8471 | \nEpoch [123/200] | Train Loss: 0.3494, Train Acc: 0.8541 | \nEarly stopping triggered.\nTest Loss: 0.3884, Test Accuracy: 0.8318, Test AUC: 0.8721\n\n--- Processing: psd_beta ---\nShape: (532, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7078, Train Acc: 0.5694 | \nEpoch [2/200] | Train Loss: 0.5536, Train Acc: 0.7153 | \nEpoch [3/200] | Train Loss: 0.4968, Train Acc: 0.7647 | \nEpoch [4/200] | Train Loss: 0.4640, Train Acc: 0.7859 | \nEpoch [5/200] | Train Loss: 0.4205, Train Acc: 0.8094 | \nEpoch [6/200] | Train Loss: 0.4417, Train Acc: 0.7953 | \nEpoch [7/200] | Train Loss: 0.4090, Train Acc: 0.8165 | \nEpoch [8/200] | Train Loss: 0.3884, Train Acc: 0.8329 | \nEpoch [9/200] | Train Loss: 0.4091, Train Acc: 0.8212 | \nEpoch [10/200] | Train Loss: 0.3995, Train Acc: 0.8165 | \nEpoch [11/200] | Train Loss: 0.4281, Train Acc: 0.8024 | \nEpoch [12/200] | Train Loss: 0.3732, Train Acc: 0.8376 | \nEpoch [13/200] | Train Loss: 0.4442, Train Acc: 0.8000 | \nEpoch [14/200] | Train Loss: 0.4288, Train Acc: 0.8071 | \nEpoch [15/200] | Train Loss: 0.4415, Train Acc: 0.7882 | \nEpoch [16/200] | Train Loss: 0.3727, Train Acc: 0.8447 | \nEpoch [17/200] | Train Loss: 0.3657, Train Acc: 0.8518 | \nEpoch [18/200] | Train Loss: 0.4202, Train Acc: 0.8141 | \nEpoch [19/200] | Train Loss: 0.4634, Train Acc: 0.8000 | \nEpoch [20/200] | Train Loss: 0.3923, Train Acc: 0.8259 | \nEpoch [21/200] | Train Loss: 0.3667, Train Acc: 0.8329 | \nEpoch [22/200] | Train Loss: 0.3759, Train Acc: 0.8353 | \nEpoch [23/200] | Train Loss: 0.3874, Train Acc: 0.8376 | \nEpoch [24/200] | Train Loss: 0.3483, Train Acc: 0.8541 | \nEpoch [25/200] | Train Loss: 0.3769, Train Acc: 0.8424 | \nEpoch [26/200] | Train Loss: 0.3499, Train Acc: 0.8424 | \nEpoch [27/200] | Train Loss: 0.3597, Train Acc: 0.8541 | \nEpoch [28/200] | Train Loss: 0.3349, Train Acc: 0.8659 | \nEpoch [29/200] | Train Loss: 0.3336, Train Acc: 0.8635 | \nEpoch [30/200] | Train Loss: 0.3792, Train Acc: 0.8376 | \nEpoch [31/200] | Train Loss: 0.3544, Train Acc: 0.8541 | \nEpoch [32/200] | Train Loss: 0.3384, Train Acc: 0.8588 | \nEpoch [33/200] | Train Loss: 0.3325, Train Acc: 0.8776 | \nEpoch [34/200] | Train Loss: 0.3168, Train Acc: 0.8635 | \nEpoch [35/200] | Train Loss: 0.3165, Train Acc: 0.8753 | \nEpoch [36/200] | Train Loss: 0.3226, Train Acc: 0.8635 | \nEpoch [37/200] | Train Loss: 0.3325, Train Acc: 0.8588 | \nEpoch [38/200] | Train Loss: 0.3536, Train Acc: 0.8588 | \nEpoch [39/200] | Train Loss: 0.3238, Train Acc: 0.8800 | \nEpoch [40/200] | Train Loss: 0.3364, Train Acc: 0.8635 | \nEpoch [41/200] | Train Loss: 0.3345, Train Acc: 0.8541 | \nEpoch [42/200] | Train Loss: 0.2979, Train Acc: 0.8894 | \nEpoch [43/200] | Train Loss: 0.3308, Train Acc: 0.8635 | \nEpoch [44/200] | Train Loss: 0.2981, Train Acc: 0.8918 | \nEpoch [45/200] | Train Loss: 0.3153, Train Acc: 0.8706 | \nEpoch [46/200] | Train Loss: 0.2981, Train Acc: 0.8729 | \nEpoch [47/200] | Train Loss: 0.2820, Train Acc: 0.8941 | \nEpoch [48/200] | Train Loss: 0.3187, Train Acc: 0.8706 | \nEpoch [49/200] | Train Loss: 0.3025, Train Acc: 0.8800 | \nEpoch [50/200] | Train Loss: 0.3062, Train Acc: 0.8847 | \nEpoch [51/200] | Train Loss: 0.3163, Train Acc: 0.8753 | \nEpoch [52/200] | Train Loss: 0.3090, Train Acc: 0.8776 | \nEpoch [53/200] | Train Loss: 0.3164, Train Acc: 0.8824 | \nEpoch [54/200] | Train Loss: 0.2936, Train Acc: 0.8800 | \nEpoch [55/200] | Train Loss: 0.3339, Train Acc: 0.8494 | \nEpoch [56/200] | Train Loss: 0.3560, Train Acc: 0.8588 | \nEpoch [57/200] | Train Loss: 0.3524, Train Acc: 0.8471 | \nEpoch 00058: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [58/200] | Train Loss: 0.3359, Train Acc: 0.8729 | \nEpoch [59/200] | Train Loss: 0.3174, Train Acc: 0.8682 | \nEpoch [60/200] | Train Loss: 0.2968, Train Acc: 0.8706 | \nEpoch [61/200] | Train Loss: 0.2851, Train Acc: 0.8753 | \nEpoch [62/200] | Train Loss: 0.2832, Train Acc: 0.8824 | \nEpoch [63/200] | Train Loss: 0.2574, Train Acc: 0.8988 | \nEpoch [64/200] | Train Loss: 0.2690, Train Acc: 0.8871 | \nEpoch [65/200] | Train Loss: 0.2617, Train Acc: 0.8918 | \nEpoch [66/200] | Train Loss: 0.2508, Train Acc: 0.9059 | \nEpoch [67/200] | Train Loss: 0.2399, Train Acc: 0.9012 | \nEpoch [68/200] | Train Loss: 0.2456, Train Acc: 0.9012 | \nEpoch [69/200] | Train Loss: 0.2440, Train Acc: 0.9059 | \nEpoch [70/200] | Train Loss: 0.2325, Train Acc: 0.9082 | \nEpoch [71/200] | Train Loss: 0.2324, Train Acc: 0.9035 | \nEpoch [72/200] | Train Loss: 0.2300, Train Acc: 0.9176 | \nEpoch [73/200] | Train Loss: 0.2382, Train Acc: 0.9106 | \nEpoch [74/200] | Train Loss: 0.2307, Train Acc: 0.9059 | \nEpoch [75/200] | Train Loss: 0.2178, Train Acc: 0.9129 | \nEpoch [76/200] | Train Loss: 0.2341, Train Acc: 0.9082 | \nEpoch [77/200] | Train Loss: 0.2307, Train Acc: 0.9129 | \nEpoch [78/200] | Train Loss: 0.2305, Train Acc: 0.9153 | \nEpoch [79/200] | Train Loss: 0.2235, Train Acc: 0.9153 | \nEpoch [80/200] | Train Loss: 0.2156, Train Acc: 0.9176 | \nEpoch [81/200] | Train Loss: 0.2153, Train Acc: 0.9153 | \nEpoch [82/200] | Train Loss: 0.2138, Train Acc: 0.9294 | \nEpoch [83/200] | Train Loss: 0.2259, Train Acc: 0.9012 | \nEpoch [84/200] | Train Loss: 0.2192, Train Acc: 0.9176 | \nEpoch [85/200] | Train Loss: 0.2117, Train Acc: 0.9129 | \nEpoch [86/200] | Train Loss: 0.2016, Train Acc: 0.9224 | \nEpoch [87/200] | Train Loss: 0.2150, Train Acc: 0.9176 | \nEpoch [88/200] | Train Loss: 0.2124, Train Acc: 0.9153 | \nEpoch [89/200] | Train Loss: 0.1991, Train Acc: 0.9271 | \nEpoch [90/200] | Train Loss: 0.2039, Train Acc: 0.9200 | \nEpoch [91/200] | Train Loss: 0.2076, Train Acc: 0.9341 | \nEpoch [92/200] | Train Loss: 0.2217, Train Acc: 0.9082 | \nEpoch [93/200] | Train Loss: 0.2024, Train Acc: 0.9318 | \nEpoch [94/200] | Train Loss: 0.2081, Train Acc: 0.9176 | \nEpoch [95/200] | Train Loss: 0.2071, Train Acc: 0.9224 | \nEpoch [96/200] | Train Loss: 0.2199, Train Acc: 0.9129 | \nEpoch [97/200] | Train Loss: 0.1885, Train Acc: 0.9294 | \nEpoch [98/200] | Train Loss: 0.1981, Train Acc: 0.9224 | \nEpoch [99/200] | Train Loss: 0.1982, Train Acc: 0.9318 | \nEpoch [100/200] | Train Loss: 0.1957, Train Acc: 0.9247 | \nEpoch [101/200] | Train Loss: 0.1862, Train Acc: 0.9294 | \nEpoch [102/200] | Train Loss: 0.1924, Train Acc: 0.9176 | \nEpoch [103/200] | Train Loss: 0.2122, Train Acc: 0.8965 | \nEpoch [104/200] | Train Loss: 0.1929, Train Acc: 0.9247 | \nEpoch [105/200] | Train Loss: 0.2068, Train Acc: 0.9106 | \nEpoch [106/200] | Train Loss: 0.2015, Train Acc: 0.9224 | \nEpoch [107/200] | Train Loss: 0.1932, Train Acc: 0.9224 | \nEpoch [108/200] | Train Loss: 0.1920, Train Acc: 0.9365 | \nEpoch [109/200] | Train Loss: 0.1819, Train Acc: 0.9294 | \nEpoch [110/200] | Train Loss: 0.1870, Train Acc: 0.9294 | \nEpoch [111/200] | Train Loss: 0.1835, Train Acc: 0.9294 | \nEpoch [112/200] | Train Loss: 0.2031, Train Acc: 0.9200 | \nEpoch [113/200] | Train Loss: 0.2052, Train Acc: 0.9176 | \nEpoch [114/200] | Train Loss: 0.1876, Train Acc: 0.9318 | \nEpoch [115/200] | Train Loss: 0.2053, Train Acc: 0.9176 | \nEpoch [116/200] | Train Loss: 0.1941, Train Acc: 0.9176 | \nEpoch [117/200] | Train Loss: 0.1804, Train Acc: 0.9247 | \nEpoch [118/200] | Train Loss: 0.1982, Train Acc: 0.9247 | \nEpoch [119/200] | Train Loss: 0.1835, Train Acc: 0.9294 | \nEpoch [120/200] | Train Loss: 0.1745, Train Acc: 0.9271 | \nEpoch [121/200] | Train Loss: 0.1823, Train Acc: 0.9271 | \nEpoch [122/200] | Train Loss: 0.1690, Train Acc: 0.9365 | \nEpoch [123/200] | Train Loss: 0.1777, Train Acc: 0.9341 | \nEpoch [124/200] | Train Loss: 0.1643, Train Acc: 0.9482 | \nEpoch [125/200] | Train Loss: 0.1794, Train Acc: 0.9271 | \nEpoch [126/200] | Train Loss: 0.1757, Train Acc: 0.9247 | \nEpoch [127/200] | Train Loss: 0.1820, Train Acc: 0.9388 | \nEpoch [128/200] | Train Loss: 0.1644, Train Acc: 0.9482 | \nEpoch [129/200] | Train Loss: 0.1652, Train Acc: 0.9482 | \nEpoch [130/200] | Train Loss: 0.1518, Train Acc: 0.9553 | \nEpoch [131/200] | Train Loss: 0.1581, Train Acc: 0.9318 | \nEpoch [132/200] | Train Loss: 0.1637, Train Acc: 0.9388 | \nEpoch [133/200] | Train Loss: 0.1738, Train Acc: 0.9341 | \nEpoch [134/200] | Train Loss: 0.1629, Train Acc: 0.9365 | \nEpoch [135/200] | Train Loss: 0.1611, Train Acc: 0.9365 | \nEpoch [136/200] | Train Loss: 0.1608, Train Acc: 0.9388 | \nEpoch [137/200] | Train Loss: 0.1657, Train Acc: 0.9365 | \nEpoch [138/200] | Train Loss: 0.1555, Train Acc: 0.9482 | \nEpoch [139/200] | Train Loss: 0.1573, Train Acc: 0.9341 | \nEpoch [140/200] | Train Loss: 0.1562, Train Acc: 0.9459 | \nEpoch 00141: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [141/200] | Train Loss: 0.1582, Train Acc: 0.9365 | \nEpoch [142/200] | Train Loss: 0.1628, Train Acc: 0.9388 | \nEpoch [143/200] | Train Loss: 0.1430, Train Acc: 0.9365 | \nEpoch [144/200] | Train Loss: 0.1526, Train Acc: 0.9388 | \nEpoch [145/200] | Train Loss: 0.1788, Train Acc: 0.9271 | \nEpoch [146/200] | Train Loss: 0.1549, Train Acc: 0.9435 | \nEpoch [147/200] | Train Loss: 0.1472, Train Acc: 0.9412 | \nEpoch [148/200] | Train Loss: 0.1453, Train Acc: 0.9506 | \nEpoch [149/200] | Train Loss: 0.1520, Train Acc: 0.9553 | \nEarly stopping triggered.\nTest Loss: 0.3300, Test Accuracy: 0.8785, Test AUC: 0.9312\n\n--- Processing: psd_highbeta ---\nShape: (532, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7272, Train Acc: 0.5341 | \nEpoch [2/200] | Train Loss: 0.5604, Train Acc: 0.7294 | \nEpoch [3/200] | Train Loss: 0.4905, Train Acc: 0.7741 | \nEpoch [4/200] | Train Loss: 0.5001, Train Acc: 0.7294 | \nEpoch [5/200] | Train Loss: 0.4832, Train Acc: 0.7718 | \nEpoch [6/200] | Train Loss: 0.5287, Train Acc: 0.7506 | \nEpoch [7/200] | Train Loss: 0.5116, Train Acc: 0.7388 | \nEpoch [8/200] | Train Loss: 0.4755, Train Acc: 0.7624 | \nEpoch [9/200] | Train Loss: 0.4927, Train Acc: 0.7647 | \nEpoch [10/200] | Train Loss: 0.4755, Train Acc: 0.7388 | \nEpoch [11/200] | Train Loss: 0.4132, Train Acc: 0.8024 | \nEpoch [12/200] | Train Loss: 0.4353, Train Acc: 0.7906 | \nEpoch [13/200] | Train Loss: 0.3841, Train Acc: 0.8118 | \nEpoch [14/200] | Train Loss: 0.4194, Train Acc: 0.8188 | \nEpoch [15/200] | Train Loss: 0.3868, Train Acc: 0.8329 | \nEpoch [16/200] | Train Loss: 0.5578, Train Acc: 0.7200 | \nEpoch [17/200] | Train Loss: 0.4828, Train Acc: 0.7812 | \nEpoch [18/200] | Train Loss: 0.4455, Train Acc: 0.8000 | \nEpoch [19/200] | Train Loss: 0.4127, Train Acc: 0.8118 | \nEpoch [20/200] | Train Loss: 0.3833, Train Acc: 0.8376 | \nEpoch [21/200] | Train Loss: 0.3877, Train Acc: 0.8235 | \nEpoch [22/200] | Train Loss: 0.3628, Train Acc: 0.8447 | \nEpoch [23/200] | Train Loss: 0.3713, Train Acc: 0.8424 | \nEpoch [24/200] | Train Loss: 0.3591, Train Acc: 0.8424 | \nEpoch [25/200] | Train Loss: 0.3580, Train Acc: 0.8447 | \nEpoch [26/200] | Train Loss: 0.3317, Train Acc: 0.8612 | \nEpoch [27/200] | Train Loss: 0.3856, Train Acc: 0.8306 | \nEpoch [28/200] | Train Loss: 0.3771, Train Acc: 0.8329 | \nEpoch [29/200] | Train Loss: 0.3528, Train Acc: 0.8471 | \nEpoch [30/200] | Train Loss: 0.3377, Train Acc: 0.8329 | \nEpoch [31/200] | Train Loss: 0.3614, Train Acc: 0.8306 | \nEpoch [32/200] | Train Loss: 0.3452, Train Acc: 0.8471 | \nEpoch [33/200] | Train Loss: 0.3156, Train Acc: 0.8753 | \nEpoch [34/200] | Train Loss: 0.3592, Train Acc: 0.8494 | \nEpoch [35/200] | Train Loss: 0.3749, Train Acc: 0.8565 | \nEpoch [36/200] | Train Loss: 0.3307, Train Acc: 0.8565 | \nEpoch [37/200] | Train Loss: 0.3313, Train Acc: 0.8565 | \nEpoch [38/200] | Train Loss: 0.3416, Train Acc: 0.8541 | \nEpoch [39/200] | Train Loss: 0.3036, Train Acc: 0.8682 | \nEpoch [40/200] | Train Loss: 0.2783, Train Acc: 0.8965 | \nEpoch [41/200] | Train Loss: 0.2654, Train Acc: 0.9012 | \nEpoch [42/200] | Train Loss: 0.3474, Train Acc: 0.8471 | \nEpoch [43/200] | Train Loss: 0.3164, Train Acc: 0.8612 | \nEpoch [44/200] | Train Loss: 0.3865, Train Acc: 0.8329 | \nEpoch [45/200] | Train Loss: 0.2932, Train Acc: 0.8753 | \nEpoch [46/200] | Train Loss: 0.2657, Train Acc: 0.8918 | \nEpoch [47/200] | Train Loss: 0.2711, Train Acc: 0.8894 | \nEpoch [48/200] | Train Loss: 0.2725, Train Acc: 0.8941 | \nEpoch [49/200] | Train Loss: 0.2803, Train Acc: 0.8941 | \nEpoch [50/200] | Train Loss: 0.2943, Train Acc: 0.8753 | \nEpoch [51/200] | Train Loss: 0.3243, Train Acc: 0.8612 | \nEpoch 00052: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [52/200] | Train Loss: 0.2807, Train Acc: 0.8894 | \nEpoch [53/200] | Train Loss: 0.2452, Train Acc: 0.9012 | \nEpoch [54/200] | Train Loss: 0.2366, Train Acc: 0.9106 | \nEpoch [55/200] | Train Loss: 0.2346, Train Acc: 0.9129 | \nEpoch [56/200] | Train Loss: 0.2251, Train Acc: 0.9153 | \nEpoch [57/200] | Train Loss: 0.2180, Train Acc: 0.9129 | \nEpoch [58/200] | Train Loss: 0.2344, Train Acc: 0.9106 | \nEpoch [59/200] | Train Loss: 0.2149, Train Acc: 0.9176 | \nEpoch [60/200] | Train Loss: 0.2147, Train Acc: 0.9224 | \nEpoch [61/200] | Train Loss: 0.2131, Train Acc: 0.9200 | \nEpoch [62/200] | Train Loss: 0.2026, Train Acc: 0.9318 | \nEpoch [63/200] | Train Loss: 0.2197, Train Acc: 0.9153 | \nEpoch [64/200] | Train Loss: 0.2012, Train Acc: 0.9200 | \nEpoch [65/200] | Train Loss: 0.2007, Train Acc: 0.9247 | \nEpoch [66/200] | Train Loss: 0.1851, Train Acc: 0.9365 | \nEpoch [67/200] | Train Loss: 0.2049, Train Acc: 0.9200 | \nEpoch [68/200] | Train Loss: 0.2006, Train Acc: 0.9271 | \nEpoch [69/200] | Train Loss: 0.1904, Train Acc: 0.9365 | \nEpoch [70/200] | Train Loss: 0.1961, Train Acc: 0.9176 | \nEpoch [71/200] | Train Loss: 0.1893, Train Acc: 0.9294 | \nEpoch [72/200] | Train Loss: 0.1800, Train Acc: 0.9294 | \nEpoch [73/200] | Train Loss: 0.1714, Train Acc: 0.9412 | \nEpoch [74/200] | Train Loss: 0.1827, Train Acc: 0.9365 | \nEpoch [75/200] | Train Loss: 0.1764, Train Acc: 0.9365 | \nEpoch [76/200] | Train Loss: 0.1790, Train Acc: 0.9412 | \nEpoch [77/200] | Train Loss: 0.1738, Train Acc: 0.9388 | \nEpoch [78/200] | Train Loss: 0.1767, Train Acc: 0.9365 | \nEpoch [79/200] | Train Loss: 0.1672, Train Acc: 0.9435 | \nEpoch [80/200] | Train Loss: 0.1895, Train Acc: 0.9341 | \nEpoch [81/200] | Train Loss: 0.1476, Train Acc: 0.9482 | \nEpoch [82/200] | Train Loss: 0.1741, Train Acc: 0.9388 | \nEpoch [83/200] | Train Loss: 0.1789, Train Acc: 0.9271 | \nEpoch [84/200] | Train Loss: 0.1641, Train Acc: 0.9435 | \nEpoch [85/200] | Train Loss: 0.1675, Train Acc: 0.9435 | \nEpoch [86/200] | Train Loss: 0.1578, Train Acc: 0.9435 | \nEpoch [87/200] | Train Loss: 0.1484, Train Acc: 0.9506 | \nEpoch [88/200] | Train Loss: 0.1678, Train Acc: 0.9482 | \nEpoch [89/200] | Train Loss: 0.1544, Train Acc: 0.9553 | \nEpoch [90/200] | Train Loss: 0.1437, Train Acc: 0.9553 | \nEpoch [91/200] | Train Loss: 0.1581, Train Acc: 0.9412 | \nEpoch [92/200] | Train Loss: 0.1781, Train Acc: 0.9294 | \nEpoch [93/200] | Train Loss: 0.1623, Train Acc: 0.9341 | \nEpoch [94/200] | Train Loss: 0.1758, Train Acc: 0.9341 | \nEpoch [95/200] | Train Loss: 0.1462, Train Acc: 0.9553 | \nEpoch [96/200] | Train Loss: 0.1655, Train Acc: 0.9388 | \nEpoch [97/200] | Train Loss: 0.1546, Train Acc: 0.9388 | \nEpoch [98/200] | Train Loss: 0.1299, Train Acc: 0.9529 | \nEpoch [99/200] | Train Loss: 0.1484, Train Acc: 0.9506 | \nEpoch [100/200] | Train Loss: 0.1396, Train Acc: 0.9506 | \nEpoch [101/200] | Train Loss: 0.1480, Train Acc: 0.9529 | \nEpoch [102/200] | Train Loss: 0.1457, Train Acc: 0.9435 | \nEpoch [103/200] | Train Loss: 0.1337, Train Acc: 0.9600 | \nEpoch [104/200] | Train Loss: 0.1445, Train Acc: 0.9459 | \nEpoch [105/200] | Train Loss: 0.1290, Train Acc: 0.9553 | \nEpoch [106/200] | Train Loss: 0.1290, Train Acc: 0.9553 | \nEpoch [107/200] | Train Loss: 0.1375, Train Acc: 0.9482 | \nEpoch [108/200] | Train Loss: 0.1167, Train Acc: 0.9553 | \nEpoch [109/200] | Train Loss: 0.1253, Train Acc: 0.9576 | \nEpoch [110/200] | Train Loss: 0.0998, Train Acc: 0.9624 | \nEpoch [111/200] | Train Loss: 0.1309, Train Acc: 0.9576 | \nEpoch [112/200] | Train Loss: 0.1026, Train Acc: 0.9671 | \nEpoch [113/200] | Train Loss: 0.1096, Train Acc: 0.9576 | \nEpoch [114/200] | Train Loss: 0.1211, Train Acc: 0.9506 | \nEpoch [115/200] | Train Loss: 0.1076, Train Acc: 0.9624 | \nEpoch [116/200] | Train Loss: 0.1027, Train Acc: 0.9694 | \nEpoch [117/200] | Train Loss: 0.1166, Train Acc: 0.9600 | \nEpoch [118/200] | Train Loss: 0.0851, Train Acc: 0.9812 | \nEpoch [119/200] | Train Loss: 0.1165, Train Acc: 0.9624 | \nEpoch [120/200] | Train Loss: 0.0998, Train Acc: 0.9624 | \nEpoch [121/200] | Train Loss: 0.1170, Train Acc: 0.9647 | \nEpoch [122/200] | Train Loss: 0.1063, Train Acc: 0.9600 | \nEpoch [123/200] | Train Loss: 0.1157, Train Acc: 0.9576 | \nEpoch [124/200] | Train Loss: 0.1143, Train Acc: 0.9529 | \nEpoch [125/200] | Train Loss: 0.1123, Train Acc: 0.9600 | \nEpoch [126/200] | Train Loss: 0.0874, Train Acc: 0.9694 | \nEpoch [127/200] | Train Loss: 0.1053, Train Acc: 0.9600 | \nEpoch [128/200] | Train Loss: 0.1372, Train Acc: 0.9459 | \nEpoch 00129: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [129/200] | Train Loss: 0.0994, Train Acc: 0.9694 | \nEpoch [130/200] | Train Loss: 0.0878, Train Acc: 0.9741 | \nEpoch [131/200] | Train Loss: 0.0906, Train Acc: 0.9694 | \nEpoch [132/200] | Train Loss: 0.1022, Train Acc: 0.9600 | \nEpoch [133/200] | Train Loss: 0.0966, Train Acc: 0.9694 | \nEpoch [134/200] | Train Loss: 0.0824, Train Acc: 0.9741 | \nEpoch [135/200] | Train Loss: 0.0958, Train Acc: 0.9694 | \nEpoch [136/200] | Train Loss: 0.0963, Train Acc: 0.9741 | \nEpoch [137/200] | Train Loss: 0.0910, Train Acc: 0.9718 | \nEarly stopping triggered.\nTest Loss: 0.5009, Test Accuracy: 0.8411, Test AUC: 0.8819\n\n--- Processing: psd_gamma ---\nShape: (532, 19)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7252, Train Acc: 0.5200 | \nEpoch [2/200] | Train Loss: 0.6084, Train Acc: 0.6871 | \nEpoch [3/200] | Train Loss: 0.4685, Train Acc: 0.7788 | \nEpoch [4/200] | Train Loss: 0.5531, Train Acc: 0.7224 | \nEpoch [5/200] | Train Loss: 0.5319, Train Acc: 0.7341 | \nEpoch [6/200] | Train Loss: 0.4600, Train Acc: 0.7906 | \nEpoch [7/200] | Train Loss: 0.4769, Train Acc: 0.7835 | \nEpoch [8/200] | Train Loss: 0.4669, Train Acc: 0.7624 | \nEpoch [9/200] | Train Loss: 0.4319, Train Acc: 0.7953 | \nEpoch [10/200] | Train Loss: 0.4278, Train Acc: 0.8188 | \nEpoch [11/200] | Train Loss: 0.4512, Train Acc: 0.8024 | \nEpoch [12/200] | Train Loss: 0.4658, Train Acc: 0.7694 | \nEpoch [13/200] | Train Loss: 0.3866, Train Acc: 0.8282 | \nEpoch [14/200] | Train Loss: 0.3951, Train Acc: 0.8235 | \nEpoch [15/200] | Train Loss: 0.3775, Train Acc: 0.8235 | \nEpoch [16/200] | Train Loss: 0.3823, Train Acc: 0.8376 | \nEpoch [17/200] | Train Loss: 0.3935, Train Acc: 0.8259 | \nEpoch [18/200] | Train Loss: 0.3882, Train Acc: 0.8329 | \nEpoch [19/200] | Train Loss: 0.3879, Train Acc: 0.8424 | \nEpoch [20/200] | Train Loss: 0.3942, Train Acc: 0.8376 | \nEpoch [21/200] | Train Loss: 0.4167, Train Acc: 0.8165 | \nEpoch [22/200] | Train Loss: 0.3698, Train Acc: 0.8376 | \nEpoch [23/200] | Train Loss: 0.3578, Train Acc: 0.8518 | \nEpoch [24/200] | Train Loss: 0.3686, Train Acc: 0.8400 | \nEpoch [25/200] | Train Loss: 0.3566, Train Acc: 0.8400 | \nEpoch [26/200] | Train Loss: 0.3460, Train Acc: 0.8447 | \nEpoch [27/200] | Train Loss: 0.3744, Train Acc: 0.8188 | \nEpoch [28/200] | Train Loss: 0.3721, Train Acc: 0.8282 | \nEpoch [29/200] | Train Loss: 0.3763, Train Acc: 0.8306 | \nEpoch [30/200] | Train Loss: 0.3586, Train Acc: 0.8306 | \nEpoch [31/200] | Train Loss: 0.3888, Train Acc: 0.8165 | \nEpoch [32/200] | Train Loss: 0.3751, Train Acc: 0.8306 | \nEpoch [33/200] | Train Loss: 0.3937, Train Acc: 0.8259 | \nEpoch [34/200] | Train Loss: 0.3407, Train Acc: 0.8376 | \nEpoch [35/200] | Train Loss: 0.3220, Train Acc: 0.8612 | \nEpoch [36/200] | Train Loss: 0.3564, Train Acc: 0.8494 | \nEpoch [37/200] | Train Loss: 0.3284, Train Acc: 0.8659 | \nEpoch [38/200] | Train Loss: 0.3156, Train Acc: 0.8635 | \nEpoch [39/200] | Train Loss: 0.3145, Train Acc: 0.8635 | \nEpoch [40/200] | Train Loss: 0.3303, Train Acc: 0.8565 | \nEpoch [41/200] | Train Loss: 0.3693, Train Acc: 0.8376 | \nEpoch [42/200] | Train Loss: 0.3455, Train Acc: 0.8541 | \nEpoch [43/200] | Train Loss: 0.3231, Train Acc: 0.8541 | \nEpoch [44/200] | Train Loss: 0.3512, Train Acc: 0.8541 | \nEpoch [45/200] | Train Loss: 0.4167, Train Acc: 0.8306 | \nEpoch [46/200] | Train Loss: 0.3662, Train Acc: 0.8353 | \nEpoch [47/200] | Train Loss: 0.3605, Train Acc: 0.8235 | \nEpoch [48/200] | Train Loss: 0.3328, Train Acc: 0.8518 | \nEpoch [49/200] | Train Loss: 0.3320, Train Acc: 0.8588 | \nEpoch 00050: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [50/200] | Train Loss: 0.3388, Train Acc: 0.8588 | \nEpoch [51/200] | Train Loss: 0.3206, Train Acc: 0.8729 | \nEpoch [52/200] | Train Loss: 0.2959, Train Acc: 0.8894 | \nEpoch [53/200] | Train Loss: 0.2986, Train Acc: 0.8659 | \nEpoch [54/200] | Train Loss: 0.2823, Train Acc: 0.8965 | \nEpoch [55/200] | Train Loss: 0.2919, Train Acc: 0.8706 | \nEpoch [56/200] | Train Loss: 0.2897, Train Acc: 0.8753 | \nEpoch [57/200] | Train Loss: 0.2745, Train Acc: 0.8965 | \nEpoch [58/200] | Train Loss: 0.2735, Train Acc: 0.8965 | \nEpoch [59/200] | Train Loss: 0.2814, Train Acc: 0.8800 | \nEpoch [60/200] | Train Loss: 0.2682, Train Acc: 0.8918 | \nEpoch [61/200] | Train Loss: 0.2659, Train Acc: 0.8847 | \nEpoch [62/200] | Train Loss: 0.2723, Train Acc: 0.8941 | \nEpoch [63/200] | Train Loss: 0.2580, Train Acc: 0.9035 | \nEpoch [64/200] | Train Loss: 0.2677, Train Acc: 0.8894 | \nEpoch [65/200] | Train Loss: 0.2601, Train Acc: 0.9106 | \nEpoch [66/200] | Train Loss: 0.2490, Train Acc: 0.8965 | \nEpoch [67/200] | Train Loss: 0.2565, Train Acc: 0.9012 | \nEpoch [68/200] | Train Loss: 0.2460, Train Acc: 0.9059 | \nEpoch [69/200] | Train Loss: 0.2652, Train Acc: 0.8918 | \nEpoch [70/200] | Train Loss: 0.2447, Train Acc: 0.9059 | \nEpoch [71/200] | Train Loss: 0.2499, Train Acc: 0.9106 | \nEpoch [72/200] | Train Loss: 0.2538, Train Acc: 0.9082 | \nEpoch [73/200] | Train Loss: 0.2448, Train Acc: 0.9035 | \nEpoch [74/200] | Train Loss: 0.2573, Train Acc: 0.9082 | \nEpoch [75/200] | Train Loss: 0.2715, Train Acc: 0.8965 | \nEpoch [76/200] | Train Loss: 0.2563, Train Acc: 0.9012 | \nEpoch [77/200] | Train Loss: 0.2548, Train Acc: 0.9012 | \nEpoch [78/200] | Train Loss: 0.2398, Train Acc: 0.9153 | \nEpoch [79/200] | Train Loss: 0.2567, Train Acc: 0.9035 | \nEpoch [80/200] | Train Loss: 0.2395, Train Acc: 0.9106 | \nEpoch [81/200] | Train Loss: 0.2423, Train Acc: 0.8988 | \nEpoch [82/200] | Train Loss: 0.2419, Train Acc: 0.8965 | \nEpoch [83/200] | Train Loss: 0.2449, Train Acc: 0.9106 | \nEpoch [84/200] | Train Loss: 0.2502, Train Acc: 0.9012 | \nEpoch [85/200] | Train Loss: 0.2415, Train Acc: 0.9035 | \nEpoch [86/200] | Train Loss: 0.2372, Train Acc: 0.9153 | \nEpoch [87/200] | Train Loss: 0.2319, Train Acc: 0.9082 | \nEpoch [88/200] | Train Loss: 0.2461, Train Acc: 0.9059 | \nEpoch [89/200] | Train Loss: 0.2263, Train Acc: 0.9153 | \nEpoch [90/200] | Train Loss: 0.2321, Train Acc: 0.9129 | \nEpoch [91/200] | Train Loss: 0.2315, Train Acc: 0.9153 | \nEpoch [92/200] | Train Loss: 0.2258, Train Acc: 0.9059 | \nEpoch [93/200] | Train Loss: 0.2307, Train Acc: 0.9106 | \nEpoch [94/200] | Train Loss: 0.2205, Train Acc: 0.9153 | \nEpoch [95/200] | Train Loss: 0.2390, Train Acc: 0.9035 | \nEpoch [96/200] | Train Loss: 0.2441, Train Acc: 0.9082 | \nEpoch [97/200] | Train Loss: 0.2274, Train Acc: 0.9153 | \nEpoch [98/200] | Train Loss: 0.2118, Train Acc: 0.9271 | \nEpoch [99/200] | Train Loss: 0.2268, Train Acc: 0.9153 | \nEpoch [100/200] | Train Loss: 0.2527, Train Acc: 0.9059 | \nEpoch [101/200] | Train Loss: 0.2555, Train Acc: 0.8918 | \nEpoch [102/200] | Train Loss: 0.2194, Train Acc: 0.9176 | \nEpoch [103/200] | Train Loss: 0.2293, Train Acc: 0.9106 | \nEpoch [104/200] | Train Loss: 0.2160, Train Acc: 0.9082 | \nEpoch [105/200] | Train Loss: 0.2262, Train Acc: 0.9035 | \nEpoch [106/200] | Train Loss: 0.2094, Train Acc: 0.9153 | \nEpoch [107/200] | Train Loss: 0.2218, Train Acc: 0.9176 | \nEpoch [108/200] | Train Loss: 0.2239, Train Acc: 0.9176 | \nEpoch [109/200] | Train Loss: 0.2072, Train Acc: 0.9153 | \nEpoch [110/200] | Train Loss: 0.2391, Train Acc: 0.9176 | \nEpoch [111/200] | Train Loss: 0.1991, Train Acc: 0.9176 | \nEpoch [112/200] | Train Loss: 0.2341, Train Acc: 0.9082 | \nEpoch [113/200] | Train Loss: 0.2099, Train Acc: 0.9294 | \nEpoch [114/200] | Train Loss: 0.2174, Train Acc: 0.9129 | \nEpoch [115/200] | Train Loss: 0.2049, Train Acc: 0.9129 | \nEpoch [116/200] | Train Loss: 0.2034, Train Acc: 0.9200 | \nEpoch [117/200] | Train Loss: 0.2027, Train Acc: 0.9247 | \nEpoch [118/200] | Train Loss: 0.2122, Train Acc: 0.9224 | \nEpoch [119/200] | Train Loss: 0.2041, Train Acc: 0.9129 | \nEpoch [120/200] | Train Loss: 0.1915, Train Acc: 0.9271 | \nEpoch [121/200] | Train Loss: 0.2253, Train Acc: 0.9129 | \nEpoch [122/200] | Train Loss: 0.2386, Train Acc: 0.9059 | \nEpoch [123/200] | Train Loss: 0.2101, Train Acc: 0.9247 | \nEpoch [124/200] | Train Loss: 0.2187, Train Acc: 0.9129 | \nEpoch [125/200] | Train Loss: 0.1943, Train Acc: 0.9247 | \nEpoch [126/200] | Train Loss: 0.2126, Train Acc: 0.9200 | \nEpoch [127/200] | Train Loss: 0.2029, Train Acc: 0.9153 | \nEpoch [128/200] | Train Loss: 0.1899, Train Acc: 0.9224 | \nEpoch [129/200] | Train Loss: 0.2026, Train Acc: 0.9247 | \nEpoch [130/200] | Train Loss: 0.2053, Train Acc: 0.9271 | \nEpoch [131/200] | Train Loss: 0.2071, Train Acc: 0.9153 | \nEpoch [132/200] | Train Loss: 0.1969, Train Acc: 0.9271 | \nEarly stopping triggered.\nTest Loss: 0.3749, Test Accuracy: 0.8692, Test AUC: 0.9067\n\n--- Processing: fc_delta ---\nShape: (532, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7075, Train Acc: 0.5576 | \nEpoch [2/200] | Train Loss: 0.6938, Train Acc: 0.5035 | \nEpoch [3/200] | Train Loss: 0.6704, Train Acc: 0.5929 | \nEpoch [4/200] | Train Loss: 0.5719, Train Acc: 0.7318 | \nEpoch [5/200] | Train Loss: 0.4813, Train Acc: 0.7812 | \nEpoch [6/200] | Train Loss: 0.4387, Train Acc: 0.8118 | \nEpoch [7/200] | Train Loss: 0.3910, Train Acc: 0.8282 | \nEpoch [8/200] | Train Loss: 0.3532, Train Acc: 0.8565 | \nEpoch [9/200] | Train Loss: 0.3819, Train Acc: 0.8471 | \nEpoch [10/200] | Train Loss: 0.3492, Train Acc: 0.8541 | \nEpoch [11/200] | Train Loss: 0.3735, Train Acc: 0.8329 | \nEpoch [12/200] | Train Loss: 0.4039, Train Acc: 0.8235 | \nEpoch [13/200] | Train Loss: 0.3223, Train Acc: 0.8635 | \nEpoch [14/200] | Train Loss: 0.3467, Train Acc: 0.8400 | \nEpoch [15/200] | Train Loss: 0.3157, Train Acc: 0.8659 | \nEpoch [16/200] | Train Loss: 0.3141, Train Acc: 0.8706 | \nEpoch [17/200] | Train Loss: 0.3088, Train Acc: 0.8729 | \nEpoch [18/200] | Train Loss: 0.2926, Train Acc: 0.8824 | \nEpoch [19/200] | Train Loss: 0.3172, Train Acc: 0.8612 | \nEpoch [20/200] | Train Loss: 0.2927, Train Acc: 0.8824 | \nEpoch [21/200] | Train Loss: 0.3962, Train Acc: 0.8541 | \nEpoch [22/200] | Train Loss: 0.3765, Train Acc: 0.8494 | \nEpoch [23/200] | Train Loss: 0.3425, Train Acc: 0.8729 | \nEpoch [24/200] | Train Loss: 0.3325, Train Acc: 0.8729 | \nEpoch [25/200] | Train Loss: 0.3334, Train Acc: 0.8541 | \nEpoch [26/200] | Train Loss: 0.3058, Train Acc: 0.8753 | \nEpoch [27/200] | Train Loss: 0.2847, Train Acc: 0.8847 | \nEpoch [28/200] | Train Loss: 0.2251, Train Acc: 0.9176 | \nEpoch [29/200] | Train Loss: 0.3282, Train Acc: 0.8659 | \nEpoch [30/200] | Train Loss: 0.2463, Train Acc: 0.8918 | \nEpoch [31/200] | Train Loss: 0.2319, Train Acc: 0.9224 | \nEpoch [32/200] | Train Loss: 0.2841, Train Acc: 0.8965 | \nEpoch [33/200] | Train Loss: 0.3171, Train Acc: 0.8612 | \nEpoch [34/200] | Train Loss: 0.2529, Train Acc: 0.9059 | \nEpoch [35/200] | Train Loss: 0.2690, Train Acc: 0.9082 | \nEpoch [36/200] | Train Loss: 0.2283, Train Acc: 0.9153 | \nEpoch [37/200] | Train Loss: 0.2090, Train Acc: 0.9176 | \nEpoch [38/200] | Train Loss: 0.1966, Train Acc: 0.9294 | \nEpoch [39/200] | Train Loss: 0.2429, Train Acc: 0.9106 | \nEpoch [40/200] | Train Loss: 0.2372, Train Acc: 0.9176 | \nEpoch [41/200] | Train Loss: 0.2260, Train Acc: 0.9082 | \nEpoch [42/200] | Train Loss: 0.2231, Train Acc: 0.9247 | \nEpoch [43/200] | Train Loss: 0.1977, Train Acc: 0.9176 | \nEpoch [44/200] | Train Loss: 0.1786, Train Acc: 0.9412 | \nEpoch [45/200] | Train Loss: 0.2085, Train Acc: 0.9271 | \nEpoch [46/200] | Train Loss: 0.1916, Train Acc: 0.9294 | \nEpoch [47/200] | Train Loss: 0.1748, Train Acc: 0.9388 | \nEpoch [48/200] | Train Loss: 0.1952, Train Acc: 0.9271 | \nEpoch [49/200] | Train Loss: 0.2753, Train Acc: 0.9012 | \nEpoch [50/200] | Train Loss: 0.2940, Train Acc: 0.8824 | \nEpoch [51/200] | Train Loss: 0.2221, Train Acc: 0.9129 | \nEpoch [52/200] | Train Loss: 0.2008, Train Acc: 0.9294 | \nEpoch [53/200] | Train Loss: 0.2027, Train Acc: 0.9294 | \nEpoch [54/200] | Train Loss: 0.2446, Train Acc: 0.9153 | \nEpoch [55/200] | Train Loss: 0.1909, Train Acc: 0.9200 | \nEpoch [56/200] | Train Loss: 0.1746, Train Acc: 0.9365 | \nEpoch [57/200] | Train Loss: 0.1722, Train Acc: 0.9318 | \nEpoch [58/200] | Train Loss: 0.1688, Train Acc: 0.9365 | \nEpoch [59/200] | Train Loss: 0.1673, Train Acc: 0.9412 | \nEpoch [60/200] | Train Loss: 0.2748, Train Acc: 0.9082 | \nEpoch [61/200] | Train Loss: 0.2960, Train Acc: 0.8871 | \nEpoch [62/200] | Train Loss: 0.2461, Train Acc: 0.9012 | \nEpoch [63/200] | Train Loss: 0.2043, Train Acc: 0.9294 | \nEarly stopping triggered.\nTest Loss: 0.3512, Test Accuracy: 0.8785, Test AUC: 0.9315\n\n--- Processing: fc_theta ---\nShape: (532, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7667, Train Acc: 0.4965 | \nEpoch [2/200] | Train Loss: 0.7006, Train Acc: 0.4894 | \nEpoch [3/200] | Train Loss: 0.6797, Train Acc: 0.6047 | \nEpoch [4/200] | Train Loss: 0.6042, Train Acc: 0.6988 | \nEpoch [5/200] | Train Loss: 0.5886, Train Acc: 0.6918 | \nEpoch [6/200] | Train Loss: 0.5035, Train Acc: 0.7788 | \nEpoch [7/200] | Train Loss: 0.4516, Train Acc: 0.7859 | \nEpoch [8/200] | Train Loss: 0.4840, Train Acc: 0.7976 | \nEpoch [9/200] | Train Loss: 0.4745, Train Acc: 0.7812 | \nEpoch [10/200] | Train Loss: 0.4337, Train Acc: 0.8165 | \nEpoch [11/200] | Train Loss: 0.3924, Train Acc: 0.8353 | \nEpoch [12/200] | Train Loss: 0.3739, Train Acc: 0.8447 | \nEpoch [13/200] | Train Loss: 0.3509, Train Acc: 0.8635 | \nEpoch [14/200] | Train Loss: 0.3215, Train Acc: 0.8706 | \nEpoch [15/200] | Train Loss: 0.3211, Train Acc: 0.8612 | \nEpoch [16/200] | Train Loss: 0.2881, Train Acc: 0.8824 | \nEpoch [17/200] | Train Loss: 0.2880, Train Acc: 0.8800 | \nEpoch [18/200] | Train Loss: 0.2792, Train Acc: 0.8847 | \nEpoch [19/200] | Train Loss: 0.3111, Train Acc: 0.8659 | \nEpoch [20/200] | Train Loss: 0.2641, Train Acc: 0.8941 | \nEpoch [21/200] | Train Loss: 0.2302, Train Acc: 0.9176 | \nEpoch [22/200] | Train Loss: 0.2611, Train Acc: 0.8847 | \nEpoch [23/200] | Train Loss: 0.2894, Train Acc: 0.8871 | \nEpoch [24/200] | Train Loss: 0.2333, Train Acc: 0.9035 | \nEpoch [25/200] | Train Loss: 0.2047, Train Acc: 0.9271 | \nEpoch [26/200] | Train Loss: 0.1941, Train Acc: 0.9224 | \nEpoch [27/200] | Train Loss: 0.2251, Train Acc: 0.9200 | \nEpoch [28/200] | Train Loss: 0.2353, Train Acc: 0.9129 | \nEpoch [29/200] | Train Loss: 0.2721, Train Acc: 0.8965 | \nEpoch [30/200] | Train Loss: 0.3089, Train Acc: 0.8800 | \nEpoch [31/200] | Train Loss: 0.2789, Train Acc: 0.8824 | \nEpoch [32/200] | Train Loss: 0.2668, Train Acc: 0.9012 | \nEpoch [33/200] | Train Loss: 0.2257, Train Acc: 0.9129 | \nEpoch [34/200] | Train Loss: 0.2029, Train Acc: 0.9224 | \nEpoch [35/200] | Train Loss: 0.1859, Train Acc: 0.9294 | \nEpoch [36/200] | Train Loss: 0.1842, Train Acc: 0.9294 | \nEpoch [37/200] | Train Loss: 0.1989, Train Acc: 0.9176 | \nEpoch [38/200] | Train Loss: 0.1620, Train Acc: 0.9412 | \nEpoch [39/200] | Train Loss: 0.1624, Train Acc: 0.9435 | \nEpoch [40/200] | Train Loss: 0.1424, Train Acc: 0.9506 | \nEpoch [41/200] | Train Loss: 0.2075, Train Acc: 0.9176 | \nEpoch [42/200] | Train Loss: 0.2632, Train Acc: 0.9035 | \nEpoch [43/200] | Train Loss: 0.2791, Train Acc: 0.8800 | \nEpoch [44/200] | Train Loss: 0.2277, Train Acc: 0.9247 | \nEpoch [45/200] | Train Loss: 0.1962, Train Acc: 0.9176 | \nEpoch [46/200] | Train Loss: 0.2576, Train Acc: 0.9012 | \nEpoch [47/200] | Train Loss: 0.2203, Train Acc: 0.9129 | \nEpoch [48/200] | Train Loss: 0.1804, Train Acc: 0.9224 | \nEpoch [49/200] | Train Loss: 0.1291, Train Acc: 0.9529 | \nEpoch [50/200] | Train Loss: 0.1413, Train Acc: 0.9365 | \nEpoch [51/200] | Train Loss: 0.1926, Train Acc: 0.9365 | \nEpoch [52/200] | Train Loss: 0.1702, Train Acc: 0.9271 | \nEpoch [53/200] | Train Loss: 0.2246, Train Acc: 0.9129 | \nEpoch [54/200] | Train Loss: 0.2023, Train Acc: 0.9106 | \nEpoch [55/200] | Train Loss: 0.1473, Train Acc: 0.9529 | \nEpoch [56/200] | Train Loss: 0.1274, Train Acc: 0.9553 | \nEpoch [57/200] | Train Loss: 0.1486, Train Acc: 0.9435 | \nEpoch [58/200] | Train Loss: 0.1292, Train Acc: 0.9553 | \nEpoch [59/200] | Train Loss: 0.1605, Train Acc: 0.9435 | \nEpoch [60/200] | Train Loss: 0.1334, Train Acc: 0.9553 | \nEpoch [61/200] | Train Loss: 0.1299, Train Acc: 0.9506 | \nEpoch [62/200] | Train Loss: 0.1367, Train Acc: 0.9482 | \nEpoch [63/200] | Train Loss: 0.1941, Train Acc: 0.9224 | \nEpoch [64/200] | Train Loss: 0.1552, Train Acc: 0.9388 | \nEpoch [65/200] | Train Loss: 0.1247, Train Acc: 0.9529 | \nEpoch [66/200] | Train Loss: 0.1167, Train Acc: 0.9647 | \nEpoch [67/200] | Train Loss: 0.0796, Train Acc: 0.9788 | \nEpoch [68/200] | Train Loss: 0.0981, Train Acc: 0.9741 | \nEpoch [69/200] | Train Loss: 0.1292, Train Acc: 0.9600 | \nEpoch [70/200] | Train Loss: 0.1270, Train Acc: 0.9459 | \nEpoch [71/200] | Train Loss: 0.1338, Train Acc: 0.9553 | \nEpoch [72/200] | Train Loss: 0.1135, Train Acc: 0.9553 | \nEpoch [73/200] | Train Loss: 0.1069, Train Acc: 0.9576 | \nEpoch [74/200] | Train Loss: 0.2011, Train Acc: 0.9412 | \nEpoch [75/200] | Train Loss: 0.2842, Train Acc: 0.8965 | \nEpoch [76/200] | Train Loss: 0.2079, Train Acc: 0.9294 | \nEpoch [77/200] | Train Loss: 0.2016, Train Acc: 0.9365 | \nEpoch 00078: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [78/200] | Train Loss: 0.1716, Train Acc: 0.9506 | \nEpoch [79/200] | Train Loss: 0.1502, Train Acc: 0.9435 | \nEpoch [80/200] | Train Loss: 0.1491, Train Acc: 0.9482 | \nEpoch [81/200] | Train Loss: 0.1270, Train Acc: 0.9553 | \nEpoch [82/200] | Train Loss: 0.1055, Train Acc: 0.9624 | \nEpoch [83/200] | Train Loss: 0.0959, Train Acc: 0.9765 | \nEpoch [84/200] | Train Loss: 0.0942, Train Acc: 0.9741 | \nEpoch [85/200] | Train Loss: 0.0983, Train Acc: 0.9694 | \nEpoch [86/200] | Train Loss: 0.1006, Train Acc: 0.9671 | \nEarly stopping triggered.\nTest Loss: 0.5067, Test Accuracy: 0.8785, Test AUC: 0.8969\n\n--- Processing: fc_alpha ---\nShape: (532, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7449, Train Acc: 0.4988 | \nEpoch [2/200] | Train Loss: 0.6925, Train Acc: 0.5435 | \nEpoch [3/200] | Train Loss: 0.6778, Train Acc: 0.5765 | \nEpoch [4/200] | Train Loss: 0.6156, Train Acc: 0.7012 | \nEpoch [5/200] | Train Loss: 0.5225, Train Acc: 0.7459 | \nEpoch [6/200] | Train Loss: 0.5198, Train Acc: 0.7247 | \nEpoch [7/200] | Train Loss: 0.4619, Train Acc: 0.8047 | \nEpoch [8/200] | Train Loss: 0.4389, Train Acc: 0.8047 | \nEpoch [9/200] | Train Loss: 0.4304, Train Acc: 0.8165 | \nEpoch [10/200] | Train Loss: 0.4073, Train Acc: 0.8094 | \nEpoch [11/200] | Train Loss: 0.4183, Train Acc: 0.8447 | \nEpoch [12/200] | Train Loss: 0.3610, Train Acc: 0.8235 | \nEpoch [13/200] | Train Loss: 0.3566, Train Acc: 0.8588 | \nEpoch [14/200] | Train Loss: 0.3336, Train Acc: 0.8565 | \nEpoch [15/200] | Train Loss: 0.3718, Train Acc: 0.8376 | \nEpoch [16/200] | Train Loss: 0.3600, Train Acc: 0.8400 | \nEpoch [17/200] | Train Loss: 0.3988, Train Acc: 0.8024 | \nEpoch [18/200] | Train Loss: 0.3003, Train Acc: 0.8800 | \nEpoch [19/200] | Train Loss: 0.3018, Train Acc: 0.8565 | \nEpoch [20/200] | Train Loss: 0.3069, Train Acc: 0.8706 | \nEpoch [21/200] | Train Loss: 0.2743, Train Acc: 0.8776 | \nEpoch [22/200] | Train Loss: 0.2688, Train Acc: 0.8800 | \nEpoch [23/200] | Train Loss: 0.2725, Train Acc: 0.8918 | \nEpoch [24/200] | Train Loss: 0.2453, Train Acc: 0.8871 | \nEpoch [25/200] | Train Loss: 0.2240, Train Acc: 0.9082 | \nEpoch [26/200] | Train Loss: 0.2271, Train Acc: 0.9059 | \nEpoch [27/200] | Train Loss: 0.2455, Train Acc: 0.8871 | \nEpoch [28/200] | Train Loss: 0.2319, Train Acc: 0.9012 | \nEpoch [29/200] | Train Loss: 0.2414, Train Acc: 0.8871 | \nEpoch [30/200] | Train Loss: 0.2816, Train Acc: 0.8941 | \nEpoch [31/200] | Train Loss: 0.2670, Train Acc: 0.8800 | \nEpoch [32/200] | Train Loss: 0.2211, Train Acc: 0.9224 | \nEpoch [33/200] | Train Loss: 0.2383, Train Acc: 0.9059 | \nEpoch [34/200] | Train Loss: 0.2764, Train Acc: 0.8918 | \nEpoch [35/200] | Train Loss: 0.2214, Train Acc: 0.9082 | \nEpoch [36/200] | Train Loss: 0.2178, Train Acc: 0.9129 | \nEpoch [37/200] | Train Loss: 0.1864, Train Acc: 0.9341 | \nEpoch [38/200] | Train Loss: 0.2128, Train Acc: 0.9082 | \nEpoch [39/200] | Train Loss: 0.2435, Train Acc: 0.8965 | \nEpoch [40/200] | Train Loss: 0.2158, Train Acc: 0.9106 | \nEpoch [41/200] | Train Loss: 0.2093, Train Acc: 0.9176 | \nEpoch [42/200] | Train Loss: 0.2217, Train Acc: 0.9129 | \nEpoch [43/200] | Train Loss: 0.2121, Train Acc: 0.9106 | \nEpoch [44/200] | Train Loss: 0.2443, Train Acc: 0.8918 | \nEpoch [45/200] | Train Loss: 0.1971, Train Acc: 0.9200 | \nEpoch [46/200] | Train Loss: 0.1802, Train Acc: 0.9224 | \nEpoch [47/200] | Train Loss: 0.1779, Train Acc: 0.9365 | \nEpoch [48/200] | Train Loss: 0.1882, Train Acc: 0.9341 | \nEpoch [49/200] | Train Loss: 0.2059, Train Acc: 0.9035 | \nEpoch [50/200] | Train Loss: 0.2012, Train Acc: 0.9082 | \nEpoch [51/200] | Train Loss: 0.2239, Train Acc: 0.9035 | \nEpoch [52/200] | Train Loss: 0.1979, Train Acc: 0.9153 | \nEpoch [53/200] | Train Loss: 0.1814, Train Acc: 0.9294 | \nEpoch [54/200] | Train Loss: 0.1556, Train Acc: 0.9506 | \nEpoch [55/200] | Train Loss: 0.1818, Train Acc: 0.9247 | \nEpoch [56/200] | Train Loss: 0.2621, Train Acc: 0.9012 | \nEpoch [57/200] | Train Loss: 0.2511, Train Acc: 0.8918 | \nEpoch [58/200] | Train Loss: 0.2093, Train Acc: 0.9082 | \nEpoch [59/200] | Train Loss: 0.1746, Train Acc: 0.9365 | \nEpoch [60/200] | Train Loss: 0.1546, Train Acc: 0.9318 | \nEpoch [61/200] | Train Loss: 0.1488, Train Acc: 0.9482 | \nEpoch [62/200] | Train Loss: 0.1358, Train Acc: 0.9412 | \nEpoch [63/200] | Train Loss: 0.1678, Train Acc: 0.9200 | \nEpoch [64/200] | Train Loss: 0.1963, Train Acc: 0.9176 | \nEpoch [65/200] | Train Loss: 0.1923, Train Acc: 0.9341 | \nEpoch [66/200] | Train Loss: 0.1992, Train Acc: 0.9224 | \nEpoch [67/200] | Train Loss: 0.1468, Train Acc: 0.9388 | \nEpoch [68/200] | Train Loss: 0.1451, Train Acc: 0.9529 | \nEpoch [69/200] | Train Loss: 0.1247, Train Acc: 0.9529 | \nEpoch [70/200] | Train Loss: 0.1056, Train Acc: 0.9600 | \nEpoch [71/200] | Train Loss: 0.1353, Train Acc: 0.9435 | \nEpoch [72/200] | Train Loss: 0.2225, Train Acc: 0.9271 | \nEpoch [73/200] | Train Loss: 0.1632, Train Acc: 0.9318 | \nEpoch [74/200] | Train Loss: 0.1496, Train Acc: 0.9365 | \nEpoch [75/200] | Train Loss: 0.1667, Train Acc: 0.9341 | \nEpoch [76/200] | Train Loss: 0.1471, Train Acc: 0.9435 | \nEpoch [77/200] | Train Loss: 0.2100, Train Acc: 0.9129 | \nEpoch [78/200] | Train Loss: 0.1657, Train Acc: 0.9271 | \nEpoch [79/200] | Train Loss: 0.1843, Train Acc: 0.9247 | \nEpoch [80/200] | Train Loss: 0.1650, Train Acc: 0.9247 | \nEpoch 00081: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [81/200] | Train Loss: 0.2176, Train Acc: 0.9200 | \nEpoch [82/200] | Train Loss: 0.1951, Train Acc: 0.9035 | \nEpoch [83/200] | Train Loss: 0.1317, Train Acc: 0.9459 | \nEpoch [84/200] | Train Loss: 0.1190, Train Acc: 0.9576 | \nEpoch [85/200] | Train Loss: 0.0945, Train Acc: 0.9647 | \nEpoch [86/200] | Train Loss: 0.1005, Train Acc: 0.9671 | \nEpoch [87/200] | Train Loss: 0.0942, Train Acc: 0.9647 | \nEpoch [88/200] | Train Loss: 0.0845, Train Acc: 0.9647 | \nEpoch [89/200] | Train Loss: 0.0863, Train Acc: 0.9718 | \nEpoch [90/200] | Train Loss: 0.0743, Train Acc: 0.9694 | \nEpoch [91/200] | Train Loss: 0.0786, Train Acc: 0.9741 | \nEpoch [92/200] | Train Loss: 0.0744, Train Acc: 0.9788 | \nEpoch [93/200] | Train Loss: 0.0658, Train Acc: 0.9765 | \nEpoch [94/200] | Train Loss: 0.0782, Train Acc: 0.9718 | \nEpoch [95/200] | Train Loss: 0.0714, Train Acc: 0.9718 | \nEpoch [96/200] | Train Loss: 0.0792, Train Acc: 0.9694 | \nEpoch [97/200] | Train Loss: 0.0542, Train Acc: 0.9812 | \nEpoch [98/200] | Train Loss: 0.0649, Train Acc: 0.9765 | \nEpoch [99/200] | Train Loss: 0.0556, Train Acc: 0.9835 | \nEpoch [100/200] | Train Loss: 0.0491, Train Acc: 0.9835 | \nEpoch [101/200] | Train Loss: 0.0550, Train Acc: 0.9788 | \nEpoch [102/200] | Train Loss: 0.0496, Train Acc: 0.9859 | \nEpoch [103/200] | Train Loss: 0.0468, Train Acc: 0.9906 | \nEpoch [104/200] | Train Loss: 0.0565, Train Acc: 0.9765 | \nEpoch [105/200] | Train Loss: 0.0496, Train Acc: 0.9812 | \nEpoch [106/200] | Train Loss: 0.0456, Train Acc: 0.9906 | \nEpoch [107/200] | Train Loss: 0.0498, Train Acc: 0.9859 | \nEpoch [108/200] | Train Loss: 0.0447, Train Acc: 0.9859 | \nEpoch [109/200] | Train Loss: 0.0465, Train Acc: 0.9859 | \nEpoch [110/200] | Train Loss: 0.0489, Train Acc: 0.9812 | \nEpoch [111/200] | Train Loss: 0.0355, Train Acc: 0.9882 | \nEpoch [112/200] | Train Loss: 0.0360, Train Acc: 0.9906 | \nEpoch [113/200] | Train Loss: 0.0456, Train Acc: 0.9812 | \nEpoch [114/200] | Train Loss: 0.0471, Train Acc: 0.9859 | \nEpoch [115/200] | Train Loss: 0.0415, Train Acc: 0.9906 | \nEpoch [116/200] | Train Loss: 0.0395, Train Acc: 0.9906 | \nEpoch [117/200] | Train Loss: 0.0382, Train Acc: 0.9882 | \nEpoch [118/200] | Train Loss: 0.0420, Train Acc: 0.9812 | \nEpoch [119/200] | Train Loss: 0.0397, Train Acc: 0.9859 | \nEpoch [120/200] | Train Loss: 0.0416, Train Acc: 0.9882 | \nEpoch [121/200] | Train Loss: 0.0383, Train Acc: 0.9882 | \nEpoch [122/200] | Train Loss: 0.0259, Train Acc: 0.9953 | \nEpoch [123/200] | Train Loss: 0.0322, Train Acc: 0.9906 | \nEpoch [124/200] | Train Loss: 0.0323, Train Acc: 0.9953 | \nEpoch [125/200] | Train Loss: 0.0327, Train Acc: 0.9906 | \nEpoch [126/200] | Train Loss: 0.0405, Train Acc: 0.9835 | \nEpoch [127/200] | Train Loss: 0.0427, Train Acc: 0.9859 | \nEpoch [128/200] | Train Loss: 0.0276, Train Acc: 0.9929 | \nEpoch [129/200] | Train Loss: 0.0368, Train Acc: 0.9882 | \nEpoch [130/200] | Train Loss: 0.0341, Train Acc: 0.9906 | \nEpoch [131/200] | Train Loss: 0.0342, Train Acc: 0.9882 | \nEpoch [132/200] | Train Loss: 0.0303, Train Acc: 0.9906 | \nEpoch 00133: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [133/200] | Train Loss: 0.0311, Train Acc: 0.9906 | \nEpoch [134/200] | Train Loss: 0.0329, Train Acc: 0.9906 | \nEpoch [135/200] | Train Loss: 0.0355, Train Acc: 0.9906 | \nEpoch [136/200] | Train Loss: 0.0501, Train Acc: 0.9859 | \nEpoch [137/200] | Train Loss: 0.0365, Train Acc: 0.9882 | \nEpoch [138/200] | Train Loss: 0.0294, Train Acc: 0.9929 | \nEpoch [139/200] | Train Loss: 0.0316, Train Acc: 0.9906 | \nEpoch [140/200] | Train Loss: 0.0206, Train Acc: 0.9976 | \nEpoch [141/200] | Train Loss: 0.0380, Train Acc: 0.9859 | \nEpoch [142/200] | Train Loss: 0.0328, Train Acc: 0.9882 | \nEpoch [143/200] | Train Loss: 0.0321, Train Acc: 0.9929 | \nEpoch [144/200] | Train Loss: 0.0324, Train Acc: 0.9906 | \nEpoch [145/200] | Train Loss: 0.0226, Train Acc: 0.9976 | \nEpoch [146/200] | Train Loss: 0.0314, Train Acc: 0.9929 | \nEpoch [147/200] | Train Loss: 0.0271, Train Acc: 0.9953 | \nEpoch [148/200] | Train Loss: 0.0218, Train Acc: 0.9953 | \nEpoch [149/200] | Train Loss: 0.0239, Train Acc: 0.9929 | \nEpoch [150/200] | Train Loss: 0.0201, Train Acc: 0.9953 | \nEpoch [151/200] | Train Loss: 0.0253, Train Acc: 0.9929 | \nEpoch [152/200] | Train Loss: 0.0268, Train Acc: 0.9953 | \nEpoch [153/200] | Train Loss: 0.0218, Train Acc: 0.9976 | \nEpoch [154/200] | Train Loss: 0.0257, Train Acc: 0.9882 | \nEpoch [155/200] | Train Loss: 0.0223, Train Acc: 0.9953 | \nEpoch [156/200] | Train Loss: 0.0306, Train Acc: 0.9882 | \nEpoch [157/200] | Train Loss: 0.0230, Train Acc: 0.9929 | \nEpoch [158/200] | Train Loss: 0.0276, Train Acc: 0.9906 | \nEpoch [159/200] | Train Loss: 0.0244, Train Acc: 0.9929 | \nEarly stopping triggered.\nTest Loss: 0.6816, Test Accuracy: 0.8224, Test AUC: 0.9029\n\n--- Processing: fc_beta ---\nShape: (532, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7216, Train Acc: 0.4918 | \nEpoch [2/200] | Train Loss: 0.7048, Train Acc: 0.5200 | \nEpoch [3/200] | Train Loss: 0.6749, Train Acc: 0.5788 | \nEpoch [4/200] | Train Loss: 0.6098, Train Acc: 0.7012 | \nEpoch [5/200] | Train Loss: 0.5570, Train Acc: 0.6847 | \nEpoch [6/200] | Train Loss: 0.5178, Train Acc: 0.7718 | \nEpoch [7/200] | Train Loss: 0.4434, Train Acc: 0.7812 | \nEpoch [8/200] | Train Loss: 0.4474, Train Acc: 0.7882 | \nEpoch [9/200] | Train Loss: 0.4385, Train Acc: 0.7694 | \nEpoch [10/200] | Train Loss: 0.4520, Train Acc: 0.7694 | \nEpoch [11/200] | Train Loss: 0.4233, Train Acc: 0.7859 | \nEpoch [12/200] | Train Loss: 0.4003, Train Acc: 0.8071 | \nEpoch [13/200] | Train Loss: 0.4188, Train Acc: 0.7624 | \nEpoch [14/200] | Train Loss: 0.3793, Train Acc: 0.8165 | \nEpoch [15/200] | Train Loss: 0.3741, Train Acc: 0.8235 | \nEpoch [16/200] | Train Loss: 0.3926, Train Acc: 0.8282 | \nEpoch [17/200] | Train Loss: 0.3326, Train Acc: 0.8635 | \nEpoch [18/200] | Train Loss: 0.3013, Train Acc: 0.8706 | \nEpoch [19/200] | Train Loss: 0.2900, Train Acc: 0.8894 | \nEpoch [20/200] | Train Loss: 0.3182, Train Acc: 0.8588 | \nEpoch [21/200] | Train Loss: 0.3229, Train Acc: 0.8541 | \nEpoch [22/200] | Train Loss: 0.3116, Train Acc: 0.8659 | \nEpoch [23/200] | Train Loss: 0.3570, Train Acc: 0.8541 | \nEpoch [24/200] | Train Loss: 0.3113, Train Acc: 0.8729 | \nEpoch [25/200] | Train Loss: 0.3549, Train Acc: 0.8376 | \nEpoch [26/200] | Train Loss: 0.3264, Train Acc: 0.8565 | \nEpoch [27/200] | Train Loss: 0.2804, Train Acc: 0.8659 | \nEpoch [28/200] | Train Loss: 0.2493, Train Acc: 0.8776 | \nEpoch [29/200] | Train Loss: 0.2943, Train Acc: 0.8659 | \nEpoch [30/200] | Train Loss: 0.2406, Train Acc: 0.8918 | \nEpoch [31/200] | Train Loss: 0.2437, Train Acc: 0.9035 | \nEpoch [32/200] | Train Loss: 0.2098, Train Acc: 0.9106 | \nEpoch [33/200] | Train Loss: 0.2388, Train Acc: 0.9035 | \nEpoch [34/200] | Train Loss: 0.1843, Train Acc: 0.9224 | \nEpoch [35/200] | Train Loss: 0.2286, Train Acc: 0.9059 | \nEpoch [36/200] | Train Loss: 0.2034, Train Acc: 0.9200 | \nEpoch [37/200] | Train Loss: 0.1916, Train Acc: 0.9247 | \nEpoch [38/200] | Train Loss: 0.1728, Train Acc: 0.9388 | \nEpoch [39/200] | Train Loss: 0.2673, Train Acc: 0.8682 | \nEpoch [40/200] | Train Loss: 0.2468, Train Acc: 0.8941 | \nEpoch [41/200] | Train Loss: 0.2465, Train Acc: 0.9012 | \nEpoch [42/200] | Train Loss: 0.2345, Train Acc: 0.8847 | \nEpoch [43/200] | Train Loss: 0.2114, Train Acc: 0.9176 | \nEpoch [44/200] | Train Loss: 0.2013, Train Acc: 0.9012 | \nEpoch [45/200] | Train Loss: 0.2089, Train Acc: 0.9082 | \nEpoch [46/200] | Train Loss: 0.1804, Train Acc: 0.9082 | \nEpoch [47/200] | Train Loss: 0.2619, Train Acc: 0.8612 | \nEpoch [48/200] | Train Loss: 0.2515, Train Acc: 0.8894 | \nEpoch 00049: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [49/200] | Train Loss: 0.1898, Train Acc: 0.9271 | \nEpoch [50/200] | Train Loss: 0.1440, Train Acc: 0.9435 | \nEpoch [51/200] | Train Loss: 0.1319, Train Acc: 0.9529 | \nEpoch [52/200] | Train Loss: 0.1149, Train Acc: 0.9506 | \nEpoch [53/200] | Train Loss: 0.1207, Train Acc: 0.9482 | \nEpoch [54/200] | Train Loss: 0.0910, Train Acc: 0.9624 | \nEpoch [55/200] | Train Loss: 0.0883, Train Acc: 0.9671 | \nEpoch [56/200] | Train Loss: 0.0801, Train Acc: 0.9694 | \nEpoch [57/200] | Train Loss: 0.0848, Train Acc: 0.9624 | \nEpoch [58/200] | Train Loss: 0.0805, Train Acc: 0.9647 | \nEpoch [59/200] | Train Loss: 0.0858, Train Acc: 0.9624 | \nEpoch [60/200] | Train Loss: 0.0783, Train Acc: 0.9694 | \nEpoch [61/200] | Train Loss: 0.0760, Train Acc: 0.9671 | \nEpoch [62/200] | Train Loss: 0.0710, Train Acc: 0.9741 | \nEpoch [63/200] | Train Loss: 0.0642, Train Acc: 0.9741 | \nEpoch [64/200] | Train Loss: 0.0652, Train Acc: 0.9671 | \nEpoch [65/200] | Train Loss: 0.0646, Train Acc: 0.9765 | \nEpoch [66/200] | Train Loss: 0.0645, Train Acc: 0.9788 | \nEpoch [67/200] | Train Loss: 0.0614, Train Acc: 0.9718 | \nEpoch [68/200] | Train Loss: 0.0698, Train Acc: 0.9694 | \nEpoch [69/200] | Train Loss: 0.0553, Train Acc: 0.9788 | \nEpoch [70/200] | Train Loss: 0.0786, Train Acc: 0.9741 | \nEpoch [71/200] | Train Loss: 0.0583, Train Acc: 0.9718 | \nEpoch [72/200] | Train Loss: 0.0656, Train Acc: 0.9671 | \nEpoch [73/200] | Train Loss: 0.0652, Train Acc: 0.9741 | \nEpoch [74/200] | Train Loss: 0.0448, Train Acc: 0.9906 | \nEpoch [75/200] | Train Loss: 0.0550, Train Acc: 0.9765 | \nEpoch [76/200] | Train Loss: 0.0587, Train Acc: 0.9741 | \nEpoch [77/200] | Train Loss: 0.0460, Train Acc: 0.9812 | \nEpoch [78/200] | Train Loss: 0.0412, Train Acc: 0.9835 | \nEpoch [79/200] | Train Loss: 0.0542, Train Acc: 0.9765 | \nEpoch [80/200] | Train Loss: 0.0427, Train Acc: 0.9859 | \nEpoch [81/200] | Train Loss: 0.0485, Train Acc: 0.9741 | \nEpoch [82/200] | Train Loss: 0.0373, Train Acc: 0.9859 | \nEpoch [83/200] | Train Loss: 0.0444, Train Acc: 0.9835 | \nEpoch [84/200] | Train Loss: 0.0450, Train Acc: 0.9859 | \nEpoch [85/200] | Train Loss: 0.0411, Train Acc: 0.9835 | \nEpoch [86/200] | Train Loss: 0.0447, Train Acc: 0.9812 | \nEpoch [87/200] | Train Loss: 0.0601, Train Acc: 0.9741 | \nEpoch [88/200] | Train Loss: 0.0578, Train Acc: 0.9741 | \nEpoch [89/200] | Train Loss: 0.0454, Train Acc: 0.9812 | \nEpoch [90/200] | Train Loss: 0.0503, Train Acc: 0.9718 | \nEpoch [91/200] | Train Loss: 0.0441, Train Acc: 0.9788 | \nEpoch [92/200] | Train Loss: 0.0549, Train Acc: 0.9788 | \nEpoch 00093: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [93/200] | Train Loss: 0.0434, Train Acc: 0.9812 | \nEarly stopping triggered.\nTest Loss: 0.5995, Test Accuracy: 0.8785, Test AUC: 0.8899\n\n--- Processing: fc_highbeta ---\nShape: (532, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7482, Train Acc: 0.4965 | \nEpoch [2/200] | Train Loss: 0.7064, Train Acc: 0.4494 | \nEpoch [3/200] | Train Loss: 0.6984, Train Acc: 0.5012 | \nEpoch [4/200] | Train Loss: 0.6934, Train Acc: 0.5271 | \nEpoch [5/200] | Train Loss: 0.7031, Train Acc: 0.4706 | \nEpoch [6/200] | Train Loss: 0.6781, Train Acc: 0.5929 | \nEpoch [7/200] | Train Loss: 0.6022, Train Acc: 0.7082 | \nEpoch [8/200] | Train Loss: 0.6008, Train Acc: 0.6871 | \nEpoch [9/200] | Train Loss: 0.5861, Train Acc: 0.6988 | \nEpoch [10/200] | Train Loss: 0.5582, Train Acc: 0.7176 | \nEpoch [11/200] | Train Loss: 0.5576, Train Acc: 0.7059 | \nEpoch [12/200] | Train Loss: 0.5152, Train Acc: 0.7576 | \nEpoch [13/200] | Train Loss: 0.4609, Train Acc: 0.7976 | \nEpoch [14/200] | Train Loss: 0.4568, Train Acc: 0.7882 | \nEpoch [15/200] | Train Loss: 0.4594, Train Acc: 0.7812 | \nEpoch [16/200] | Train Loss: 0.4446, Train Acc: 0.7953 | \nEpoch [17/200] | Train Loss: 0.4196, Train Acc: 0.8094 | \nEpoch [18/200] | Train Loss: 0.4092, Train Acc: 0.8188 | \nEpoch [19/200] | Train Loss: 0.3479, Train Acc: 0.8518 | \nEpoch [20/200] | Train Loss: 0.4434, Train Acc: 0.8047 | \nEpoch [21/200] | Train Loss: 0.3741, Train Acc: 0.8424 | \nEpoch [22/200] | Train Loss: 0.4151, Train Acc: 0.8118 | \nEpoch [23/200] | Train Loss: 0.3609, Train Acc: 0.8400 | \nEpoch [24/200] | Train Loss: 0.3416, Train Acc: 0.8612 | \nEpoch [25/200] | Train Loss: 0.3317, Train Acc: 0.8541 | \nEpoch [26/200] | Train Loss: 0.3594, Train Acc: 0.8376 | \nEpoch [27/200] | Train Loss: 0.2973, Train Acc: 0.8894 | \nEpoch [28/200] | Train Loss: 0.2838, Train Acc: 0.8871 | \nEpoch [29/200] | Train Loss: 0.2594, Train Acc: 0.8988 | \nEpoch [30/200] | Train Loss: 0.2640, Train Acc: 0.8965 | \nEpoch [31/200] | Train Loss: 0.4795, Train Acc: 0.8141 | \nEpoch [32/200] | Train Loss: 0.3865, Train Acc: 0.8212 | \nEpoch [33/200] | Train Loss: 0.3943, Train Acc: 0.8118 | \nEpoch [34/200] | Train Loss: 0.3362, Train Acc: 0.8588 | \nEpoch [35/200] | Train Loss: 0.3533, Train Acc: 0.8518 | \nEpoch [36/200] | Train Loss: 0.3295, Train Acc: 0.8612 | \nEpoch [37/200] | Train Loss: 0.2732, Train Acc: 0.8918 | \nEpoch [38/200] | Train Loss: 0.2963, Train Acc: 0.8800 | \nEpoch [39/200] | Train Loss: 0.2649, Train Acc: 0.8871 | \nEpoch [40/200] | Train Loss: 0.2519, Train Acc: 0.9012 | \nEpoch [41/200] | Train Loss: 0.2139, Train Acc: 0.9106 | \nEpoch [42/200] | Train Loss: 0.2378, Train Acc: 0.8918 | \nEpoch [43/200] | Train Loss: 0.2817, Train Acc: 0.8847 | \nEpoch [44/200] | Train Loss: 0.4743, Train Acc: 0.8024 | \nEpoch [45/200] | Train Loss: 0.3978, Train Acc: 0.8141 | \nEpoch [46/200] | Train Loss: 0.3588, Train Acc: 0.8424 | \nEpoch [47/200] | Train Loss: 0.3304, Train Acc: 0.8588 | \nEpoch [48/200] | Train Loss: 0.2843, Train Acc: 0.8753 | \nEpoch [49/200] | Train Loss: 0.2539, Train Acc: 0.9059 | \nEpoch [50/200] | Train Loss: 0.2520, Train Acc: 0.8941 | \nEpoch [51/200] | Train Loss: 0.2469, Train Acc: 0.8941 | \nEpoch 00052: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [52/200] | Train Loss: 0.2236, Train Acc: 0.9224 | \nEpoch [53/200] | Train Loss: 0.2125, Train Acc: 0.9176 | \nEpoch [54/200] | Train Loss: 0.1992, Train Acc: 0.9271 | \nEpoch [55/200] | Train Loss: 0.1856, Train Acc: 0.9318 | \nEpoch [56/200] | Train Loss: 0.1514, Train Acc: 0.9506 | \nEpoch [57/200] | Train Loss: 0.1647, Train Acc: 0.9412 | \nEpoch [58/200] | Train Loss: 0.1749, Train Acc: 0.9318 | \nEpoch [59/200] | Train Loss: 0.1586, Train Acc: 0.9341 | \nEpoch [60/200] | Train Loss: 0.1450, Train Acc: 0.9435 | \nEpoch [61/200] | Train Loss: 0.1639, Train Acc: 0.9365 | \nEpoch [62/200] | Train Loss: 0.1380, Train Acc: 0.9482 | \nEpoch [63/200] | Train Loss: 0.1386, Train Acc: 0.9365 | \nEpoch [64/200] | Train Loss: 0.1176, Train Acc: 0.9647 | \nEpoch [65/200] | Train Loss: 0.1266, Train Acc: 0.9600 | \nEpoch [66/200] | Train Loss: 0.1198, Train Acc: 0.9553 | \nEpoch [67/200] | Train Loss: 0.1214, Train Acc: 0.9576 | \nEpoch [68/200] | Train Loss: 0.1247, Train Acc: 0.9506 | \nEpoch [69/200] | Train Loss: 0.1276, Train Acc: 0.9412 | \nEpoch [70/200] | Train Loss: 0.1246, Train Acc: 0.9482 | \nEpoch [71/200] | Train Loss: 0.1203, Train Acc: 0.9624 | \nEpoch [72/200] | Train Loss: 0.1225, Train Acc: 0.9482 | \nEpoch [73/200] | Train Loss: 0.1256, Train Acc: 0.9553 | \nEpoch [74/200] | Train Loss: 0.1059, Train Acc: 0.9718 | \nEpoch [75/200] | Train Loss: 0.0916, Train Acc: 0.9694 | \nEpoch [76/200] | Train Loss: 0.1134, Train Acc: 0.9647 | \nEpoch [77/200] | Train Loss: 0.1031, Train Acc: 0.9694 | \nEpoch [78/200] | Train Loss: 0.1159, Train Acc: 0.9553 | \nEpoch [79/200] | Train Loss: 0.1041, Train Acc: 0.9647 | \nEpoch [80/200] | Train Loss: 0.0966, Train Acc: 0.9624 | \nEpoch [81/200] | Train Loss: 0.0922, Train Acc: 0.9718 | \nEpoch [82/200] | Train Loss: 0.0897, Train Acc: 0.9671 | \nEpoch [83/200] | Train Loss: 0.0806, Train Acc: 0.9671 | \nEpoch [84/200] | Train Loss: 0.0719, Train Acc: 0.9788 | \nEpoch [85/200] | Train Loss: 0.0743, Train Acc: 0.9788 | \nEpoch [86/200] | Train Loss: 0.0832, Train Acc: 0.9718 | \nEpoch [87/200] | Train Loss: 0.1008, Train Acc: 0.9671 | \nEpoch [88/200] | Train Loss: 0.0704, Train Acc: 0.9694 | \nEpoch [89/200] | Train Loss: 0.0702, Train Acc: 0.9765 | \nEpoch [90/200] | Train Loss: 0.0674, Train Acc: 0.9718 | \nEpoch [91/200] | Train Loss: 0.0748, Train Acc: 0.9671 | \nEpoch [92/200] | Train Loss: 0.0778, Train Acc: 0.9694 | \nEpoch [93/200] | Train Loss: 0.0773, Train Acc: 0.9718 | \nEpoch [94/200] | Train Loss: 0.0714, Train Acc: 0.9788 | \nEpoch [95/200] | Train Loss: 0.0768, Train Acc: 0.9741 | \nEpoch [96/200] | Train Loss: 0.0747, Train Acc: 0.9694 | \nEpoch [97/200] | Train Loss: 0.0572, Train Acc: 0.9788 | \nEpoch [98/200] | Train Loss: 0.0667, Train Acc: 0.9765 | \nEpoch [99/200] | Train Loss: 0.0970, Train Acc: 0.9553 | \nEpoch [100/200] | Train Loss: 0.0560, Train Acc: 0.9835 | \nEpoch [101/200] | Train Loss: 0.0610, Train Acc: 0.9835 | \nEpoch [102/200] | Train Loss: 0.0538, Train Acc: 0.9812 | \nEpoch [103/200] | Train Loss: 0.0642, Train Acc: 0.9765 | \nEpoch [104/200] | Train Loss: 0.0724, Train Acc: 0.9765 | \nEpoch [105/200] | Train Loss: 0.0754, Train Acc: 0.9741 | \nEpoch [106/200] | Train Loss: 0.0632, Train Acc: 0.9788 | \nEpoch [107/200] | Train Loss: 0.0678, Train Acc: 0.9741 | \nEpoch [108/200] | Train Loss: 0.0457, Train Acc: 0.9835 | \nEpoch [109/200] | Train Loss: 0.0442, Train Acc: 0.9835 | \nEpoch [110/200] | Train Loss: 0.0740, Train Acc: 0.9671 | \nEpoch [111/200] | Train Loss: 0.0444, Train Acc: 0.9882 | \nEpoch [112/200] | Train Loss: 0.0436, Train Acc: 0.9882 | \nEpoch [113/200] | Train Loss: 0.0650, Train Acc: 0.9765 | \nEpoch [114/200] | Train Loss: 0.0514, Train Acc: 0.9694 | \nEpoch [115/200] | Train Loss: 0.0619, Train Acc: 0.9718 | \nEpoch [116/200] | Train Loss: 0.0529, Train Acc: 0.9835 | \nEpoch [117/200] | Train Loss: 0.0685, Train Acc: 0.9812 | \nEpoch [118/200] | Train Loss: 0.0592, Train Acc: 0.9765 | \nEpoch [119/200] | Train Loss: 0.0642, Train Acc: 0.9812 | \nEpoch [120/200] | Train Loss: 0.0324, Train Acc: 0.9929 | \nEpoch [121/200] | Train Loss: 0.0319, Train Acc: 0.9859 | \nEpoch [122/200] | Train Loss: 0.0463, Train Acc: 0.9812 | \nEpoch [123/200] | Train Loss: 0.0435, Train Acc: 0.9859 | \nEpoch [124/200] | Train Loss: 0.0526, Train Acc: 0.9741 | \nEpoch [125/200] | Train Loss: 0.0569, Train Acc: 0.9788 | \nEpoch [126/200] | Train Loss: 0.0665, Train Acc: 0.9765 | \nEpoch [127/200] | Train Loss: 0.0648, Train Acc: 0.9788 | \nEpoch [128/200] | Train Loss: 0.0805, Train Acc: 0.9788 | \nEpoch [129/200] | Train Loss: 0.0313, Train Acc: 0.9929 | \nEpoch [130/200] | Train Loss: 0.0372, Train Acc: 0.9882 | \nEpoch [131/200] | Train Loss: 0.0392, Train Acc: 0.9882 | \nEpoch [132/200] | Train Loss: 0.0518, Train Acc: 0.9788 | \nEpoch [133/200] | Train Loss: 0.0470, Train Acc: 0.9835 | \nEpoch [134/200] | Train Loss: 0.0311, Train Acc: 0.9906 | \nEpoch [135/200] | Train Loss: 0.0525, Train Acc: 0.9812 | \nEpoch [136/200] | Train Loss: 0.0682, Train Acc: 0.9718 | \nEpoch [137/200] | Train Loss: 0.0923, Train Acc: 0.9671 | \nEpoch [138/200] | Train Loss: 0.0556, Train Acc: 0.9741 | \nEpoch [139/200] | Train Loss: 0.0881, Train Acc: 0.9694 | \nEpoch [140/200] | Train Loss: 0.0295, Train Acc: 0.9976 | \nEpoch [141/200] | Train Loss: 0.0398, Train Acc: 0.9929 | \nEpoch [142/200] | Train Loss: 0.0307, Train Acc: 0.9929 | \nEpoch [143/200] | Train Loss: 0.0241, Train Acc: 0.9953 | \nEpoch [144/200] | Train Loss: 0.0335, Train Acc: 0.9906 | \nEpoch [145/200] | Train Loss: 0.0458, Train Acc: 0.9835 | \nEpoch [146/200] | Train Loss: 0.0474, Train Acc: 0.9812 | \nEpoch [147/200] | Train Loss: 0.0428, Train Acc: 0.9882 | \nEpoch [148/200] | Train Loss: 0.0312, Train Acc: 0.9882 | \nEpoch [149/200] | Train Loss: 0.0356, Train Acc: 0.9859 | \nEpoch [150/200] | Train Loss: 0.0361, Train Acc: 0.9906 | \nEpoch [151/200] | Train Loss: 0.0528, Train Acc: 0.9859 | \nEpoch [152/200] | Train Loss: 0.0334, Train Acc: 0.9906 | \nEpoch [153/200] | Train Loss: 0.0641, Train Acc: 0.9765 | \nEpoch 00154: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [154/200] | Train Loss: 0.0443, Train Acc: 0.9835 | \nEpoch [155/200] | Train Loss: 0.0558, Train Acc: 0.9859 | \nEpoch [156/200] | Train Loss: 0.0354, Train Acc: 0.9812 | \nEpoch [157/200] | Train Loss: 0.0345, Train Acc: 0.9882 | \nEpoch [158/200] | Train Loss: 0.0294, Train Acc: 0.9906 | \nEpoch [159/200] | Train Loss: 0.0297, Train Acc: 0.9906 | \nEarly stopping triggered.\nTest Loss: 0.3943, Test Accuracy: 0.9065, Test AUC: 0.9287\n\n--- Processing: fc_gamma ---\nShape: (532, 171)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7154, Train Acc: 0.5553 | \nEpoch [2/200] | Train Loss: 0.6905, Train Acc: 0.5176 | \nEpoch [3/200] | Train Loss: 0.6625, Train Acc: 0.6282 | \nEpoch [4/200] | Train Loss: 0.5766, Train Acc: 0.6847 | \nEpoch [5/200] | Train Loss: 0.5809, Train Acc: 0.7035 | \nEpoch [6/200] | Train Loss: 0.5140, Train Acc: 0.7435 | \nEpoch [7/200] | Train Loss: 0.5042, Train Acc: 0.7600 | \nEpoch [8/200] | Train Loss: 0.5475, Train Acc: 0.7224 | \nEpoch [9/200] | Train Loss: 0.4749, Train Acc: 0.7741 | \nEpoch [10/200] | Train Loss: 0.4124, Train Acc: 0.8376 | \nEpoch [11/200] | Train Loss: 0.4425, Train Acc: 0.8047 | \nEpoch [12/200] | Train Loss: 0.4305, Train Acc: 0.8047 | \nEpoch [13/200] | Train Loss: 0.4468, Train Acc: 0.8047 | \nEpoch [14/200] | Train Loss: 0.4124, Train Acc: 0.8212 | \nEpoch [15/200] | Train Loss: 0.4325, Train Acc: 0.8165 | \nEpoch [16/200] | Train Loss: 0.3944, Train Acc: 0.8282 | \nEpoch [17/200] | Train Loss: 0.3984, Train Acc: 0.8212 | \nEpoch [18/200] | Train Loss: 0.3959, Train Acc: 0.8259 | \nEpoch [19/200] | Train Loss: 0.4910, Train Acc: 0.7624 | \nEpoch [20/200] | Train Loss: 0.4586, Train Acc: 0.7718 | \nEpoch [21/200] | Train Loss: 0.3820, Train Acc: 0.8447 | \nEpoch [22/200] | Train Loss: 0.3982, Train Acc: 0.8424 | \nEpoch [23/200] | Train Loss: 0.4195, Train Acc: 0.8165 | \nEpoch [24/200] | Train Loss: 0.4214, Train Acc: 0.8094 | \nEpoch [25/200] | Train Loss: 0.3875, Train Acc: 0.8212 | \nEpoch [26/200] | Train Loss: 0.3534, Train Acc: 0.8541 | \nEpoch [27/200] | Train Loss: 0.3772, Train Acc: 0.8282 | \nEpoch [28/200] | Train Loss: 0.3516, Train Acc: 0.8565 | \nEpoch [29/200] | Train Loss: 0.3158, Train Acc: 0.8800 | \nEpoch [30/200] | Train Loss: 0.3064, Train Acc: 0.8776 | \nEpoch [31/200] | Train Loss: 0.3402, Train Acc: 0.8588 | \nEpoch [32/200] | Train Loss: 0.3540, Train Acc: 0.8518 | \nEpoch [33/200] | Train Loss: 0.2949, Train Acc: 0.8894 | \nEpoch [34/200] | Train Loss: 0.2971, Train Acc: 0.8871 | \nEpoch [35/200] | Train Loss: 0.3205, Train Acc: 0.8588 | \nEpoch [36/200] | Train Loss: 0.3084, Train Acc: 0.8635 | \nEpoch [37/200] | Train Loss: 0.3250, Train Acc: 0.8729 | \nEpoch [38/200] | Train Loss: 0.3556, Train Acc: 0.8329 | \nEpoch [39/200] | Train Loss: 0.2824, Train Acc: 0.8871 | \nEpoch [40/200] | Train Loss: 0.2889, Train Acc: 0.8776 | \nEpoch [41/200] | Train Loss: 0.3044, Train Acc: 0.8706 | \nEpoch [42/200] | Train Loss: 0.2834, Train Acc: 0.8918 | \nEpoch [43/200] | Train Loss: 0.2513, Train Acc: 0.9059 | \nEpoch [44/200] | Train Loss: 0.3063, Train Acc: 0.8729 | \nEpoch [45/200] | Train Loss: 0.4158, Train Acc: 0.8306 | \nEpoch [46/200] | Train Loss: 0.3705, Train Acc: 0.8471 | \nEpoch [47/200] | Train Loss: 0.3417, Train Acc: 0.8447 | \nEpoch [48/200] | Train Loss: 0.3511, Train Acc: 0.8635 | \nEpoch [49/200] | Train Loss: 0.3046, Train Acc: 0.8847 | \nEpoch [50/200] | Train Loss: 0.2738, Train Acc: 0.9059 | \nEpoch [51/200] | Train Loss: 0.2382, Train Acc: 0.9153 | \nEpoch [52/200] | Train Loss: 0.2657, Train Acc: 0.8988 | \nEpoch [53/200] | Train Loss: 0.3640, Train Acc: 0.8565 | \nEpoch [54/200] | Train Loss: 0.2905, Train Acc: 0.8847 | \nEpoch [55/200] | Train Loss: 0.2667, Train Acc: 0.8988 | \nEpoch [56/200] | Train Loss: 0.2160, Train Acc: 0.9224 | \nEpoch [57/200] | Train Loss: 0.2542, Train Acc: 0.9035 | \nEpoch [58/200] | Train Loss: 0.2203, Train Acc: 0.9200 | \nEpoch [59/200] | Train Loss: 0.2246, Train Acc: 0.9247 | \nEpoch [60/200] | Train Loss: 0.2843, Train Acc: 0.8753 | \nEpoch [61/200] | Train Loss: 0.3890, Train Acc: 0.8235 | \nEpoch [62/200] | Train Loss: 0.3952, Train Acc: 0.8235 | \nEpoch [63/200] | Train Loss: 0.2948, Train Acc: 0.8824 | \nEpoch [64/200] | Train Loss: 0.2339, Train Acc: 0.9176 | \nEpoch [65/200] | Train Loss: 0.2249, Train Acc: 0.9200 | \nEpoch [66/200] | Train Loss: 0.2797, Train Acc: 0.8847 | \nEpoch 00067: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [67/200] | Train Loss: 0.2604, Train Acc: 0.8941 | \nEpoch [68/200] | Train Loss: 0.2368, Train Acc: 0.9082 | \nEpoch [69/200] | Train Loss: 0.2085, Train Acc: 0.9224 | \nEpoch [70/200] | Train Loss: 0.1896, Train Acc: 0.9412 | \nEpoch [71/200] | Train Loss: 0.1865, Train Acc: 0.9341 | \nEpoch [72/200] | Train Loss: 0.1811, Train Acc: 0.9365 | \nEpoch [73/200] | Train Loss: 0.1757, Train Acc: 0.9435 | \nEpoch [74/200] | Train Loss: 0.1604, Train Acc: 0.9506 | \nEpoch [75/200] | Train Loss: 0.1695, Train Acc: 0.9412 | \nEpoch [76/200] | Train Loss: 0.1636, Train Acc: 0.9482 | \nEpoch [77/200] | Train Loss: 0.1515, Train Acc: 0.9506 | \nEpoch [78/200] | Train Loss: 0.1507, Train Acc: 0.9553 | \nEpoch [79/200] | Train Loss: 0.1632, Train Acc: 0.9459 | \nEpoch [80/200] | Train Loss: 0.1642, Train Acc: 0.9482 | \nEpoch [81/200] | Train Loss: 0.1581, Train Acc: 0.9506 | \nEpoch [82/200] | Train Loss: 0.1539, Train Acc: 0.9506 | \nEpoch [83/200] | Train Loss: 0.1370, Train Acc: 0.9600 | \nEpoch [84/200] | Train Loss: 0.1383, Train Acc: 0.9600 | \nEpoch [85/200] | Train Loss: 0.1324, Train Acc: 0.9624 | \nEpoch [86/200] | Train Loss: 0.1440, Train Acc: 0.9576 | \nEpoch [87/200] | Train Loss: 0.1295, Train Acc: 0.9647 | \nEpoch [88/200] | Train Loss: 0.1368, Train Acc: 0.9600 | \nEpoch [89/200] | Train Loss: 0.1352, Train Acc: 0.9576 | \nEpoch [90/200] | Train Loss: 0.1212, Train Acc: 0.9647 | \nEpoch [91/200] | Train Loss: 0.1289, Train Acc: 0.9600 | \nEpoch [92/200] | Train Loss: 0.1455, Train Acc: 0.9529 | \nEpoch [93/200] | Train Loss: 0.1279, Train Acc: 0.9600 | \nEpoch [94/200] | Train Loss: 0.1210, Train Acc: 0.9671 | \nEpoch [95/200] | Train Loss: 0.1264, Train Acc: 0.9576 | \nEpoch [96/200] | Train Loss: 0.1311, Train Acc: 0.9553 | \nEpoch [97/200] | Train Loss: 0.1239, Train Acc: 0.9576 | \nEpoch [98/200] | Train Loss: 0.1279, Train Acc: 0.9576 | \nEpoch [99/200] | Train Loss: 0.1117, Train Acc: 0.9671 | \nEpoch [100/200] | Train Loss: 0.1057, Train Acc: 0.9741 | \nEpoch [101/200] | Train Loss: 0.1173, Train Acc: 0.9624 | \nEpoch [102/200] | Train Loss: 0.1066, Train Acc: 0.9718 | \nEpoch [103/200] | Train Loss: 0.1091, Train Acc: 0.9647 | \nEpoch [104/200] | Train Loss: 0.1067, Train Acc: 0.9718 | \nEpoch [105/200] | Train Loss: 0.0940, Train Acc: 0.9741 | \nEpoch [106/200] | Train Loss: 0.1066, Train Acc: 0.9694 | \nEpoch [107/200] | Train Loss: 0.1009, Train Acc: 0.9671 | \nEpoch [108/200] | Train Loss: 0.1169, Train Acc: 0.9624 | \nEpoch [109/200] | Train Loss: 0.0983, Train Acc: 0.9671 | \nEpoch [110/200] | Train Loss: 0.0959, Train Acc: 0.9741 | \nEpoch [111/200] | Train Loss: 0.1025, Train Acc: 0.9647 | \nEpoch [112/200] | Train Loss: 0.1010, Train Acc: 0.9741 | \nEpoch [113/200] | Train Loss: 0.0996, Train Acc: 0.9647 | \nEpoch [114/200] | Train Loss: 0.0854, Train Acc: 0.9718 | \nEpoch [115/200] | Train Loss: 0.0895, Train Acc: 0.9741 | \nEpoch [116/200] | Train Loss: 0.1000, Train Acc: 0.9718 | \nEpoch [117/200] | Train Loss: 0.0858, Train Acc: 0.9671 | \nEpoch [118/200] | Train Loss: 0.0977, Train Acc: 0.9718 | \nEpoch [119/200] | Train Loss: 0.0875, Train Acc: 0.9765 | \nEpoch [120/200] | Train Loss: 0.0923, Train Acc: 0.9671 | \nEpoch [121/200] | Train Loss: 0.0914, Train Acc: 0.9624 | \nEpoch [122/200] | Train Loss: 0.0900, Train Acc: 0.9694 | \nEpoch [123/200] | Train Loss: 0.0862, Train Acc: 0.9788 | \nEpoch [124/200] | Train Loss: 0.0912, Train Acc: 0.9741 | \nEpoch 00125: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [125/200] | Train Loss: 0.1005, Train Acc: 0.9741 | \nEpoch [126/200] | Train Loss: 0.1013, Train Acc: 0.9694 | \nEpoch [127/200] | Train Loss: 0.0909, Train Acc: 0.9718 | \nEpoch [128/200] | Train Loss: 0.0806, Train Acc: 0.9765 | \nEpoch [129/200] | Train Loss: 0.0829, Train Acc: 0.9765 | \nEpoch [130/200] | Train Loss: 0.0838, Train Acc: 0.9718 | \nEpoch [131/200] | Train Loss: 0.0725, Train Acc: 0.9788 | \nEpoch [132/200] | Train Loss: 0.0830, Train Acc: 0.9694 | \nEpoch [133/200] | Train Loss: 0.0743, Train Acc: 0.9788 | \nEpoch [134/200] | Train Loss: 0.0784, Train Acc: 0.9788 | \nEpoch [135/200] | Train Loss: 0.0811, Train Acc: 0.9788 | \nEpoch [136/200] | Train Loss: 0.0548, Train Acc: 0.9812 | \nEpoch [137/200] | Train Loss: 0.0762, Train Acc: 0.9741 | \nEpoch [138/200] | Train Loss: 0.0824, Train Acc: 0.9788 | \nEpoch [139/200] | Train Loss: 0.0660, Train Acc: 0.9788 | \nEpoch [140/200] | Train Loss: 0.0760, Train Acc: 0.9694 | \nEpoch [141/200] | Train Loss: 0.0791, Train Acc: 0.9741 | \nEpoch [142/200] | Train Loss: 0.0768, Train Acc: 0.9741 | \nEpoch [143/200] | Train Loss: 0.0721, Train Acc: 0.9765 | \nEpoch [144/200] | Train Loss: 0.0772, Train Acc: 0.9694 | \nEpoch [145/200] | Train Loss: 0.0790, Train Acc: 0.9765 | \nEpoch [146/200] | Train Loss: 0.0742, Train Acc: 0.9741 | \nEpoch 00147: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [147/200] | Train Loss: 0.0838, Train Acc: 0.9741 | \nEpoch [148/200] | Train Loss: 0.0654, Train Acc: 0.9788 | \nEpoch [149/200] | Train Loss: 0.0620, Train Acc: 0.9835 | \nEpoch [150/200] | Train Loss: 0.0654, Train Acc: 0.9765 | \nEpoch [151/200] | Train Loss: 0.0657, Train Acc: 0.9812 | \nEpoch [152/200] | Train Loss: 0.0678, Train Acc: 0.9741 | \nEpoch [153/200] | Train Loss: 0.0705, Train Acc: 0.9788 | \nEpoch [154/200] | Train Loss: 0.0772, Train Acc: 0.9788 | \nEpoch [155/200] | Train Loss: 0.0713, Train Acc: 0.9788 | \nEpoch [156/200] | Train Loss: 0.0824, Train Acc: 0.9718 | \nEpoch [157/200] | Train Loss: 0.0705, Train Acc: 0.9741 | \nEpoch 00158: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [158/200] | Train Loss: 0.0637, Train Acc: 0.9765 | \nEpoch [159/200] | Train Loss: 0.0822, Train Acc: 0.9671 | \nEpoch [160/200] | Train Loss: 0.0631, Train Acc: 0.9859 | \nEpoch [161/200] | Train Loss: 0.0581, Train Acc: 0.9859 | \nEpoch [162/200] | Train Loss: 0.0918, Train Acc: 0.9694 | \nEpoch [163/200] | Train Loss: 0.0722, Train Acc: 0.9765 | \nEpoch [164/200] | Train Loss: 0.0738, Train Acc: 0.9718 | \nEpoch [165/200] | Train Loss: 0.0648, Train Acc: 0.9835 | \nEpoch [166/200] | Train Loss: 0.0631, Train Acc: 0.9788 | \nEpoch [167/200] | Train Loss: 0.0793, Train Acc: 0.9765 | \nEpoch [168/200] | Train Loss: 0.0543, Train Acc: 0.9835 | \nEpoch [169/200] | Train Loss: 0.0699, Train Acc: 0.9765 | \nEpoch [170/200] | Train Loss: 0.0688, Train Acc: 0.9812 | \nEpoch [171/200] | Train Loss: 0.0661, Train Acc: 0.9812 | \nEpoch [172/200] | Train Loss: 0.0740, Train Acc: 0.9741 | \nEpoch [173/200] | Train Loss: 0.0654, Train Acc: 0.9812 | \nEpoch [174/200] | Train Loss: 0.0743, Train Acc: 0.9765 | \nEpoch [175/200] | Train Loss: 0.0599, Train Acc: 0.9882 | \nEpoch [176/200] | Train Loss: 0.0754, Train Acc: 0.9812 | \nEpoch [177/200] | Train Loss: 0.0782, Train Acc: 0.9765 | \nEpoch [178/200] | Train Loss: 0.0669, Train Acc: 0.9835 | \nEpoch 00179: reducing learning rate of group 0 to 1.0000e-08.\nEpoch [179/200] | Train Loss: 0.0607, Train Acc: 0.9812 | \nEpoch [180/200] | Train Loss: 0.0682, Train Acc: 0.9788 | \nEpoch [181/200] | Train Loss: 0.0662, Train Acc: 0.9835 | \nEpoch [182/200] | Train Loss: 0.0717, Train Acc: 0.9765 | \nEpoch [183/200] | Train Loss: 0.0655, Train Acc: 0.9835 | \nEpoch [184/200] | Train Loss: 0.0663, Train Acc: 0.9812 | \nEpoch [185/200] | Train Loss: 0.0661, Train Acc: 0.9788 | \nEpoch [186/200] | Train Loss: 0.0616, Train Acc: 0.9835 | \nEpoch [187/200] | Train Loss: 0.0537, Train Acc: 0.9859 | \nEpoch [188/200] | Train Loss: 0.0691, Train Acc: 0.9859 | \nEpoch [189/200] | Train Loss: 0.0683, Train Acc: 0.9835 | \nEpoch [190/200] | Train Loss: 0.0686, Train Acc: 0.9788 | \nEpoch [191/200] | Train Loss: 0.0589, Train Acc: 0.9835 | \nEpoch [192/200] | Train Loss: 0.0758, Train Acc: 0.9788 | \nEpoch [193/200] | Train Loss: 0.0661, Train Acc: 0.9812 | \nEpoch [194/200] | Train Loss: 0.0616, Train Acc: 0.9812 | \nEarly stopping triggered.\nTest Loss: 0.4468, Test Accuracy: 0.8785, Test AUC: 0.9133\n\n--- Processing: psd_fc_delta ---\nShape: (532, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7602, Train Acc: 0.4800 | \nEpoch [2/200] | Train Loss: 0.6963, Train Acc: 0.5129 | \nEpoch [3/200] | Train Loss: 0.7029, Train Acc: 0.4941 | \nEpoch [4/200] | Train Loss: 0.6945, Train Acc: 0.5388 | \nEpoch [5/200] | Train Loss: 0.6588, Train Acc: 0.6165 | \nEpoch [6/200] | Train Loss: 0.6211, Train Acc: 0.6518 | \nEpoch [7/200] | Train Loss: 0.5096, Train Acc: 0.7741 | \nEpoch [8/200] | Train Loss: 0.4542, Train Acc: 0.7741 | \nEpoch [9/200] | Train Loss: 0.3995, Train Acc: 0.8329 | \nEpoch [10/200] | Train Loss: 0.4273, Train Acc: 0.8282 | \nEpoch [11/200] | Train Loss: 0.4077, Train Acc: 0.8071 | \nEpoch [12/200] | Train Loss: 0.3533, Train Acc: 0.8565 | \nEpoch [13/200] | Train Loss: 0.2990, Train Acc: 0.8824 | \nEpoch [14/200] | Train Loss: 0.2976, Train Acc: 0.8918 | \nEpoch [15/200] | Train Loss: 0.4031, Train Acc: 0.8306 | \nEpoch [16/200] | Train Loss: 0.3566, Train Acc: 0.8447 | \nEpoch [17/200] | Train Loss: 0.3334, Train Acc: 0.8588 | \nEpoch [18/200] | Train Loss: 0.2951, Train Acc: 0.8635 | \nEpoch [19/200] | Train Loss: 0.3162, Train Acc: 0.8729 | \nEpoch [20/200] | Train Loss: 0.3302, Train Acc: 0.8565 | \nEpoch [21/200] | Train Loss: 0.3588, Train Acc: 0.8400 | \nEpoch [22/200] | Train Loss: 0.3139, Train Acc: 0.8682 | \nEpoch [23/200] | Train Loss: 0.2759, Train Acc: 0.8776 | \nEpoch [24/200] | Train Loss: 0.3052, Train Acc: 0.8800 | \nEpoch [25/200] | Train Loss: 0.3007, Train Acc: 0.8612 | \nEpoch [26/200] | Train Loss: 0.2580, Train Acc: 0.8871 | \nEpoch [27/200] | Train Loss: 0.2478, Train Acc: 0.8824 | \nEpoch [28/200] | Train Loss: 0.2361, Train Acc: 0.9012 | \nEpoch [29/200] | Train Loss: 0.2717, Train Acc: 0.8800 | \nEpoch [30/200] | Train Loss: 0.2601, Train Acc: 0.8941 | \nEpoch [31/200] | Train Loss: 0.3679, Train Acc: 0.8212 | \nEpoch [32/200] | Train Loss: 0.2842, Train Acc: 0.8918 | \nEpoch [33/200] | Train Loss: 0.2662, Train Acc: 0.8918 | \nEpoch [34/200] | Train Loss: 0.2551, Train Acc: 0.8894 | \nEpoch [35/200] | Train Loss: 0.2186, Train Acc: 0.9200 | \nEpoch [36/200] | Train Loss: 0.2133, Train Acc: 0.9176 | \nEpoch [37/200] | Train Loss: 0.1841, Train Acc: 0.9224 | \nEpoch [38/200] | Train Loss: 0.2526, Train Acc: 0.9035 | \nEpoch [39/200] | Train Loss: 0.4104, Train Acc: 0.8024 | \nEpoch [40/200] | Train Loss: 0.2836, Train Acc: 0.8871 | \nEpoch [41/200] | Train Loss: 0.2498, Train Acc: 0.8941 | \nEpoch [42/200] | Train Loss: 0.2336, Train Acc: 0.9129 | \nEpoch [43/200] | Train Loss: 0.2262, Train Acc: 0.9082 | \nEpoch [44/200] | Train Loss: 0.2495, Train Acc: 0.9082 | \nEpoch [45/200] | Train Loss: 0.2442, Train Acc: 0.9059 | \nEpoch [46/200] | Train Loss: 0.1779, Train Acc: 0.9365 | \nEpoch [47/200] | Train Loss: 0.1668, Train Acc: 0.9318 | \nEpoch [48/200] | Train Loss: 0.1931, Train Acc: 0.9224 | \nEpoch [49/200] | Train Loss: 0.1768, Train Acc: 0.9271 | \nEpoch [50/200] | Train Loss: 0.1593, Train Acc: 0.9412 | \nEpoch [51/200] | Train Loss: 0.2077, Train Acc: 0.9318 | \nEpoch [52/200] | Train Loss: 0.1946, Train Acc: 0.9294 | \nEpoch [53/200] | Train Loss: 0.1472, Train Acc: 0.9482 | \nEpoch [54/200] | Train Loss: 0.1314, Train Acc: 0.9529 | \nEpoch [55/200] | Train Loss: 0.1262, Train Acc: 0.9482 | \nEpoch [56/200] | Train Loss: 0.1879, Train Acc: 0.9435 | \nEpoch [57/200] | Train Loss: 0.2000, Train Acc: 0.9247 | \nEpoch [58/200] | Train Loss: 0.1710, Train Acc: 0.9459 | \nEpoch [59/200] | Train Loss: 0.1668, Train Acc: 0.9412 | \nEpoch [60/200] | Train Loss: 0.1547, Train Acc: 0.9435 | \nEpoch [61/200] | Train Loss: 0.1241, Train Acc: 0.9600 | \nEpoch [62/200] | Train Loss: 0.1278, Train Acc: 0.9506 | \nEpoch [63/200] | Train Loss: 0.1797, Train Acc: 0.9294 | \nEpoch [64/200] | Train Loss: 0.2237, Train Acc: 0.9224 | \nEpoch [65/200] | Train Loss: 0.1690, Train Acc: 0.9412 | \nEpoch [66/200] | Train Loss: 0.2073, Train Acc: 0.9176 | \nEpoch [67/200] | Train Loss: 0.1631, Train Acc: 0.9459 | \nEpoch [68/200] | Train Loss: 0.1575, Train Acc: 0.9506 | \nEpoch [69/200] | Train Loss: 0.1983, Train Acc: 0.9200 | \nEpoch [70/200] | Train Loss: 0.1576, Train Acc: 0.9459 | \nEpoch [71/200] | Train Loss: 0.1382, Train Acc: 0.9576 | \nEpoch 00072: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [72/200] | Train Loss: 0.1476, Train Acc: 0.9553 | \nEpoch [73/200] | Train Loss: 0.1308, Train Acc: 0.9576 | \nEpoch [74/200] | Train Loss: 0.0983, Train Acc: 0.9671 | \nEpoch [75/200] | Train Loss: 0.0899, Train Acc: 0.9812 | \nEpoch [76/200] | Train Loss: 0.0785, Train Acc: 0.9788 | \nEpoch [77/200] | Train Loss: 0.0842, Train Acc: 0.9788 | \nEpoch [78/200] | Train Loss: 0.0772, Train Acc: 0.9765 | \nEpoch [79/200] | Train Loss: 0.0784, Train Acc: 0.9765 | \nEpoch [80/200] | Train Loss: 0.0735, Train Acc: 0.9812 | \nEpoch [81/200] | Train Loss: 0.0747, Train Acc: 0.9788 | \nEpoch [82/200] | Train Loss: 0.0683, Train Acc: 0.9835 | \nEpoch [83/200] | Train Loss: 0.0705, Train Acc: 0.9788 | \nEpoch [84/200] | Train Loss: 0.0770, Train Acc: 0.9741 | \nEpoch [85/200] | Train Loss: 0.0632, Train Acc: 0.9812 | \nEpoch [86/200] | Train Loss: 0.0696, Train Acc: 0.9788 | \nEpoch [87/200] | Train Loss: 0.0652, Train Acc: 0.9859 | \nEpoch [88/200] | Train Loss: 0.0627, Train Acc: 0.9788 | \nEpoch [89/200] | Train Loss: 0.0570, Train Acc: 0.9859 | \nEpoch [90/200] | Train Loss: 0.0616, Train Acc: 0.9835 | \nEpoch [91/200] | Train Loss: 0.0469, Train Acc: 0.9929 | \nEpoch [92/200] | Train Loss: 0.0502, Train Acc: 0.9882 | \nEpoch [93/200] | Train Loss: 0.0541, Train Acc: 0.9835 | \nEpoch [94/200] | Train Loss: 0.0478, Train Acc: 0.9882 | \nEpoch [95/200] | Train Loss: 0.0481, Train Acc: 0.9882 | \nEpoch [96/200] | Train Loss: 0.0518, Train Acc: 0.9882 | \nEpoch [97/200] | Train Loss: 0.0448, Train Acc: 0.9882 | \nEpoch [98/200] | Train Loss: 0.0373, Train Acc: 0.9882 | \nEpoch [99/200] | Train Loss: 0.0461, Train Acc: 0.9906 | \nEpoch [100/200] | Train Loss: 0.0574, Train Acc: 0.9882 | \nEpoch [101/200] | Train Loss: 0.0584, Train Acc: 0.9788 | \nEpoch [102/200] | Train Loss: 0.0525, Train Acc: 0.9835 | \nEpoch [103/200] | Train Loss: 0.0426, Train Acc: 0.9906 | \nEpoch [104/200] | Train Loss: 0.0476, Train Acc: 0.9882 | \nEpoch [105/200] | Train Loss: 0.0473, Train Acc: 0.9835 | \nEpoch [106/200] | Train Loss: 0.0450, Train Acc: 0.9882 | \nEpoch [107/200] | Train Loss: 0.0439, Train Acc: 0.9929 | \nEpoch [108/200] | Train Loss: 0.0422, Train Acc: 0.9859 | \nEpoch 00109: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [109/200] | Train Loss: 0.0485, Train Acc: 0.9882 | \nEpoch [110/200] | Train Loss: 0.0446, Train Acc: 0.9882 | \nEarly stopping triggered.\nTest Loss: 0.5918, Test Accuracy: 0.8411, Test AUC: 0.8938\n\n--- Processing: psd_fc_theta ---\nShape: (532, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7716, Train Acc: 0.4941 | \nEpoch [2/200] | Train Loss: 0.6994, Train Acc: 0.4941 | \nEpoch [3/200] | Train Loss: 0.6968, Train Acc: 0.4918 | \nEpoch [4/200] | Train Loss: 0.6886, Train Acc: 0.5341 | \nEpoch [5/200] | Train Loss: 0.6259, Train Acc: 0.6776 | \nEpoch [6/200] | Train Loss: 0.5692, Train Acc: 0.7200 | \nEpoch [7/200] | Train Loss: 0.5041, Train Acc: 0.7647 | \nEpoch [8/200] | Train Loss: 0.4260, Train Acc: 0.8094 | \nEpoch [9/200] | Train Loss: 0.4246, Train Acc: 0.8024 | \nEpoch [10/200] | Train Loss: 0.3882, Train Acc: 0.8376 | \nEpoch [11/200] | Train Loss: 0.3292, Train Acc: 0.8612 | \nEpoch [12/200] | Train Loss: 0.3991, Train Acc: 0.8071 | \nEpoch [13/200] | Train Loss: 0.3694, Train Acc: 0.8282 | \nEpoch [14/200] | Train Loss: 0.3668, Train Acc: 0.8282 | \nEpoch [15/200] | Train Loss: 0.3193, Train Acc: 0.8588 | \nEpoch [16/200] | Train Loss: 0.2947, Train Acc: 0.8659 | \nEpoch [17/200] | Train Loss: 0.2899, Train Acc: 0.8753 | \nEpoch [18/200] | Train Loss: 0.3139, Train Acc: 0.8471 | \nEpoch [19/200] | Train Loss: 0.2769, Train Acc: 0.8776 | \nEpoch [20/200] | Train Loss: 0.3653, Train Acc: 0.8682 | \nEpoch [21/200] | Train Loss: 0.2866, Train Acc: 0.8871 | \nEpoch [22/200] | Train Loss: 0.2722, Train Acc: 0.8706 | \nEpoch [23/200] | Train Loss: 0.2639, Train Acc: 0.8847 | \nEpoch [24/200] | Train Loss: 0.3095, Train Acc: 0.8659 | \nEpoch [25/200] | Train Loss: 0.3022, Train Acc: 0.8635 | \nEpoch [26/200] | Train Loss: 0.2289, Train Acc: 0.9012 | \nEpoch [27/200] | Train Loss: 0.3291, Train Acc: 0.8447 | \nEpoch [28/200] | Train Loss: 0.3188, Train Acc: 0.8588 | \nEpoch [29/200] | Train Loss: 0.2661, Train Acc: 0.8941 | \nEpoch [30/200] | Train Loss: 0.2575, Train Acc: 0.8941 | \nEpoch [31/200] | Train Loss: 0.2174, Train Acc: 0.9082 | \nEpoch [32/200] | Train Loss: 0.2456, Train Acc: 0.8871 | \nEpoch [33/200] | Train Loss: 0.2309, Train Acc: 0.8988 | \nEpoch [34/200] | Train Loss: 0.2345, Train Acc: 0.8965 | \nEpoch [35/200] | Train Loss: 0.1652, Train Acc: 0.9365 | \nEpoch [36/200] | Train Loss: 0.1765, Train Acc: 0.9224 | \nEpoch [37/200] | Train Loss: 0.1686, Train Acc: 0.9341 | \nEpoch [38/200] | Train Loss: 0.1801, Train Acc: 0.9247 | \nEpoch [39/200] | Train Loss: 0.1776, Train Acc: 0.9318 | \nEpoch [40/200] | Train Loss: 0.1637, Train Acc: 0.9318 | \nEpoch [41/200] | Train Loss: 0.3606, Train Acc: 0.8424 | \nEpoch [42/200] | Train Loss: 0.2576, Train Acc: 0.8847 | \nEpoch [43/200] | Train Loss: 0.2127, Train Acc: 0.9176 | \nEpoch [44/200] | Train Loss: 0.2093, Train Acc: 0.9153 | \nEpoch [45/200] | Train Loss: 0.1756, Train Acc: 0.9412 | \nEpoch [46/200] | Train Loss: 0.1888, Train Acc: 0.9224 | \nEpoch [47/200] | Train Loss: 0.1906, Train Acc: 0.9271 | \nEpoch [48/200] | Train Loss: 0.2060, Train Acc: 0.9153 | \nEpoch [49/200] | Train Loss: 0.1803, Train Acc: 0.9271 | \nEpoch [50/200] | Train Loss: 0.1531, Train Acc: 0.9412 | \nEpoch [51/200] | Train Loss: 0.1646, Train Acc: 0.9341 | \nEpoch [52/200] | Train Loss: 0.2214, Train Acc: 0.9153 | \nEpoch [53/200] | Train Loss: 0.1860, Train Acc: 0.9294 | \nEpoch [54/200] | Train Loss: 0.1784, Train Acc: 0.9388 | \nEpoch [55/200] | Train Loss: 0.1900, Train Acc: 0.9271 | \nEpoch [56/200] | Train Loss: 0.1509, Train Acc: 0.9459 | \nEpoch [57/200] | Train Loss: 0.1472, Train Acc: 0.9482 | \nEpoch [58/200] | Train Loss: 0.1722, Train Acc: 0.9294 | \nEpoch [59/200] | Train Loss: 0.4028, Train Acc: 0.8494 | \nEpoch [60/200] | Train Loss: 0.3554, Train Acc: 0.8494 | \nEpoch [61/200] | Train Loss: 0.2708, Train Acc: 0.8941 | \nEpoch [62/200] | Train Loss: 0.1940, Train Acc: 0.9271 | \nEpoch [63/200] | Train Loss: 0.1859, Train Acc: 0.9318 | \nEpoch [64/200] | Train Loss: 0.2128, Train Acc: 0.9153 | \nEpoch [65/200] | Train Loss: 0.1877, Train Acc: 0.9200 | \nEpoch [66/200] | Train Loss: 0.1870, Train Acc: 0.9200 | \nEpoch [67/200] | Train Loss: 0.1692, Train Acc: 0.9365 | \nEpoch [68/200] | Train Loss: 0.1268, Train Acc: 0.9529 | \nEpoch [69/200] | Train Loss: 0.1317, Train Acc: 0.9553 | \nEpoch [70/200] | Train Loss: 0.1333, Train Acc: 0.9506 | \nEpoch [71/200] | Train Loss: 0.1703, Train Acc: 0.9341 | \nEpoch [72/200] | Train Loss: 0.1677, Train Acc: 0.9388 | \nEpoch [73/200] | Train Loss: 0.2584, Train Acc: 0.8776 | \nEpoch [74/200] | Train Loss: 0.1920, Train Acc: 0.9341 | \nEpoch [75/200] | Train Loss: 0.1793, Train Acc: 0.9294 | \nEpoch [76/200] | Train Loss: 0.1597, Train Acc: 0.9294 | \nEpoch [77/200] | Train Loss: 0.1898, Train Acc: 0.9271 | \nEpoch [78/200] | Train Loss: 0.1682, Train Acc: 0.9412 | \nEpoch 00079: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [79/200] | Train Loss: 0.1724, Train Acc: 0.9412 | \nEpoch [80/200] | Train Loss: 0.1245, Train Acc: 0.9576 | \nEpoch [81/200] | Train Loss: 0.1279, Train Acc: 0.9529 | \nEpoch [82/200] | Train Loss: 0.1114, Train Acc: 0.9624 | \nEpoch [83/200] | Train Loss: 0.1058, Train Acc: 0.9553 | \nEpoch [84/200] | Train Loss: 0.1071, Train Acc: 0.9600 | \nEpoch [85/200] | Train Loss: 0.0960, Train Acc: 0.9624 | \nEpoch [86/200] | Train Loss: 0.1020, Train Acc: 0.9671 | \nEpoch [87/200] | Train Loss: 0.0935, Train Acc: 0.9647 | \nEpoch [88/200] | Train Loss: 0.0922, Train Acc: 0.9647 | \nEpoch [89/200] | Train Loss: 0.0922, Train Acc: 0.9671 | \nEpoch [90/200] | Train Loss: 0.0886, Train Acc: 0.9624 | \nEpoch [91/200] | Train Loss: 0.0724, Train Acc: 0.9694 | \nEpoch [92/200] | Train Loss: 0.0938, Train Acc: 0.9647 | \nEpoch [93/200] | Train Loss: 0.0770, Train Acc: 0.9741 | \nEpoch [94/200] | Train Loss: 0.0693, Train Acc: 0.9741 | \nEpoch [95/200] | Train Loss: 0.0674, Train Acc: 0.9741 | \nEpoch [96/200] | Train Loss: 0.0718, Train Acc: 0.9718 | \nEpoch [97/200] | Train Loss: 0.0797, Train Acc: 0.9718 | \nEpoch [98/200] | Train Loss: 0.0691, Train Acc: 0.9741 | \nEpoch [99/200] | Train Loss: 0.0780, Train Acc: 0.9741 | \nEpoch [100/200] | Train Loss: 0.0673, Train Acc: 0.9788 | \nEpoch [101/200] | Train Loss: 0.0642, Train Acc: 0.9765 | \nEpoch [102/200] | Train Loss: 0.0883, Train Acc: 0.9647 | \nEpoch [103/200] | Train Loss: 0.0954, Train Acc: 0.9741 | \nEpoch [104/200] | Train Loss: 0.0870, Train Acc: 0.9694 | \nEpoch [105/200] | Train Loss: 0.0738, Train Acc: 0.9741 | \nEpoch [106/200] | Train Loss: 0.0709, Train Acc: 0.9859 | \nEpoch [107/200] | Train Loss: 0.0739, Train Acc: 0.9718 | \nEpoch [108/200] | Train Loss: 0.0727, Train Acc: 0.9788 | \nEpoch [109/200] | Train Loss: 0.0620, Train Acc: 0.9835 | \nEpoch [110/200] | Train Loss: 0.0664, Train Acc: 0.9765 | \nEpoch [111/200] | Train Loss: 0.0698, Train Acc: 0.9788 | \nEpoch [112/200] | Train Loss: 0.0764, Train Acc: 0.9718 | \nEpoch [113/200] | Train Loss: 0.0690, Train Acc: 0.9765 | \nEpoch [114/200] | Train Loss: 0.0743, Train Acc: 0.9741 | \nEpoch [115/200] | Train Loss: 0.0685, Train Acc: 0.9741 | \nEpoch [116/200] | Train Loss: 0.0509, Train Acc: 0.9859 | \nEpoch [117/200] | Train Loss: 0.0496, Train Acc: 0.9859 | \nEpoch [118/200] | Train Loss: 0.0533, Train Acc: 0.9788 | \nEpoch [119/200] | Train Loss: 0.0688, Train Acc: 0.9741 | \nEpoch [120/200] | Train Loss: 0.0526, Train Acc: 0.9835 | \nEpoch [121/200] | Train Loss: 0.0488, Train Acc: 0.9835 | \nEpoch [122/200] | Train Loss: 0.0548, Train Acc: 0.9835 | \nEpoch [123/200] | Train Loss: 0.0568, Train Acc: 0.9812 | \nEpoch [124/200] | Train Loss: 0.0511, Train Acc: 0.9812 | \nEpoch [125/200] | Train Loss: 0.0452, Train Acc: 0.9835 | \nEarly stopping triggered.\nTest Loss: 0.4941, Test Accuracy: 0.8785, Test AUC: 0.9165\n\n--- Processing: psd_fc_alpha ---\nShape: (532, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7286, Train Acc: 0.5176 | \nEpoch [2/200] | Train Loss: 0.6955, Train Acc: 0.5176 | \nEpoch [3/200] | Train Loss: 0.6972, Train Acc: 0.4847 | \nEpoch [4/200] | Train Loss: 0.6688, Train Acc: 0.5553 | \nEpoch [5/200] | Train Loss: 0.5598, Train Acc: 0.7176 | \nEpoch [6/200] | Train Loss: 0.5557, Train Acc: 0.7388 | \nEpoch [7/200] | Train Loss: 0.5130, Train Acc: 0.7529 | \nEpoch [8/200] | Train Loss: 0.5771, Train Acc: 0.6988 | \nEpoch [9/200] | Train Loss: 0.5224, Train Acc: 0.7553 | \nEpoch [10/200] | Train Loss: 0.5190, Train Acc: 0.7506 | \nEpoch [11/200] | Train Loss: 0.4635, Train Acc: 0.7812 | \nEpoch [12/200] | Train Loss: 0.4213, Train Acc: 0.8259 | \nEpoch [13/200] | Train Loss: 0.4768, Train Acc: 0.7882 | \nEpoch [14/200] | Train Loss: 0.4043, Train Acc: 0.7953 | \nEpoch [15/200] | Train Loss: 0.3891, Train Acc: 0.8141 | \nEpoch [16/200] | Train Loss: 0.3759, Train Acc: 0.8165 | \nEpoch [17/200] | Train Loss: 0.4309, Train Acc: 0.7882 | \nEpoch [18/200] | Train Loss: 0.3799, Train Acc: 0.8188 | \nEpoch [19/200] | Train Loss: 0.3657, Train Acc: 0.8424 | \nEpoch [20/200] | Train Loss: 0.3328, Train Acc: 0.8565 | \nEpoch [21/200] | Train Loss: 0.3925, Train Acc: 0.8165 | \nEpoch [22/200] | Train Loss: 0.3342, Train Acc: 0.8447 | \nEpoch [23/200] | Train Loss: 0.3005, Train Acc: 0.8635 | \nEpoch [24/200] | Train Loss: 0.3877, Train Acc: 0.8494 | \nEpoch [25/200] | Train Loss: 0.3245, Train Acc: 0.8565 | \nEpoch [26/200] | Train Loss: 0.3234, Train Acc: 0.8682 | \nEpoch [27/200] | Train Loss: 0.2900, Train Acc: 0.8753 | \nEpoch [28/200] | Train Loss: 0.3016, Train Acc: 0.8729 | \nEpoch [29/200] | Train Loss: 0.3994, Train Acc: 0.8235 | \nEpoch [30/200] | Train Loss: 0.3015, Train Acc: 0.8824 | \nEpoch [31/200] | Train Loss: 0.2905, Train Acc: 0.8847 | \nEpoch [32/200] | Train Loss: 0.2818, Train Acc: 0.8776 | \nEpoch [33/200] | Train Loss: 0.2670, Train Acc: 0.8941 | \nEpoch [34/200] | Train Loss: 0.2303, Train Acc: 0.9012 | \nEpoch [35/200] | Train Loss: 0.2279, Train Acc: 0.8965 | \nEpoch [36/200] | Train Loss: 0.2797, Train Acc: 0.8965 | \nEpoch [37/200] | Train Loss: 0.2729, Train Acc: 0.8753 | \nEpoch [38/200] | Train Loss: 0.2177, Train Acc: 0.9224 | \nEpoch [39/200] | Train Loss: 0.2130, Train Acc: 0.9129 | \nEpoch [40/200] | Train Loss: 0.2244, Train Acc: 0.9035 | \nEpoch [41/200] | Train Loss: 0.2192, Train Acc: 0.9224 | \nEpoch [42/200] | Train Loss: 0.2225, Train Acc: 0.9035 | \nEpoch [43/200] | Train Loss: 0.1968, Train Acc: 0.9176 | \nEpoch [44/200] | Train Loss: 0.2066, Train Acc: 0.9176 | \nEpoch [45/200] | Train Loss: 0.2732, Train Acc: 0.8941 | \nEpoch [46/200] | Train Loss: 0.2733, Train Acc: 0.8918 | \nEpoch [47/200] | Train Loss: 0.2396, Train Acc: 0.8941 | \nEpoch [48/200] | Train Loss: 0.2385, Train Acc: 0.9153 | \nEpoch [49/200] | Train Loss: 0.2766, Train Acc: 0.8635 | \nEpoch [50/200] | Train Loss: 0.2247, Train Acc: 0.9106 | \nEpoch [51/200] | Train Loss: 0.1620, Train Acc: 0.9388 | \nEpoch [52/200] | Train Loss: 0.2097, Train Acc: 0.9341 | \nEpoch [53/200] | Train Loss: 0.3266, Train Acc: 0.8918 | \nEpoch [54/200] | Train Loss: 0.2110, Train Acc: 0.9224 | \nEpoch [55/200] | Train Loss: 0.1702, Train Acc: 0.9388 | \nEpoch [56/200] | Train Loss: 0.1389, Train Acc: 0.9553 | \nEpoch [57/200] | Train Loss: 0.1755, Train Acc: 0.9388 | \nEpoch [58/200] | Train Loss: 0.2113, Train Acc: 0.9200 | \nEpoch [59/200] | Train Loss: 0.2385, Train Acc: 0.9082 | \nEpoch [60/200] | Train Loss: 0.1996, Train Acc: 0.9200 | \nEpoch [61/200] | Train Loss: 0.2712, Train Acc: 0.8988 | \nEpoch [62/200] | Train Loss: 0.2030, Train Acc: 0.9294 | \nEpoch [63/200] | Train Loss: 0.2360, Train Acc: 0.9059 | \nEpoch [64/200] | Train Loss: 0.1344, Train Acc: 0.9482 | \nEpoch [65/200] | Train Loss: 0.1303, Train Acc: 0.9482 | \nEpoch [66/200] | Train Loss: 0.1595, Train Acc: 0.9365 | \nEpoch [67/200] | Train Loss: 0.1151, Train Acc: 0.9553 | \nEpoch [68/200] | Train Loss: 0.1806, Train Acc: 0.9318 | \nEpoch [69/200] | Train Loss: 0.1257, Train Acc: 0.9506 | \nEpoch [70/200] | Train Loss: 0.1154, Train Acc: 0.9624 | \nEpoch [71/200] | Train Loss: 0.0862, Train Acc: 0.9694 | \nEpoch [72/200] | Train Loss: 0.1159, Train Acc: 0.9671 | \nEpoch [73/200] | Train Loss: 0.1065, Train Acc: 0.9600 | \nEpoch [74/200] | Train Loss: 0.0919, Train Acc: 0.9671 | \nEpoch [75/200] | Train Loss: 0.0864, Train Acc: 0.9694 | \nEpoch [76/200] | Train Loss: 0.0739, Train Acc: 0.9835 | \nEpoch [77/200] | Train Loss: 0.0767, Train Acc: 0.9741 | \nEpoch [78/200] | Train Loss: 0.0961, Train Acc: 0.9671 | \nEpoch [79/200] | Train Loss: 0.0759, Train Acc: 0.9788 | \nEpoch [80/200] | Train Loss: 0.1271, Train Acc: 0.9506 | \nEpoch [81/200] | Train Loss: 0.0904, Train Acc: 0.9718 | \nEpoch [82/200] | Train Loss: 0.1165, Train Acc: 0.9576 | \nEpoch [83/200] | Train Loss: 0.0760, Train Acc: 0.9694 | \nEpoch [84/200] | Train Loss: 0.0622, Train Acc: 0.9859 | \nEpoch [85/200] | Train Loss: 0.1164, Train Acc: 0.9694 | \nEpoch [86/200] | Train Loss: 0.1779, Train Acc: 0.9388 | \nEpoch [87/200] | Train Loss: 0.1542, Train Acc: 0.9435 | \nEpoch [88/200] | Train Loss: 0.0990, Train Acc: 0.9765 | \nEpoch [89/200] | Train Loss: 0.1229, Train Acc: 0.9576 | \nEpoch [90/200] | Train Loss: 0.0798, Train Acc: 0.9718 | \nEpoch [91/200] | Train Loss: 0.1139, Train Acc: 0.9694 | \nEpoch [92/200] | Train Loss: 0.0780, Train Acc: 0.9671 | \nEpoch [93/200] | Train Loss: 0.0933, Train Acc: 0.9647 | \nEpoch [94/200] | Train Loss: 0.0577, Train Acc: 0.9859 | \nEpoch [95/200] | Train Loss: 0.2817, Train Acc: 0.8918 | \nEpoch [96/200] | Train Loss: 0.1910, Train Acc: 0.9224 | \nEpoch [97/200] | Train Loss: 0.1836, Train Acc: 0.9388 | \nEpoch [98/200] | Train Loss: 0.1290, Train Acc: 0.9506 | \nEpoch [99/200] | Train Loss: 0.0938, Train Acc: 0.9694 | \nEpoch [100/200] | Train Loss: 0.0758, Train Acc: 0.9812 | \nEpoch [101/200] | Train Loss: 0.0580, Train Acc: 0.9812 | \nEpoch [102/200] | Train Loss: 0.0789, Train Acc: 0.9718 | \nEpoch [103/200] | Train Loss: 0.0858, Train Acc: 0.9718 | \nEarly stopping triggered.\nTest Loss: 0.3465, Test Accuracy: 0.9252, Test AUC: 0.9347\n\n--- Processing: psd_fc_beta ---\nShape: (532, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7177, Train Acc: 0.5341 | \nEpoch [2/200] | Train Loss: 0.7028, Train Acc: 0.4847 | \nEpoch [3/200] | Train Loss: 0.6957, Train Acc: 0.5035 | \nEpoch [4/200] | Train Loss: 0.6680, Train Acc: 0.6212 | \nEpoch [5/200] | Train Loss: 0.6497, Train Acc: 0.6235 | \nEpoch [6/200] | Train Loss: 0.5813, Train Acc: 0.7200 | \nEpoch [7/200] | Train Loss: 0.4674, Train Acc: 0.8024 | \nEpoch [8/200] | Train Loss: 0.4664, Train Acc: 0.7741 | \nEpoch [9/200] | Train Loss: 0.3973, Train Acc: 0.8353 | \nEpoch [10/200] | Train Loss: 0.3813, Train Acc: 0.8306 | \nEpoch [11/200] | Train Loss: 0.3285, Train Acc: 0.8659 | \nEpoch [12/200] | Train Loss: 0.3454, Train Acc: 0.8612 | \nEpoch [13/200] | Train Loss: 0.3398, Train Acc: 0.8518 | \nEpoch [14/200] | Train Loss: 0.2971, Train Acc: 0.8800 | \nEpoch [15/200] | Train Loss: 0.2917, Train Acc: 0.8753 | \nEpoch [16/200] | Train Loss: 0.3391, Train Acc: 0.8635 | \nEpoch [17/200] | Train Loss: 0.3139, Train Acc: 0.8518 | \nEpoch [18/200] | Train Loss: 0.3047, Train Acc: 0.8659 | \nEpoch [19/200] | Train Loss: 0.2653, Train Acc: 0.8988 | \nEpoch [20/200] | Train Loss: 0.2557, Train Acc: 0.8965 | \nEpoch [21/200] | Train Loss: 0.3567, Train Acc: 0.8400 | \nEpoch [22/200] | Train Loss: 0.2639, Train Acc: 0.9035 | \nEpoch [23/200] | Train Loss: 0.3249, Train Acc: 0.8659 | \nEpoch [24/200] | Train Loss: 0.3125, Train Acc: 0.8659 | \nEpoch [25/200] | Train Loss: 0.2937, Train Acc: 0.8706 | \nEpoch [26/200] | Train Loss: 0.2980, Train Acc: 0.8612 | \nEpoch [27/200] | Train Loss: 0.2734, Train Acc: 0.8871 | \nEpoch [28/200] | Train Loss: 0.2369, Train Acc: 0.9012 | \nEpoch [29/200] | Train Loss: 0.2608, Train Acc: 0.8894 | \nEpoch [30/200] | Train Loss: 0.2625, Train Acc: 0.8894 | \nEpoch [31/200] | Train Loss: 0.2473, Train Acc: 0.9035 | \nEpoch [32/200] | Train Loss: 0.3931, Train Acc: 0.8329 | \nEpoch [33/200] | Train Loss: 0.2993, Train Acc: 0.8565 | \nEpoch [34/200] | Train Loss: 0.2541, Train Acc: 0.9012 | \nEpoch [35/200] | Train Loss: 0.2379, Train Acc: 0.9153 | \nEpoch [36/200] | Train Loss: 0.2007, Train Acc: 0.9271 | \nEpoch [37/200] | Train Loss: 0.3248, Train Acc: 0.8706 | \nEpoch [38/200] | Train Loss: 0.2655, Train Acc: 0.8824 | \nEpoch [39/200] | Train Loss: 0.2108, Train Acc: 0.9035 | \nEpoch [40/200] | Train Loss: 0.1751, Train Acc: 0.9294 | \nEpoch [41/200] | Train Loss: 0.2520, Train Acc: 0.8988 | \nEpoch [42/200] | Train Loss: 0.2269, Train Acc: 0.9035 | \nEpoch [43/200] | Train Loss: 0.2317, Train Acc: 0.9176 | \nEpoch [44/200] | Train Loss: 0.2011, Train Acc: 0.9082 | \nEpoch [45/200] | Train Loss: 0.1742, Train Acc: 0.9294 | \nEpoch [46/200] | Train Loss: 0.1341, Train Acc: 0.9435 | \nEpoch [47/200] | Train Loss: 0.1588, Train Acc: 0.9271 | \nEpoch [48/200] | Train Loss: 0.2135, Train Acc: 0.9247 | \nEpoch [49/200] | Train Loss: 0.2484, Train Acc: 0.9012 | \nEpoch [50/200] | Train Loss: 0.2962, Train Acc: 0.8729 | \nEpoch [51/200] | Train Loss: 0.2804, Train Acc: 0.8894 | \nEpoch [52/200] | Train Loss: 0.1990, Train Acc: 0.9153 | \nEpoch [53/200] | Train Loss: 0.2203, Train Acc: 0.9012 | \nEpoch [54/200] | Train Loss: 0.1690, Train Acc: 0.9271 | \nEpoch [55/200] | Train Loss: 0.1528, Train Acc: 0.9247 | \nEpoch [56/200] | Train Loss: 0.1507, Train Acc: 0.9318 | \nEpoch 00057: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [57/200] | Train Loss: 0.1659, Train Acc: 0.9341 | \nEpoch [58/200] | Train Loss: 0.1257, Train Acc: 0.9506 | \nEpoch [59/200] | Train Loss: 0.1208, Train Acc: 0.9482 | \nEpoch [60/200] | Train Loss: 0.1161, Train Acc: 0.9459 | \nEpoch [61/200] | Train Loss: 0.0929, Train Acc: 0.9671 | \nEpoch [62/200] | Train Loss: 0.0930, Train Acc: 0.9694 | \nEpoch [63/200] | Train Loss: 0.0845, Train Acc: 0.9718 | \nEpoch [64/200] | Train Loss: 0.0809, Train Acc: 0.9694 | \nEpoch [65/200] | Train Loss: 0.0820, Train Acc: 0.9694 | \nEpoch [66/200] | Train Loss: 0.0775, Train Acc: 0.9694 | \nEpoch [67/200] | Train Loss: 0.0751, Train Acc: 0.9718 | \nEpoch [68/200] | Train Loss: 0.0737, Train Acc: 0.9765 | \nEpoch [69/200] | Train Loss: 0.0667, Train Acc: 0.9741 | \nEpoch [70/200] | Train Loss: 0.0622, Train Acc: 0.9812 | \nEpoch [71/200] | Train Loss: 0.0726, Train Acc: 0.9741 | \nEpoch [72/200] | Train Loss: 0.0624, Train Acc: 0.9812 | \nEpoch [73/200] | Train Loss: 0.0579, Train Acc: 0.9765 | \nEpoch [74/200] | Train Loss: 0.0575, Train Acc: 0.9812 | \nEpoch [75/200] | Train Loss: 0.0551, Train Acc: 0.9835 | \nEpoch [76/200] | Train Loss: 0.0467, Train Acc: 0.9859 | \nEpoch [77/200] | Train Loss: 0.0507, Train Acc: 0.9835 | \nEpoch [78/200] | Train Loss: 0.0455, Train Acc: 0.9859 | \nEpoch [79/200] | Train Loss: 0.0515, Train Acc: 0.9835 | \nEpoch [80/200] | Train Loss: 0.0433, Train Acc: 0.9812 | \nEpoch [81/200] | Train Loss: 0.0512, Train Acc: 0.9788 | \nEpoch [82/200] | Train Loss: 0.0430, Train Acc: 0.9835 | \nEpoch [83/200] | Train Loss: 0.0463, Train Acc: 0.9859 | \nEpoch [84/200] | Train Loss: 0.0546, Train Acc: 0.9859 | \nEpoch [85/200] | Train Loss: 0.0567, Train Acc: 0.9812 | \nEpoch [86/200] | Train Loss: 0.0563, Train Acc: 0.9788 | \nEpoch [87/200] | Train Loss: 0.0380, Train Acc: 0.9882 | \nEpoch [88/200] | Train Loss: 0.0284, Train Acc: 0.9953 | \nEpoch [89/200] | Train Loss: 0.0387, Train Acc: 0.9859 | \nEpoch [90/200] | Train Loss: 0.0411, Train Acc: 0.9788 | \nEpoch [91/200] | Train Loss: 0.0418, Train Acc: 0.9882 | \nEpoch [92/200] | Train Loss: 0.0406, Train Acc: 0.9835 | \nEpoch [93/200] | Train Loss: 0.0488, Train Acc: 0.9812 | \nEpoch [94/200] | Train Loss: 0.0358, Train Acc: 0.9906 | \nEpoch [95/200] | Train Loss: 0.0321, Train Acc: 0.9906 | \nEpoch [96/200] | Train Loss: 0.0476, Train Acc: 0.9812 | \nEpoch [97/200] | Train Loss: 0.0337, Train Acc: 0.9859 | \nEpoch [98/200] | Train Loss: 0.0333, Train Acc: 0.9882 | \nEpoch 00099: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [99/200] | Train Loss: 0.0434, Train Acc: 0.9859 | \nEpoch [100/200] | Train Loss: 0.0372, Train Acc: 0.9906 | \nEpoch [101/200] | Train Loss: 0.0376, Train Acc: 0.9882 | \nEpoch [102/200] | Train Loss: 0.0301, Train Acc: 0.9906 | \nEpoch [103/200] | Train Loss: 0.0291, Train Acc: 0.9906 | \nEpoch [104/200] | Train Loss: 0.0339, Train Acc: 0.9906 | \nEpoch [105/200] | Train Loss: 0.0327, Train Acc: 0.9906 | \nEpoch [106/200] | Train Loss: 0.0472, Train Acc: 0.9835 | \nEpoch [107/200] | Train Loss: 0.0269, Train Acc: 0.9929 | \nEarly stopping triggered.\nTest Loss: 0.4530, Test Accuracy: 0.8785, Test AUC: 0.9392\n\n--- Processing: psd_fc_highbeta ---\nShape: (532, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7392, Train Acc: 0.5035 | \nEpoch [2/200] | Train Loss: 0.7095, Train Acc: 0.5106 | \nEpoch [3/200] | Train Loss: 0.6918, Train Acc: 0.5482 | \nEpoch [4/200] | Train Loss: 0.6766, Train Acc: 0.5882 | \nEpoch [5/200] | Train Loss: 0.6136, Train Acc: 0.6824 | \nEpoch [6/200] | Train Loss: 0.6256, Train Acc: 0.6659 | \nEpoch [7/200] | Train Loss: 0.5577, Train Acc: 0.7153 | \nEpoch [8/200] | Train Loss: 0.5040, Train Acc: 0.7482 | \nEpoch [9/200] | Train Loss: 0.4964, Train Acc: 0.7529 | \nEpoch [10/200] | Train Loss: 0.4713, Train Acc: 0.7882 | \nEpoch [11/200] | Train Loss: 0.4523, Train Acc: 0.7741 | \nEpoch [12/200] | Train Loss: 0.4498, Train Acc: 0.8071 | \nEpoch [13/200] | Train Loss: 0.4465, Train Acc: 0.7882 | \nEpoch [14/200] | Train Loss: 0.4015, Train Acc: 0.8141 | \nEpoch [15/200] | Train Loss: 0.3758, Train Acc: 0.8376 | \nEpoch [16/200] | Train Loss: 0.3472, Train Acc: 0.8447 | \nEpoch [17/200] | Train Loss: 0.3300, Train Acc: 0.8635 | \nEpoch [18/200] | Train Loss: 0.3786, Train Acc: 0.8259 | \nEpoch [19/200] | Train Loss: 0.3571, Train Acc: 0.8588 | \nEpoch [20/200] | Train Loss: 0.3275, Train Acc: 0.8659 | \nEpoch [21/200] | Train Loss: 0.3186, Train Acc: 0.8541 | \nEpoch [22/200] | Train Loss: 0.3080, Train Acc: 0.8612 | \nEpoch [23/200] | Train Loss: 0.2961, Train Acc: 0.8824 | \nEpoch [24/200] | Train Loss: 0.3530, Train Acc: 0.8471 | \nEpoch [25/200] | Train Loss: 0.3292, Train Acc: 0.8588 | \nEpoch [26/200] | Train Loss: 0.3515, Train Acc: 0.8588 | \nEpoch [27/200] | Train Loss: 0.3026, Train Acc: 0.8776 | \nEpoch [28/200] | Train Loss: 0.3321, Train Acc: 0.8541 | \nEpoch [29/200] | Train Loss: 0.3062, Train Acc: 0.8847 | \nEpoch [30/200] | Train Loss: 0.2706, Train Acc: 0.8871 | \nEpoch [31/200] | Train Loss: 0.2695, Train Acc: 0.8894 | \nEpoch [32/200] | Train Loss: 0.2532, Train Acc: 0.8965 | \nEpoch [33/200] | Train Loss: 0.3016, Train Acc: 0.8682 | \nEpoch [34/200] | Train Loss: 0.3104, Train Acc: 0.8612 | \nEpoch [35/200] | Train Loss: 0.2688, Train Acc: 0.8871 | \nEpoch [36/200] | Train Loss: 0.2791, Train Acc: 0.8847 | \nEpoch [37/200] | Train Loss: 0.2850, Train Acc: 0.8847 | \nEpoch [38/200] | Train Loss: 0.4014, Train Acc: 0.8141 | \nEpoch [39/200] | Train Loss: 0.2636, Train Acc: 0.8894 | \nEpoch [40/200] | Train Loss: 0.2515, Train Acc: 0.9035 | \nEpoch [41/200] | Train Loss: 0.2448, Train Acc: 0.9035 | \nEpoch [42/200] | Train Loss: 0.1997, Train Acc: 0.9224 | \nEpoch [43/200] | Train Loss: 0.3217, Train Acc: 0.8565 | \nEpoch [44/200] | Train Loss: 0.2405, Train Acc: 0.8988 | \nEpoch [45/200] | Train Loss: 0.2546, Train Acc: 0.9012 | \nEpoch [46/200] | Train Loss: 0.1819, Train Acc: 0.9224 | \nEpoch [47/200] | Train Loss: 0.2044, Train Acc: 0.9200 | \nEpoch [48/200] | Train Loss: 0.1958, Train Acc: 0.9200 | \nEpoch [49/200] | Train Loss: 0.2062, Train Acc: 0.9200 | \nEpoch [50/200] | Train Loss: 0.2057, Train Acc: 0.9271 | \nEpoch [51/200] | Train Loss: 0.1882, Train Acc: 0.9271 | \nEpoch [52/200] | Train Loss: 0.2053, Train Acc: 0.9271 | \nEpoch [53/200] | Train Loss: 0.1579, Train Acc: 0.9459 | \nEpoch [54/200] | Train Loss: 0.1576, Train Acc: 0.9412 | \nEpoch [55/200] | Train Loss: 0.1713, Train Acc: 0.9341 | \nEpoch [56/200] | Train Loss: 0.1704, Train Acc: 0.9388 | \nEpoch [57/200] | Train Loss: 0.2145, Train Acc: 0.9224 | \nEpoch [58/200] | Train Loss: 0.2071, Train Acc: 0.9129 | \nEpoch [59/200] | Train Loss: 0.1746, Train Acc: 0.9365 | \nEpoch [60/200] | Train Loss: 0.1637, Train Acc: 0.9294 | \nEpoch [61/200] | Train Loss: 0.1386, Train Acc: 0.9506 | \nEpoch [62/200] | Train Loss: 0.3295, Train Acc: 0.8565 | \nEpoch [63/200] | Train Loss: 0.2826, Train Acc: 0.8800 | \nEpoch [64/200] | Train Loss: 0.2204, Train Acc: 0.9247 | \nEpoch [65/200] | Train Loss: 0.2994, Train Acc: 0.8871 | \nEpoch [66/200] | Train Loss: 0.2981, Train Acc: 0.8635 | \nEpoch [67/200] | Train Loss: 0.2217, Train Acc: 0.8988 | \nEpoch [68/200] | Train Loss: 0.1592, Train Acc: 0.9388 | \nEpoch [69/200] | Train Loss: 0.1667, Train Acc: 0.9318 | \nEpoch [70/200] | Train Loss: 0.2004, Train Acc: 0.9341 | \nEpoch [71/200] | Train Loss: 0.1765, Train Acc: 0.9247 | \nEpoch 00072: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [72/200] | Train Loss: 0.1612, Train Acc: 0.9459 | \nEpoch [73/200] | Train Loss: 0.1392, Train Acc: 0.9600 | \nEpoch [74/200] | Train Loss: 0.1195, Train Acc: 0.9600 | \nEpoch [75/200] | Train Loss: 0.1070, Train Acc: 0.9600 | \nEpoch [76/200] | Train Loss: 0.0933, Train Acc: 0.9741 | \nEpoch [77/200] | Train Loss: 0.0997, Train Acc: 0.9741 | \nEpoch [78/200] | Train Loss: 0.0859, Train Acc: 0.9765 | \nEpoch [79/200] | Train Loss: 0.0834, Train Acc: 0.9788 | \nEpoch [80/200] | Train Loss: 0.0877, Train Acc: 0.9718 | \nEpoch [81/200] | Train Loss: 0.0847, Train Acc: 0.9718 | \nEpoch [82/200] | Train Loss: 0.0857, Train Acc: 0.9741 | \nEpoch [83/200] | Train Loss: 0.0660, Train Acc: 0.9835 | \nEpoch [84/200] | Train Loss: 0.0745, Train Acc: 0.9788 | \nEpoch [85/200] | Train Loss: 0.0661, Train Acc: 0.9812 | \nEpoch [86/200] | Train Loss: 0.0687, Train Acc: 0.9859 | \nEpoch [87/200] | Train Loss: 0.0707, Train Acc: 0.9765 | \nEpoch [88/200] | Train Loss: 0.0613, Train Acc: 0.9859 | \nEpoch [89/200] | Train Loss: 0.0708, Train Acc: 0.9812 | \nEpoch [90/200] | Train Loss: 0.0639, Train Acc: 0.9835 | \nEpoch [91/200] | Train Loss: 0.0647, Train Acc: 0.9812 | \nEpoch [92/200] | Train Loss: 0.0763, Train Acc: 0.9694 | \nEpoch [93/200] | Train Loss: 0.0557, Train Acc: 0.9812 | \nEpoch [94/200] | Train Loss: 0.0520, Train Acc: 0.9906 | \nEpoch [95/200] | Train Loss: 0.0653, Train Acc: 0.9835 | \nEpoch [96/200] | Train Loss: 0.0495, Train Acc: 0.9859 | \nEpoch [97/200] | Train Loss: 0.0580, Train Acc: 0.9859 | \nEpoch [98/200] | Train Loss: 0.0517, Train Acc: 0.9882 | \nEpoch [99/200] | Train Loss: 0.0454, Train Acc: 0.9859 | \nEpoch [100/200] | Train Loss: 0.0581, Train Acc: 0.9835 | \nEpoch [101/200] | Train Loss: 0.0491, Train Acc: 0.9859 | \nEpoch [102/200] | Train Loss: 0.0544, Train Acc: 0.9835 | \nEpoch [103/200] | Train Loss: 0.0509, Train Acc: 0.9882 | \nEpoch [104/200] | Train Loss: 0.0423, Train Acc: 0.9882 | \nEpoch [105/200] | Train Loss: 0.0466, Train Acc: 0.9859 | \nEpoch [106/200] | Train Loss: 0.0519, Train Acc: 0.9882 | \nEpoch [107/200] | Train Loss: 0.0426, Train Acc: 0.9882 | \nEpoch [108/200] | Train Loss: 0.0522, Train Acc: 0.9882 | \nEpoch [109/200] | Train Loss: 0.0389, Train Acc: 0.9906 | \nEpoch [110/200] | Train Loss: 0.0433, Train Acc: 0.9835 | \nEpoch [111/200] | Train Loss: 0.0455, Train Acc: 0.9859 | \nEpoch [112/200] | Train Loss: 0.0540, Train Acc: 0.9835 | \nEpoch [113/200] | Train Loss: 0.0614, Train Acc: 0.9835 | \nEarly stopping triggered.\nTest Loss: 0.4103, Test Accuracy: 0.8692, Test AUC: 0.9235\n\n--- Processing: psd_fc_gamma ---\nShape: (532, 190)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7030, Train Acc: 0.5200 | \nEpoch [2/200] | Train Loss: 0.6820, Train Acc: 0.5671 | \nEpoch [3/200] | Train Loss: 0.6044, Train Acc: 0.6612 | \nEpoch [4/200] | Train Loss: 0.5995, Train Acc: 0.6776 | \nEpoch [5/200] | Train Loss: 0.4968, Train Acc: 0.7718 | \nEpoch [6/200] | Train Loss: 0.5162, Train Acc: 0.7765 | \nEpoch [7/200] | Train Loss: 0.4377, Train Acc: 0.7882 | \nEpoch [8/200] | Train Loss: 0.4310, Train Acc: 0.8071 | \nEpoch [9/200] | Train Loss: 0.4267, Train Acc: 0.8141 | \nEpoch [10/200] | Train Loss: 0.5081, Train Acc: 0.7412 | \nEpoch [11/200] | Train Loss: 0.4361, Train Acc: 0.8000 | \nEpoch [12/200] | Train Loss: 0.4400, Train Acc: 0.7812 | \nEpoch [13/200] | Train Loss: 0.4113, Train Acc: 0.8141 | \nEpoch [14/200] | Train Loss: 0.4058, Train Acc: 0.8376 | \nEpoch [15/200] | Train Loss: 0.3571, Train Acc: 0.8565 | \nEpoch [16/200] | Train Loss: 0.3456, Train Acc: 0.8471 | \nEpoch [17/200] | Train Loss: 0.3876, Train Acc: 0.8329 | \nEpoch [18/200] | Train Loss: 0.4096, Train Acc: 0.8024 | \nEpoch [19/200] | Train Loss: 0.3431, Train Acc: 0.8518 | \nEpoch [20/200] | Train Loss: 0.4734, Train Acc: 0.8000 | \nEpoch [21/200] | Train Loss: 0.3957, Train Acc: 0.8282 | \nEpoch [22/200] | Train Loss: 0.3543, Train Acc: 0.8376 | \nEpoch [23/200] | Train Loss: 0.2982, Train Acc: 0.8753 | \nEpoch [24/200] | Train Loss: 0.3162, Train Acc: 0.8635 | \nEpoch [25/200] | Train Loss: 0.3490, Train Acc: 0.8494 | \nEpoch [26/200] | Train Loss: 0.3487, Train Acc: 0.8400 | \nEpoch [27/200] | Train Loss: 0.2722, Train Acc: 0.8847 | \nEpoch [28/200] | Train Loss: 0.2640, Train Acc: 0.8965 | \nEpoch [29/200] | Train Loss: 0.3186, Train Acc: 0.8753 | \nEpoch [30/200] | Train Loss: 0.2635, Train Acc: 0.8824 | \nEpoch [31/200] | Train Loss: 0.2828, Train Acc: 0.8800 | \nEpoch [32/200] | Train Loss: 0.2507, Train Acc: 0.9059 | \nEpoch [33/200] | Train Loss: 0.2491, Train Acc: 0.8988 | \nEpoch [34/200] | Train Loss: 0.2070, Train Acc: 0.9153 | \nEpoch [35/200] | Train Loss: 0.1897, Train Acc: 0.9271 | \nEpoch [36/200] | Train Loss: 0.2110, Train Acc: 0.9106 | \nEpoch [37/200] | Train Loss: 0.2284, Train Acc: 0.8941 | \nEpoch [38/200] | Train Loss: 0.2090, Train Acc: 0.9271 | \nEpoch [39/200] | Train Loss: 0.2225, Train Acc: 0.9247 | \nEpoch [40/200] | Train Loss: 0.1737, Train Acc: 0.9294 | \nEpoch [41/200] | Train Loss: 0.1526, Train Acc: 0.9482 | \nEpoch [42/200] | Train Loss: 0.2254, Train Acc: 0.9082 | \nEpoch [43/200] | Train Loss: 0.2056, Train Acc: 0.9200 | \nEpoch [44/200] | Train Loss: 0.1748, Train Acc: 0.9482 | \nEpoch [45/200] | Train Loss: 0.2019, Train Acc: 0.9247 | \nEpoch [46/200] | Train Loss: 0.2169, Train Acc: 0.9176 | \nEpoch [47/200] | Train Loss: 0.2426, Train Acc: 0.8918 | \nEpoch [48/200] | Train Loss: 0.1941, Train Acc: 0.9176 | \nEpoch [49/200] | Train Loss: 0.1824, Train Acc: 0.9176 | \nEpoch [50/200] | Train Loss: 0.1756, Train Acc: 0.9318 | \nEpoch [51/200] | Train Loss: 0.1373, Train Acc: 0.9482 | \nEpoch [52/200] | Train Loss: 0.1163, Train Acc: 0.9553 | \nEpoch [53/200] | Train Loss: 0.2052, Train Acc: 0.9224 | \nEpoch [54/200] | Train Loss: 0.3250, Train Acc: 0.8706 | \nEpoch [55/200] | Train Loss: 0.2878, Train Acc: 0.8753 | \nEpoch [56/200] | Train Loss: 0.2396, Train Acc: 0.9129 | \nEpoch [57/200] | Train Loss: 0.2015, Train Acc: 0.9271 | \nEpoch [58/200] | Train Loss: 0.2151, Train Acc: 0.9129 | \nEpoch [59/200] | Train Loss: 0.1965, Train Acc: 0.9129 | \nEpoch [60/200] | Train Loss: 0.2682, Train Acc: 0.8635 | \nEpoch [61/200] | Train Loss: 0.2003, Train Acc: 0.9224 | \nEpoch [62/200] | Train Loss: 0.1405, Train Acc: 0.9459 | \nEpoch 00063: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [63/200] | Train Loss: 0.3198, Train Acc: 0.8565 | \nEpoch [64/200] | Train Loss: 0.2137, Train Acc: 0.9129 | \nEpoch [65/200] | Train Loss: 0.1688, Train Acc: 0.9459 | \nEpoch [66/200] | Train Loss: 0.1474, Train Acc: 0.9647 | \nEpoch [67/200] | Train Loss: 0.1340, Train Acc: 0.9576 | \nEpoch [68/200] | Train Loss: 0.1080, Train Acc: 0.9647 | \nEpoch [69/200] | Train Loss: 0.1120, Train Acc: 0.9671 | \nEpoch [70/200] | Train Loss: 0.0899, Train Acc: 0.9694 | \nEpoch [71/200] | Train Loss: 0.0915, Train Acc: 0.9765 | \nEpoch [72/200] | Train Loss: 0.0851, Train Acc: 0.9647 | \nEpoch [73/200] | Train Loss: 0.0748, Train Acc: 0.9694 | \nEpoch [74/200] | Train Loss: 0.0651, Train Acc: 0.9859 | \nEpoch [75/200] | Train Loss: 0.0613, Train Acc: 0.9835 | \nEpoch [76/200] | Train Loss: 0.0596, Train Acc: 0.9812 | \nEpoch [77/200] | Train Loss: 0.0509, Train Acc: 0.9859 | \nEpoch [78/200] | Train Loss: 0.0589, Train Acc: 0.9835 | \nEpoch [79/200] | Train Loss: 0.0468, Train Acc: 0.9882 | \nEpoch [80/200] | Train Loss: 0.0481, Train Acc: 0.9859 | \nEpoch [81/200] | Train Loss: 0.0473, Train Acc: 0.9882 | \nEpoch [82/200] | Train Loss: 0.0443, Train Acc: 0.9859 | \nEpoch [83/200] | Train Loss: 0.0414, Train Acc: 0.9929 | \nEpoch [84/200] | Train Loss: 0.0465, Train Acc: 0.9882 | \nEpoch [85/200] | Train Loss: 0.0347, Train Acc: 0.9906 | \nEpoch [86/200] | Train Loss: 0.0327, Train Acc: 0.9906 | \nEpoch [87/200] | Train Loss: 0.0295, Train Acc: 0.9906 | \nEpoch [88/200] | Train Loss: 0.0535, Train Acc: 0.9859 | \nEpoch [89/200] | Train Loss: 0.0419, Train Acc: 0.9882 | \nEpoch [90/200] | Train Loss: 0.0352, Train Acc: 0.9929 | \nEpoch [91/200] | Train Loss: 0.0483, Train Acc: 0.9859 | \nEpoch [92/200] | Train Loss: 0.0209, Train Acc: 0.9953 | \nEpoch [93/200] | Train Loss: 0.0271, Train Acc: 0.9953 | \nEpoch [94/200] | Train Loss: 0.0307, Train Acc: 0.9929 | \nEpoch [95/200] | Train Loss: 0.0230, Train Acc: 0.9929 | \nEpoch [96/200] | Train Loss: 0.0290, Train Acc: 0.9906 | \nEpoch [97/200] | Train Loss: 0.0291, Train Acc: 0.9906 | \nEpoch [98/200] | Train Loss: 0.0429, Train Acc: 0.9859 | \nEpoch [99/200] | Train Loss: 0.0165, Train Acc: 0.9976 | \nEpoch [100/200] | Train Loss: 0.0345, Train Acc: 0.9906 | \nEpoch [101/200] | Train Loss: 0.0170, Train Acc: 0.9976 | \nEpoch [102/200] | Train Loss: 0.0126, Train Acc: 1.0000 | \nEpoch [103/200] | Train Loss: 0.0164, Train Acc: 0.9953 | \nEpoch [104/200] | Train Loss: 0.0243, Train Acc: 0.9953 | \nEpoch [105/200] | Train Loss: 0.0154, Train Acc: 0.9953 | \nEpoch [106/200] | Train Loss: 0.0155, Train Acc: 0.9976 | \nEpoch [107/200] | Train Loss: 0.0169, Train Acc: 0.9976 | \nEpoch [108/200] | Train Loss: 0.0084, Train Acc: 1.0000 | \nEpoch [109/200] | Train Loss: 0.0115, Train Acc: 0.9976 | \nEpoch [110/200] | Train Loss: 0.0096, Train Acc: 0.9976 | \nEpoch [111/200] | Train Loss: 0.0163, Train Acc: 0.9976 | \nEpoch [112/200] | Train Loss: 0.0180, Train Acc: 0.9953 | \nEpoch [113/200] | Train Loss: 0.0103, Train Acc: 0.9976 | \nEpoch [114/200] | Train Loss: 0.0130, Train Acc: 0.9953 | \nEpoch [115/200] | Train Loss: 0.0068, Train Acc: 1.0000 | \nEpoch [116/200] | Train Loss: 0.0111, Train Acc: 0.9953 | \nEpoch [117/200] | Train Loss: 0.0123, Train Acc: 0.9976 | \nEpoch [118/200] | Train Loss: 0.0077, Train Acc: 0.9976 | \nEpoch [119/200] | Train Loss: 0.0139, Train Acc: 0.9953 | \nEpoch [120/200] | Train Loss: 0.0228, Train Acc: 0.9929 | \nEpoch [121/200] | Train Loss: 0.0253, Train Acc: 0.9929 | \nEarly stopping triggered.\nTest Loss: 0.4798, Test Accuracy: 0.8879, Test AUC: 0.9354\n\n=== Disorder: Obsessive compulsive disorder ===\n\n--- Processing: psd_all_bands ---\nShape: (190, 114)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.7972, Train Acc: 0.4605 | \nEpoch [2/200] | Train Loss: 0.7173, Train Acc: 0.3947 | \nEpoch [3/200] | Train Loss: 0.6958, Train Acc: 0.4737 | \nEpoch [4/200] | Train Loss: 0.7090, Train Acc: 0.5000 | \nEpoch [5/200] | Train Loss: 0.6882, Train Acc: 0.5461 | \nEpoch [6/200] | Train Loss: 0.6910, Train Acc: 0.5132 | \nEpoch [7/200] | Train Loss: 0.6800, Train Acc: 0.5789 | \nEpoch [8/200] | Train Loss: 0.6698, Train Acc: 0.6250 | \nEpoch [9/200] | Train Loss: 0.6631, Train Acc: 0.6250 | \nEpoch [10/200] | Train Loss: 0.6339, Train Acc: 0.6250 | \nEpoch [11/200] | Train Loss: 0.6339, Train Acc: 0.6776 | \nEpoch [12/200] | Train Loss: 0.5701, Train Acc: 0.7237 | \nEpoch [13/200] | Train Loss: 0.5850, Train Acc: 0.7237 | \nEpoch [14/200] | Train Loss: 0.5634, Train Acc: 0.7237 | \nEpoch [15/200] | Train Loss: 0.5289, Train Acc: 0.7434 | \nEpoch [16/200] | Train Loss: 0.4831, Train Acc: 0.7632 | \nEpoch [17/200] | Train Loss: 0.4974, Train Acc: 0.7697 | \nEpoch [18/200] | Train Loss: 0.5361, Train Acc: 0.7171 | \nEpoch [19/200] | Train Loss: 0.4208, Train Acc: 0.8421 | \nEpoch [20/200] | Train Loss: 0.3763, Train Acc: 0.8289 | \nEpoch [21/200] | Train Loss: 0.3515, Train Acc: 0.8553 | \nEpoch [22/200] | Train Loss: 0.3656, Train Acc: 0.8553 | \nEpoch [23/200] | Train Loss: 0.3402, Train Acc: 0.8553 | \nEpoch [24/200] | Train Loss: 0.3038, Train Acc: 0.9013 | \nEpoch [25/200] | Train Loss: 0.3362, Train Acc: 0.8816 | \nEpoch [26/200] | Train Loss: 0.3537, Train Acc: 0.8618 | \nEpoch [27/200] | Train Loss: 0.2673, Train Acc: 0.8816 | \nEpoch [28/200] | Train Loss: 0.2778, Train Acc: 0.9013 | \nEpoch [29/200] | Train Loss: 0.2394, Train Acc: 0.9079 | \nEpoch [30/200] | Train Loss: 0.2654, Train Acc: 0.8947 | \nEpoch [31/200] | Train Loss: 0.2130, Train Acc: 0.9079 | \nEpoch [32/200] | Train Loss: 0.5193, Train Acc: 0.7961 | \nEpoch [33/200] | Train Loss: 0.4686, Train Acc: 0.7500 | \nEpoch [34/200] | Train Loss: 0.3485, Train Acc: 0.8289 | \nEpoch [35/200] | Train Loss: 0.2656, Train Acc: 0.9013 | \nEpoch [36/200] | Train Loss: 0.2294, Train Acc: 0.9145 | \nEpoch [37/200] | Train Loss: 0.1690, Train Acc: 0.9605 | \nEpoch [38/200] | Train Loss: 0.2041, Train Acc: 0.9342 | \nEpoch [39/200] | Train Loss: 0.2101, Train Acc: 0.9145 | \nEpoch [40/200] | Train Loss: 0.2465, Train Acc: 0.9013 | \nEpoch [41/200] | Train Loss: 0.1835, Train Acc: 0.9342 | \nEpoch [42/200] | Train Loss: 0.1526, Train Acc: 0.9605 | \nEpoch [43/200] | Train Loss: 0.1847, Train Acc: 0.9276 | \nEpoch [44/200] | Train Loss: 0.1204, Train Acc: 0.9671 | \nEpoch [45/200] | Train Loss: 0.2647, Train Acc: 0.9145 | \nEpoch [46/200] | Train Loss: 0.2499, Train Acc: 0.9079 | \nEpoch [47/200] | Train Loss: 0.2824, Train Acc: 0.8816 | \nEpoch [48/200] | Train Loss: 0.2284, Train Acc: 0.9211 | \nEpoch [49/200] | Train Loss: 0.2263, Train Acc: 0.9211 | \nEpoch [50/200] | Train Loss: 0.1829, Train Acc: 0.9342 | \nEpoch [51/200] | Train Loss: 0.2026, Train Acc: 0.9276 | \nEpoch [52/200] | Train Loss: 0.1611, Train Acc: 0.9539 | \nEpoch [53/200] | Train Loss: 0.1728, Train Acc: 0.9276 | \nEpoch [54/200] | Train Loss: 0.1157, Train Acc: 0.9539 | \nEpoch [55/200] | Train Loss: 0.1077, Train Acc: 0.9605 | \nEpoch [56/200] | Train Loss: 0.0849, Train Acc: 0.9803 | \nEpoch [57/200] | Train Loss: 0.2018, Train Acc: 0.9211 | \nEpoch [58/200] | Train Loss: 0.1812, Train Acc: 0.9079 | \nEpoch [59/200] | Train Loss: 0.2692, Train Acc: 0.8947 | \nEpoch [60/200] | Train Loss: 0.2589, Train Acc: 0.8618 | \nEpoch [61/200] | Train Loss: 0.3265, Train Acc: 0.8553 | \nEpoch [62/200] | Train Loss: 0.2170, Train Acc: 0.9276 | \nEpoch [63/200] | Train Loss: 0.1764, Train Acc: 0.9342 | \nEpoch [64/200] | Train Loss: 0.2041, Train Acc: 0.9342 | \nEpoch [65/200] | Train Loss: 0.1359, Train Acc: 0.9539 | \nEpoch [66/200] | Train Loss: 0.1021, Train Acc: 0.9605 | \nEpoch 00067: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [67/200] | Train Loss: 0.1106, Train Acc: 0.9539 | \nEpoch [68/200] | Train Loss: 0.0716, Train Acc: 0.9803 | \nEpoch [69/200] | Train Loss: 0.0321, Train Acc: 1.0000 | \nEpoch [70/200] | Train Loss: 0.0385, Train Acc: 0.9868 | \nEpoch [71/200] | Train Loss: 0.0394, Train Acc: 0.9868 | \nEpoch [72/200] | Train Loss: 0.0416, Train Acc: 0.9934 | \nEpoch [73/200] | Train Loss: 0.0305, Train Acc: 0.9934 | \nEpoch [74/200] | Train Loss: 0.0236, Train Acc: 1.0000 | \nEpoch [75/200] | Train Loss: 0.0233, Train Acc: 0.9934 | \nEpoch [76/200] | Train Loss: 0.0290, Train Acc: 0.9934 | \nEpoch [77/200] | Train Loss: 0.0223, Train Acc: 1.0000 | \nEpoch [78/200] | Train Loss: 0.0327, Train Acc: 0.9934 | \nEpoch [79/200] | Train Loss: 0.0205, Train Acc: 1.0000 | \nEpoch [80/200] | Train Loss: 0.0134, Train Acc: 1.0000 | \nEpoch [81/200] | Train Loss: 0.0161, Train Acc: 1.0000 | \nEpoch [82/200] | Train Loss: 0.0192, Train Acc: 0.9934 | \nEpoch [83/200] | Train Loss: 0.0253, Train Acc: 0.9934 | \nEpoch [84/200] | Train Loss: 0.0122, Train Acc: 1.0000 | \nEpoch [85/200] | Train Loss: 0.0137, Train Acc: 1.0000 | \nEpoch [86/200] | Train Loss: 0.0141, Train Acc: 1.0000 | \nEpoch [87/200] | Train Loss: 0.0177, Train Acc: 0.9934 | \nEpoch [88/200] | Train Loss: 0.0173, Train Acc: 0.9934 | \nEarly stopping triggered.\nTest Loss: 1.0018, Test Accuracy: 0.7632, Test AUC: 0.8366\n\n--- Processing: fc_all_bands ---\nShape: (190, 1026)\nMissing values: 0\nEpoch [1/200] | Train Loss: 0.8067, Train Acc: 0.4737 | \nEpoch [2/200] | Train Loss: 0.6920, Train Acc: 0.5132 | \nEpoch [3/200] | Train Loss: 0.7033, Train Acc: 0.4868 | \nEpoch [4/200] | Train Loss: 0.6971, Train Acc: 0.4934 | \nEpoch [5/200] | Train Loss: 0.6933, Train Acc: 0.5132 | \nEpoch [6/200] | Train Loss: 0.6893, Train Acc: 0.4934 | \nEpoch [7/200] | Train Loss: 0.6952, Train Acc: 0.5132 | \nEpoch [8/200] | Train Loss: 0.6812, Train Acc: 0.5526 | \nEpoch [9/200] | Train Loss: 0.6727, Train Acc: 0.5789 | \nEpoch [10/200] | Train Loss: 0.6474, Train Acc: 0.5921 | \nEpoch [11/200] | Train Loss: 0.6380, Train Acc: 0.6579 | \nEpoch [12/200] | Train Loss: 0.5253, Train Acc: 0.7632 | \nEpoch [13/200] | Train Loss: 0.4294, Train Acc: 0.7961 | \nEpoch [14/200] | Train Loss: 0.4809, Train Acc: 0.7829 | \nEpoch [15/200] | Train Loss: 0.3499, Train Acc: 0.8816 | \nEpoch [16/200] | Train Loss: 0.2654, Train Acc: 0.9013 | \nEpoch [17/200] | Train Loss: 0.4126, Train Acc: 0.8553 | \nEpoch [18/200] | Train Loss: 0.3934, Train Acc: 0.8092 | \nEpoch [19/200] | Train Loss: 0.3328, Train Acc: 0.8750 | \nEpoch [20/200] | Train Loss: 0.2944, Train Acc: 0.9145 | \nEpoch [21/200] | Train Loss: 0.2076, Train Acc: 0.9079 | \nEpoch [22/200] | Train Loss: 0.1221, Train Acc: 0.9803 | \nEpoch [23/200] | Train Loss: 0.1347, Train Acc: 0.9605 | \nEpoch [24/200] | Train Loss: 0.0993, Train Acc: 0.9671 | \nEpoch [25/200] | Train Loss: 0.1424, Train Acc: 0.9539 | \nEpoch [26/200] | Train Loss: 0.2362, Train Acc: 0.9211 | \nEpoch [27/200] | Train Loss: 0.2368, Train Acc: 0.9145 | \nEpoch [28/200] | Train Loss: 0.1449, Train Acc: 0.9474 | \nEpoch [29/200] | Train Loss: 0.1681, Train Acc: 0.9276 | \nEpoch [30/200] | Train Loss: 0.1299, Train Acc: 0.9474 | \nEpoch [31/200] | Train Loss: 0.1186, Train Acc: 0.9342 | \nEpoch [32/200] | Train Loss: 0.0607, Train Acc: 0.9803 | \nEpoch [33/200] | Train Loss: 0.0552, Train Acc: 0.9803 | \nEpoch [34/200] | Train Loss: 0.0513, Train Acc: 0.9868 | \nEpoch [35/200] | Train Loss: 0.0254, Train Acc: 0.9934 | \nEpoch [36/200] | Train Loss: 0.0734, Train Acc: 0.9671 | \nEpoch [37/200] | Train Loss: 0.1028, Train Acc: 0.9671 | \nEpoch [38/200] | Train Loss: 0.2043, Train Acc: 0.9211 | \nEpoch [39/200] | Train Loss: 0.2742, Train Acc: 0.8947 | \nEpoch [40/200] | Train Loss: 0.1525, Train Acc: 0.9605 | \nEpoch [41/200] | Train Loss: 0.1689, Train Acc: 0.9408 | \nEpoch [42/200] | Train Loss: 0.1117, Train Acc: 0.9605 | \nEpoch [43/200] | Train Loss: 0.0599, Train Acc: 0.9868 | \nEpoch [44/200] | Train Loss: 0.0616, Train Acc: 0.9803 | \nEpoch [45/200] | Train Loss: 0.0387, Train Acc: 0.9803 | \nEpoch 00046: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [46/200] | Train Loss: 0.0500, Train Acc: 0.9737 | \nEpoch [47/200] | Train Loss: 0.0786, Train Acc: 0.9737 | \nEpoch [48/200] | Train Loss: 0.0260, Train Acc: 0.9868 | \nEpoch [49/200] | Train Loss: 0.0108, Train Acc: 1.0000 | \nEpoch [50/200] | Train Loss: 0.0271, Train Acc: 0.9868 | \nEpoch [51/200] | Train Loss: 0.0104, Train Acc: 1.0000 | \nEpoch [52/200] | Train Loss: 0.0078, Train Acc: 1.0000 | \nEpoch [53/200] | Train Loss: 0.0072, Train Acc: 1.0000 | \n","output_type":"stream"}],"execution_count":null},{"id":"61286a0d","cell_type":"markdown","source":"## | 6. Results","metadata":{}},{"id":"3f42e84d","cell_type":"code","source":"rows = []\n\nfor disorder, dicts in all_results.items():\n    for dict_name, df_results in dicts.items():\n        for df_name, metrics in df_results.items():\n            row = {\n                'Disorder': disorder,\n                'Dict': dict_name,\n                'DataFrame': df_name,\n                'Accuracy': metrics['accuracy'],\n                'AUC': metrics['auc'],\n                'Classification Report': metrics['classification_report']\n            }\n            \n            rows.append(row)\n\nresults_df = pd.DataFrame(rows)\nresults_df.to_csv(f'{experiment_name}.csv', index=False)\n\n# best_results = df.loc[df.groupby('Disorder')['Accuracy'].idxmax()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8fc72ccb","cell_type":"markdown","source":"## | 7. Conclusions, Problems, and Limitations","metadata":{"papermill":{"duration":0.030435,"end_time":"2024-11-21T17:48:20.74518","exception":false,"start_time":"2024-11-21T17:48:20.714745","status":"completed"},"tags":[]}},{"id":"512b5d66","cell_type":"markdown","source":"[need to update]","metadata":{"papermill":{"duration":0.03075,"end_time":"2024-11-21T17:48:20.808076","exception":false,"start_time":"2024-11-21T17:48:20.777326","status":"completed"},"tags":[]}}]}